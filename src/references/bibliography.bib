@inproceedings{adamopoulou-moussiades:2020:overview-chatbot-technology,
  title = {An {{Overview}} of {{Chatbot Technology}}},
  booktitle = {Artificial {{Intelligence Applications}} and {{Innovations}}},
  author = {Adamopoulou, Eleni and Moussiades, Lefteris},
  editor = {Maglogiannis, Ilias and Iliadis, Lazaros and Pimenidis, Elias},
  date = {2020},
  series = {{{IFIP Advances}} in {{Information}} and {{Communication Technology}}},
  pages = {373--383},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10/ghj8},
  abstract = {The use of chatbots evolved rapidly in numerous fields in recent years, including Marketing, Supporting Systems, Education, Health Care, Cultural Heritage, and Entertainment. In this paper, we first present a historical overview of the evolution of the international community’s interest in chatbots. Next, we discuss the motivations that drive the use of chatbots, and we clarify chatbots’ usefulness in a variety of areas. Moreover, we highlight the impact of social stereotypes on chatbots design. After clarifying necessary technological concepts, we move on to a chatbot classification based on various criteria, such as the area of knowledge they refer to, the need they serve and others. Furthermore, we present the general architecture of modern chatbots while also mentioning the main platforms for their creation. Our engagement with the subject so far, reassures us of the prospects of chatbots and encourages us to study them in greater extent and depth.},
  isbn = {978-3-030-49186-4},
  langid = {english},
  keywords = {*CITATION,Artificial Intelligence,Chatbot,Chatbot architecture,Machine learning,NLU},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\IKPC4LT8\Adamopoulou and Moussiades - 2020 - An Overview of Chatbot Technology.pdf}
}

@article{aized-amin-soofi-arshad-awan:2017:classification-techniques-machine,
  title = {Classification {{Techniques}} in {{Machine Learning}}: {{Applications}} and {{Issues}}},
  shorttitle = {Classification {{Techniques}} in {{Machine Learning}}},
  author = {{Aized Amin Soofi} and {Arshad Awan}},
  date = {2017-01-05},
  journaltitle = {J. Basic Appl. Sci.},
  volume = {13},
  pages = {459--465},
  issn = {1927-5129, 1814-8085},
  doi = {10.6000/1927-5129.2017.13.76},
  url = {https://setpublisher.com/pms/index.php/jbas/article/view/1715},
  urldate = {2023-09-18},
  abstract = {Classification is a data mining (machine learning) technique used to predict group membership for data instances. There are several classification techniques that can be used for classification purpose. In this paper, we present the basic classification techniques. Later we discuss some major types of classification method including Bayesian networks, decision tree induction, k-nearest neighbor classifier and Support Vector Machines (SVM) with their strengths, weaknesses, potential applications and issues with their available solution. The goal of this study is to provide a comprehensive review of different classification techniques in machine learning. This work will be helpful for both academia and new comers in the field of machine learning to further strengthen the basis of classification methods.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\F6U6GM2B\Aized Amin Soofi et Arshad Awan - 2017 - Classification Techniques in Machine Learning App.pdf}
}

@software{alammar-grefenstette:2022:cohere-sandbox,
  title = {Cohere {{Sandbox}}},
  author = {Alammar, Jay and Grefenstette, Edward},
  date = {2022},
  url = {https://github.com/cohere-ai/sandbox-topically},
  organization = {{Cohere.ai}},
  keywords = {*CITATION}
}

@article{alasadi-bhaya:2017:review-data-preprocessing,
  title = {Review of Data Preprocessing Techniques in Data Mining},
  author = {Alasadi, Suad A and Bhaya, Wesam S},
  date = {2017},
  journaltitle = {Journal of Engineering and Applied Sciences},
  volume = {12},
  number = {16},
  pages = {4102--4107},
  issn = {1816-949X},
  url = {https://www.academia.edu/download/54509277/4102-4107.pdf},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\EU53T374\Alasadi et Bhaya - 2017 - Review of data preprocessing techniques in data mi.pdf}
}

@article{alexa-internet:2018:keyword-research-competitor,
  title = {Keyword {{Research}}, {{Competitor Analysis}}, \& {{Website Ranking}} | {{Alexa}}},
  author = {{Alexa Internet}},
  date = {2018-01-27},
  url = {https://www.alexa.com},
  keywords = {*CITATION}
}

@book{anderson:2013:architecture-cognition,
  title = {The {{Architecture}} of {{Cognition}}},
  author = {Anderson, John R.},
  date = {2013-11-19},
  edition = {0},
  publisher = {{Psychology Press}},
  url = {https://www.taylorfrancis.com/books/9781317759539},
  urldate = {2023-06-07},
  isbn = {978-1-317-75953-9},
  langid = {english},
  keywords = {*CITATION,Ergonomie,Temps adaptation}
}

@article{artstein-poesio:2008:intercoder-agreement-computational,
  title = {Inter-{{Coder Agreement}} for {{Computational Linguistics}}},
  author = {Artstein, Ron and Poesio, Massimo},
  date = {2008-12},
  journaltitle = {Computational Linguistics},
  volume = {34},
  number = {4},
  pages = {555--596},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli.07-034-R2},
  url = {https://direct.mit.edu/coli/article/34/4/555-596/1999},
  urldate = {2023-08-31},
  abstract = {This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\5MU8HATV\Artstein et Poesio - 2008 - Inter-Coder Agreement for Computational Linguistic.pdf}
}

@article{asher-etal:2017:manuel-annotation-actes,
  title = {Manuel d’annotation en actes de dialogue pour le corpus Datcha},
  author = {Asher, Nicholas and Nasr, Alexis and Perrotin, Robin},
  date = {2017-05-02},
  langid = {french},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\PSXD8FDM\Asher et al. - Manuel d’annotation en actes de dialogue pour le c.pdf}
}

@software{audacity-team:2000:audacity-free-audio,
  title = {Audacity: {{Free Audio Editor}} and {{Recorder}}},
  author = {{Audacity Team}},
  date = {2000},
  url = {https://www.audacityteam.org/},
  organization = {{Muse Group}},
  keywords = {*CITATION}
}

@article{awel-abidi:2019:review-optical-character,
  title = {Review on {{Optical Character Recognition}}},
  author = {Awel, Muna Ahmed and Abidi, Ali Imam},
  date = {2019},
  volume = {06},
  number = {06},
  abstract = {Optical Character Recognition is the area of Pattern Recognition that has a topic of studies over the past some decades. Optical character recognition is technique of automatically identifying of different character from a record picture additionally provide full alphanumeric recognition of printed or handwritten characters, text numerical, letters, and symbols in to a computer process able layout including ASCII, Unicode and so forth. Optical character recognition is the bottom for many distinct styles of programs in diverse fields, a lot of which we use in our daily lives. Cost effective and less time consuming, corporations, submit offices, banks, security systems, and even the field of robotics hire this system as the base in their Operations. These days, there are numerous portions of research and making use of OCR technology. These OCR technologies help to examine unique documents written in English, Chinese, Hindu, Arabic, Russian, and others languages. On This paper present review of some researches has been made in English, Arabic and Devanagaricharacters. And explained the methodology they use and challenge they face during development of Optical character recognition.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\IND7PUJD\Awel et Abidi - 2019 - Review on Optical Character Recognition.pdf}
}

@thesis{baledent:2022:complexite-annotation-manuelle,
  title = {De la complexité de l'annotation manuelle : méthodologie, biais et recommandations},
  author = {Baledent, Anaelle},
  date = {2023-12-01},
  institution = {{Normandie Université}},
  url = {https://theses.hal.science/tel-04011353},
  abstract = {Les corpus de référence annotés constituent des éléments primordiaux de nombreuses tâches du Traitement Automatique des Langues. Leur construction fait l'objet d’une attention particulière, notamment lors de campagnes d’annotation manuelle. Ces dernières impliquent de multiples aspects, déjà étudiés dans la littérature mais souvent de manière séparée. Nous présentons une synthèse des problèmes rencontrés lors des différentes étapes d'une campagne, attirant l’attention des gestionnaires sur des points de vigilance, afin qu'ils fassent preuve de prudence durant leur campagne.Cette thèse donne une première définition des biais d’annotation, qui sont des phénomènes perturbateurs et variés pouvant avoir une incidence sur les annotations. Nous proposons une méthode et des moyens d'observation pour détecter et analyser la présence de biais d’annotation. Deux campagnes d’annotation, menées spécialement dans le but d'étudier des biais particuliers, servent d'illustration et nous ont permis de constater l'influence tangible de certains paramètres sur l’annotation. Dans cette optique, nous avons aussi introduit la notion de consensualité, qui permet en particulier de situer un annotateur par rapport à un groupe. Nous montrons un premier lien entre les annotateurs les moins consensuels et les moins performants.},
  langid = {french},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\PK64N4R8\Baledent - 2022 - De la complexité de l'annotation manuelle  méthod.pdf}
}

@article{bayerl-paul:2011:what-determines-intercoder,
  title = {What {{Determines Inter-Coder Agreement}} in {{Manual Annotations}}? {{A Meta-Analytic Investigation}}},
  shorttitle = {What {{Determines Inter-Coder Agreement}} in {{Manual Annotations}}?},
  author = {Bayerl, Petra Saskia and Paul, Karsten Ingmar},
  date = {2011-12},
  journaltitle = {Computational Linguistics},
  volume = {37},
  number = {4},
  pages = {699--725},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/COLI_a_00074},
  url = {https://direct.mit.edu/coli/article/37/4/699-725/2129},
  urldate = {2023-08-31},
  abstract = {Recent discussions of annotator agreement have mostly centered around its calculation and interpretation, and the correct choice of indices. Although these discussions are important, they only consider the “back-end” of the story, namely, what to do once the data are collected. Just as important in our opinion is to know how agreement is reached in the first place and what factors influence coder agreement as part of the annotation process or setting, as this knowledge can provide concrete guidelines for the planning and set-up of annotation projects. To investigate whether there are factors that consistently impact annotator agreement we conducted a meta-analytic investigation of annotation studies reporting agreement percentages. Our meta-analysis synthesized factors reported in 96 annotation studies from three domains (word-sense disambiguation, prosodic transcriptions, and phonetic transcriptions) and was based on a total of 346 agreement indices. Our analysis identified seven factors that influence reported agreement values: annotation domain, number of categories in a coding scheme, number of annotators in a project, whether annotators received training, the intensity of annotator training, the annotation purpose, and the method used for the calculation of percentage agreements. Based on our results we develop practical recommendations for the assessment, interpretation, calculation, and reporting of coder agreement. We also briefly discuss theoretical implications for the concept of annotation quality.},
  langid = {english},
  keywords = {*CITATION,Biais,Différences},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\VF72XVM7\Bayerl et Paul - 2011 - What Determines Inter-Coder Agreement in Manual An.pdf}
}

@article{becet:2020:10-metiers-prometteurs,
  entrysubtype = {newspaper},
  title = {Les 10 métiers les plus prometteurs du monde de la data},
  author = {Bécet, Jean},
  date = {2020-09-25},
  edition = {Les Echos Start},
  url = {https://start.lesechos.fr/travailler-mieux/classements/les-10-metiers-les-plus-prometteurs-du-monde-de-la-data-1248702},
  abstract = {Quels sont les professionnels des données qui seront le plus en vue dans les prochaines années ? L'organisation professionnelle qui réunit l'ensemble des métiers du conseil vient de dévoiler une cartographie des métiers les plus prisés et prometteurs.},
  journalsubtitle = {Travailler mieux},
  langid = {french},
  keywords = {*CITATION}
}

@inproceedings{berchmans-kumar:2014:optical-character-recognition,
  title = {Optical Character Recognition: {{An}} Overview and an Insight},
  shorttitle = {Optical Character Recognition},
  booktitle = {2014 {{International Conference}} on {{Control}}, {{Instrumentation}}, {{Communication}} and {{Computational Technologies}} ({{ICCICCT}})},
  author = {Berchmans, Deepa and Kumar, S. S.},
  date = {2014-07},
  pages = {1361--1365},
  publisher = {{IEEE}},
  location = {{Kanyakumari}},
  doi = {10.1109/ICCICCT.2014.6993174},
  url = {http://ieeexplore.ieee.org/document/6993174/},
  urldate = {2023-09-15},
  abstract = {Many researches are going on in the field of optical character recognition (OCR) for the last few decades and a lot of articles have been published. Also a large number of OCR is available commercially. In this literature a review of the OCR history and the various techniques used for OCR development in the chronological order is being done.},
  eventtitle = {2014 {{International Conference}} on {{Control}}, {{Instrumentation}}, {{Communication}} and {{Computational Technologies}} ({{ICCICCT}})},
  isbn = {978-1-4799-4191-9 978-1-4799-4190-2},
  keywords = {*CITATION}
}

@article{blei-etal:2003:latent-dirichlet-allocation,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2003},
  journaltitle = {Journal of machine Learning research},
  number = {3},
  pages = {993--1022},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\WW295LFP\Blei - 2003 - Latent Dirichlet Allocation.pdf}
}

@article{bocklisch-etal:2017:rasa-open-source,
  title = {Rasa: {{Open Source Language Understanding}} and {{Dialogue Management}}},
  shorttitle = {Rasa},
  author = {Bocklisch, Tom and Faulkner, Joey and Pawlowski, Nick and Nichol, Alan},
  date = {2017-12-15},
  journaltitle = {ArXiv preprint},
  url = {http://arxiv.org/abs/1712.05181},
  urldate = {2020-10-23},
  abstract = {We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source python libraries for building conversational software. Their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers. In terms of design philosophy, we aim for ease of use, and bootstrapping from minimal (or no) initial training data. Both packages are extensively documented and ship with a comprehensive suite of tests. The code is available at https://github.com/RasaHQ/},
  langid = {english},
  keywords = {*CITATION,Artificial Intelligence,Computation and Language,Machine learning,Rasa},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\4ME935TW\Bocklisch et al. - 2017 - Rasa Open Source Language Understanding and Dialo.pdf}
}

@incollection{bohmova-etal:2003:prague-dependency-treebank,
  title = {The {{Prague Dependency Treebank}}},
  booktitle = {Treebanks},
  author = {Böhmová, Alena and Hajič, Jan and Hajičová, Eva and Hladká, Barbora},
  editor = {Abeillé, Anne},
  editorb = {Ide, Nancy and Véronis, Jean},
  editorbtype = {redactor},
  date = {2003},
  volume = {20},
  pages = {103--127},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  url = {http://link.springer.com/10.1007/978-94-010-0201-1_7},
  urldate = {2023-09-21},
  isbn = {978-1-4020-1335-5 978-94-010-0201-1},
  keywords = {*CITATION}
}

@article{bojanowski-etal:2016:enriching-word-vectors,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  date = {2016},
  journaltitle = {ArXiv preprint},
  keywords = {*CITATION,Computation and Language,Machine learning},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\BZMD9R5Y\Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf}
}

@inproceedings{bonneau-maynard-etal:2005:semantic-annotation-french,
  title = {Semantic Annotation of the {{French}} Media Dialog Corpus},
  booktitle = {Interspeech 2005},
  author = {Bonneau-Maynard, H. and Rosset, Sophie and Ayache, C. and Kuhn, A. and Mostefa, Djamel},
  date = {2005-09-04},
  pages = {3457--3460},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2005-312},
  url = {https://www.isca-speech.org/archive/interspeech_2005/bonneaumaynard05_interspeech.html},
  urldate = {2023-08-29},
  eventtitle = {Interspeech 2005},
  langid = {english},
  keywords = {*ALIRE/AIMPLÉMENTER,*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\K4EL85ZY\Bonneau-Maynard et al. - 2005 - Semantic annotation of the French media dialog cor.pdf}
}

@article{brown-etal:2020:language-models-are,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  urldate = {2023-07-17},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  keywords = {*CITATION,Computation and Language,FOS: Computer and information sciences},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\X8K8BLEW\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@article{brysbaert:2019:how-many-words,
  title = {How Many Words Do We Read per Minute? {{A}} Review and Meta-Analysis of Reading Rate},
  shorttitle = {How Many Words Do We Read per Minute?},
  author = {Brysbaert, Marc},
  date = {2019-12},
  journaltitle = {Journal of Memory and Language},
  volume = {109},
  pages = {104047},
  issn = {0749596X},
  doi = {10.1016/j.jml.2019.104047},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X19300786},
  urldate = {2023-06-27},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\6HQS5M2Y\Brysbaert - 2019 - How many words do we read per minute A review and.pdf}
}

@inproceedings{callison-burch-dredze:2010:creating-speech-language,
  title = {Creating Speech and Language Data with {{Amazon}}'s {{Mechanical Turk}}},
  booktitle = {Proceedings of the {{NAACL HLT}} 2010 Workshop on Creating Speech and Language Data with {{Amazon}}'s Mechanical Turk},
  author = {Callison-Burch, Chris and Dredze, Mark},
  date = {2010-06},
  pages = {1--12},
  publisher = {{Association for Computational Linguistics}},
  location = {{Los Angeles}},
  url = {https://aclanthology.org/W10-0701},
  abstract = {In this paper we give an introduction to using Amazon’s Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies. We survey the papers published in the NAACL2010 Workshop. 24 researchers participated in the workshop’s shared task to create data for speech and language applications with \$100.},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\PZWDA5T5\Callison-Burch et Dredze - 2010 - Creating speech and language data with Amazon's Me.pdf}
}

@inproceedings{candito-seddah:2012:corpus-sequoia-annotation,
  title = {Le Corpus {{Sequoia}} : Annotation Syntaxique et Exploitation Pour l'adaptation d'analyseur Par Pont Lexical},
  booktitle = {{{TALN}} 2012 - 19e Conférence Sur Le {{Traitement Automatique}} Des {{Langues Naturelles}}},
  author = {Candito, Marie and Seddah, Djamé},
  date = {2012-06},
  location = {{Grenoble, France}},
  url = {https://inria.hal.science/hal-00698938},
  abstract = {Nous présentons dans cet article la méthodologie de constitution et les caractéristiques du corpus Sequoia, un corpus en français, syntaxiquement annoté d’après un schéma d’annotation très proche de celui du French Treebank (Abeillé et Barrier, 2004), et librement disponible, en constituants et en dépendances. Le corpus comporte des phrases de quatre origines : Europarl français, le journal l’Est Républicain, Wikipédia Fr et des documents de l’Agence Européenne du Médicament, pour un total de 3204 phrases et 69246 tokens. En outre, nous présentons une application de ce corpus : l’évaluation d’une technique d’adaptation d’analyseurs syntaxiques probabilistes à des domaines et/ou genres autres que ceux du corpus sur lequel ces analyseurs sont entraînés. Cette technique utilise des clusters de mots obtenus d’abord par regroupement morphologique à l’aide d’un lexique, puis par regroupement non supervisé, et permet une nette amélioration de l’analyse des domaines cibles (le corpus Sequoia), tout en préservant le même niveau de performance sur le domaine source (le FTB), ce qui fournit un analyseur multi-domaines, à la différence d’autres techniques d’adaptation comme le self-training.},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\KZGE6R98\Candito et Seddah - 2012 - Le corpus Sequoia  annotation syntaxique et explo.pdf}
}

@article{chen-etal:2017:survey-dialogue-systems,
  title = {A {{Survey}} on {{Dialogue Systems}}: {{Recent Advances}} and {{New Frontiers}}},
  shorttitle = {A {{Survey}} on {{Dialogue Systems}}},
  author = {Chen, Hongshen and Liu, Xiaorui and Yin, Dawei and Tang, Jiliang},
  date = {2017-11-21},
  journaltitle = {SIGKDD Explor. Newsl.},
  volume = {19},
  number = {2},
  pages = {25--35},
  issn = {1931-0145, 1931-0153},
  doi = {10.1145/3166054.3166058},
  url = {https://dl.acm.org/doi/10.1145/3166054.3166058},
  urldate = {2023-07-26},
  abstract = {Dialogue systems have attracted more and more attention. Recent advances on dialogue systems are overwhelmingly contributed by deep learning techniques, which have been employed to enhance a wide range of big data applications such as computer vision, natural language processing, and recommender systems. For dialogue systems, deep learning can leverage a massive amount of data to learn meaningful feature representations and response generation strategies, while requiring a minimum amount of hand-crafting. In this article, we give an overview to these recent advances on dialogue systems from various perspectives and discuss some possible research directions. In particular, we generally divide existing dialogue systems into task-oriented and nontask- oriented models, then detail how deep learning techniques help them with representative algorithms and finally discuss some appealing research directions that can bring the dialogue system research into a new frontier},
  langid = {english},
  keywords = {*ALIRE/AIMPLÉMENTER,*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\S2RHDJ72\Chen et al. - 2017 - A Survey on Dialogue Systems Recent Advances and .pdf}
}

@article{clemmensen-kjaersgaard:2022:data-representativity-machine,
  title = {Data {{Representativity}} for {{Machine Learning}} and {{AI Systems}}},
  author = {Clemmensen, Line H. and Kjaersgaard, Rune D.},
  date = {2022},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.2203.04706},
  url = {https://arxiv.org/abs/2203.04706},
  urldate = {2023-10-03},
  abstract = {Data representativity is crucial when drawing inference from data through machine learning models. Scholars have increased focus on unraveling the bias and fairness in models, also in relation to inherent biases in the input data. However, limited work exists on the representativity of samples (datasets) for appropriate inference in AI systems. This paper reviews definitions and notions of a representative sample and surveys their use in scientific AI literature. We introduce three measurable concepts to help focus the notions and evaluate different data samples. Furthermore, we demonstrate that the contrast between a representative sample in the sense of coverage of the input space, versus a representative sample mimicking the distribution of the target population is of particular relevance when building AI systems. Through empirical demonstrations on US Census data, we evaluate the opposing inherent qualities of these concepts. Finally, we propose a framework of questions for creating and documenting data with data representativity in mind, as an addition to existing dataset documentation templates.},
  keywords = {*CITATION,Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\JMT5BBA7\Clemmensen et Kjaersgaard - 2022 - Data Representativity for Machine Learning and AI .pdf}
}

@inbook{collins:2017:chapter-overfitting,
  title = {Chapter 7: {{Overfitting}}},
  booktitle = {Algorithms {{To Live By}}: {{The}} Computer Science of Human Decisions},
  author = {Collins, William},
  date = {2017-04},
  pages = {149--168},
  bookauthor = {Christian, Brian and Griffiths, Tom},
  isbn = {978-0-00-754799-9},
  keywords = {*CITATION}
}

@article{cortes-vapnik:1995:supportvector-networks,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  date = {1995-09},
  journaltitle = {Mach Learn},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00994018},
  url = {http://link.springer.com/10.1007/BF00994018},
  urldate = {2023-09-18},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\FEREHLHS\Cortes et Vapnik - 1995 - Support-vector networks.pdf}
}

@article{costello-lodolce:2019:gartner-top-technologies,
  entrysubtype = {newspaper},
  title = {Gartner {{Top Technologies}} and {{Trends Driving}} the {{Digital Workplace}}},
  author = {Costello, Katie and LoDolce, Matt},
  date = {2019-03-18},
  journaltitle = {Gartner, Inc},
  url = {https://www.gartner.com/smarterwithgartner/top-10-technologies-driving-the-digital-workplace/},
  urldate = {2020-10-23},
  abstract = {How artificial intelligence, smart workspaces and talent markets will boost employee digital dexterity in future digital workplaces.},
  langid = {american},
  keywords = {*CITATION}
}

@article{costello-lodolce:2022:gartner-predicts-chatbots,
  entrysubtype = {newspaper},
  title = {Gartner {{Predicts Chatbots Will Become}} a {{Primary Customer Service Channel Within Five Years}}},
  author = {Costello, Katie and LoDolce, Matt},
  date = {2022-07},
  journaltitle = {Gartner, Inc},
  url = {https://www.gartner.com/en/newsroom/press-releases/2022-07-27-gartner-predicts-chatbots-will-become-a-primary-customer-service-channel-within-five-years},
  urldate = {2023-10-09},
  abstract = {Chatbot Investment on the Rise but Low ROI and Other Challenges Persist},
  langid = {american},
  keywords = {*CITATION}
}

@standard{creative-commons:2013:cc-bync-legal,
  type = {Licence},
  title = {{{CC BY-NC}} 4.0 {{LEGAL CODE}} - {{Attribution-NonCommercial}} 4.0 {{International}}},
  author = {{Creative commons}},
  date = {2013},
  number = {BY-NC},
  url = {https://creativecommons.org/licenses/by-nc/4.0/legalcode.en},
  urldate = {2023-09-29},
  abstract = {Creative Commons Corporation ("Creative Commons") is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an "as-is" basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.},
  version = {4.0},
  keywords = {*CITATION}
}

@software{cvat.ai-corporation:2019:computer-vision-annotation,
  title = {Computer {{Vision Annotation Tool}} ({{CVAT}})},
  author = {{CVAT.Ai Corporation}},
  date = {2019-10-17},
  url = {https://www.cvat.ai/},
  urldate = {2023-09-27},
  abstract = {Annotate better with CVAT, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale.},
  organization = {{CVAT.ai Corporation}},
  keywords = {*CITATION,Annotation,Annotation-tool,computer-vision,computer-vision-annotation,Deep learning,Image-classification,Image-labeling,Image-labeling-tool,labeling-tool,object-detection,Semantic-segmentation,video-annotation}
}

@inproceedings{dagan-etal:2005:pascal-recognising-textual,
  title = {The {{PASCAL Recognising Textual Entailment Challenge}}},
  booktitle = {Machine {{Learning Challenges}}. {{Evaluating Predictive Uncertainty}}, {{Visual Object Classification}}, and {{Recognising Tectual Entailment}}},
  author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  date = {2005-01},
  volume = {3944},
  pages = {177--190},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11736790_9},
  url = {http://link.springer.com/10.1007/11736790_9},
  abstract = {This paper describes the Second PASCAL Recognising Textual Entailment Challenge (RTE-2).1 We describe the RTE2 dataset and overview the submissions for the challenge. One of the main goals for this year’s dataset was to provide more “realistic” text-hypothesis examples, based mostly on outputs of actual systems. The 23 submissions for the challenge present diverse approaches and research directions, and the best results achieved this year are considerably higher than last year’s state of the art.},
  isbn = {978-3-540-33427-9},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\NXZHY47L\Dagan et al. - 2006 - The PASCAL Recognising Textual Entailment Challeng.pdf}
}

@inproceedings{dandapat-etal:2009:complex-linguistic-annotation,
  title = {Complex Linguistic Annotation – No Easy Way out! {{A}} Case from {{Bangla}} and {{Hindi POS}} Labeling Tasks},
  booktitle = {Proceedings of the Third Linguistic Annotation Workshop ({{LAW III}})},
  author = {Dandapat, Sandipan and Biswas, Priyanka and Choudhury, Monojit and Bali, Kalika},
  date = {2009-08},
  pages = {10--18},
  publisher = {{Association for Computational Linguistics}},
  location = {{Suntec, Singapore}},
  url = {https://aclanthology.org/W09-3002},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\GX7YEZSW\Dandapat et al. - 2009 - Complex linguistic annotation – no easy way out! A.pdf}
}

@online{databird:2023:10-metiers-data,
  title = {Les 10 métiers data les plus recherchés en 2023},
  author = {{DataBird}},
  date = {2023-07},
  url = {https://www.data-bird.co/blog/metiers-data},
  urldate = {2023-09-26},
  abstract = {En 2023, l’analyse de données est un enjeu crucial pour les entreprises. Comment valoriser les nombreuses données qu’elles génèrent et qu’elles récoltent ? C’est là qu’interviennent les professionnels de la data. Cet article te décrit les 10 métiers les plus recherchés dans l’univers de la Data science.},
  langid = {french},
  organization = {{DataBird}},
  keywords = {*CITATION}
}

@article{davidson-ravi:2005:agglomerative-hierarchical-clustering,
  title = {Agglomerative {{Hierarchical Clustering}} with {{Constraints}}: {{Theoretical}} and {{Empirical Results}}},
  shorttitle = {Agglomerative {{Hierarchical Clustering}} with {{Constraints}}},
  author = {Davidson, Ian and Ravi, S. S.},
  editor = {Jorge, Alípio Mário and Torgo, Luís and Brazdil, Pavel and Camacho, Rui and Gama, João},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {collaborator},
  date = {2005},
  journaltitle = {Knowledge Discovery in Databases: PKDD 2005},
  volume = {3721},
  pages = {59--70},
  url = {http://link.springer.com/10.1007/11564126_11},
  urldate = {2020-10-22},
  abstract = {We explore the use of instance and cluster-level constraints with agglomerative hierarchical clustering. Though previous work has illustrated the benefits of using constraints for non-hierarchical clustering, their application to hierarchical clustering is not straight-forward for two primary reasons. First, some constraint combinations make the feasibility problem (Does there exist a single feasible solution?) NP-complete. Second, some constraint combinations when used with traditional agglomerative algorithms can cause the dendrogram to stop prematurely in a dead-end solution even though there exist other feasible solutions with a significantly smaller number of clusters. When constraints lead to efficiently solvable feasibility problems and standard agglomerative algorithms do not give rise to dead-end solutions, we empirically illustrate the benefits of using constraints to improve cluster purity and average distortion. Furthermore, we introduce the new γ constraint and use it in conjunction with the triangle inequality to considerably improve the efficiency of agglomerative clustering.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Constrained clustering,Hierarchcal clustering},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\QEHQH3QT\Davidson et Ravi - 2005 - Agglomerative Hierarchical Clustering with Constra.pdf}
}

@article{devlin-etal:2019:bert-pretraining-deep,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  journaltitle = {ArXiv preprint},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2020-06-10},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  keywords = {*CITATION,BERT,Computation and Language,Transformers},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\E53CNC5D\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@article{diamond-etal:1990:analysis-binary-data,
  title = {Analysis of {{Binary Data}}. 2nd {{Edn}}.},
  author = {Diamond, Ian and Cox, D. R. and Snell, E. J.},
  date = {1990},
  journaltitle = {Applied Statistics},
  volume = {39},
  number = {2},
  eprint = {10.2307/2347766},
  eprinttype = {jstor},
  pages = {260},
  issn = {00359254},
  doi = {10.2307/2347766},
  url = {https://www.jstor.org/stable/10.2307/2347766?origin=crossref},
  urldate = {2023-07-06},
  keywords = {*CITATION,R²}
}

@inproceedings{dipper-etal:2004:useradaptive-annotation-guidelines,
  title = {Towards {{User-Adaptive Annotation Guidelines}}},
  booktitle = {Proceedings of the 5th International Workshop on Linguistically Interpreted Corpora},
  author = {Dipper, Stefanie and Gotze, Michael and Skopeteas, Stavros},
  date = {2004-08-28},
  pages = {23--30},
  publisher = {{COLING}},
  url = {https://aclanthology.org/W04-1904},
  abstract = {In this paper we address the issue of useradaptivity for annotation guidelines. We show that different user groups have different needs towards these documents, a fact neglected by most of current annotation guidelines. We propose a formal specification of the structure of annotation guidelines, thus suggesting a minimum set of requirements that guidelines should fulfill. Finally, we sketch the use of these specifications by exemplary applications, resulting in user-specific guideline representations.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\MUFUET4M\Dipper et al. - Towards User-Adaptive Annotation Guidelines.pdf}
}

@article{dzieza:2023:ai-lot-work,
  entrysubtype = {newspaper},
  title = {{{AI}} Is a Lot of Work},
  author = {Dzieza, Josh},
  date = {2023-06-20},
  journaltitle = {New York Magazine},
  url = {https://nymag.com/intelligencer/article/ai-artificial-intelligence-humans-technology-business-factory.html},
  urldate = {2023-09-29},
  abstract = {As the technology becomes ubiquitous, a vast tasker underclass is emerging — and not going anywhere},
  journalsubtitle = {Artificial Intelligence},
  langid = {english},
  keywords = {*CITATION}
}

@inproceedings{eckart-de-castilho-etal:2016:webbased-tool-integrated,
  title = {A Web-Based Tool for the Integrated Annotation of Semantic and Syntactic Structures},
  booktitle = {Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities ({{LT4DH}})},
  author = {Eckart de Castilho, Richard and Mújdricza-Maydt, Éva and Yimam, Seid Muhie and Hartmann, Silvana and Gurevych, Iryna and Frank, Anette and Biemann, Chris},
  date = {2016-12},
  pages = {76--84},
  publisher = {{The COLING 2016 Organizing Committee}},
  location = {{Osaka, Japan}},
  url = {https://webanno.github.io/webanno/},
  abstract = {We introduce the third major release of WebAnno, a generic web-based annotation tool for distributed teams. New features in this release focus on semantic annotation tasks (e.g. semantic role labelling or event annotation) and allow the tight integration of semantic annotations with syntactic annotations. In particular, we introduce the concept of slot features, a novel constraint mechanism that allows modelling the interaction between semantic and syntactic annotations, as well as a new annotation user interface. The new features were developed and used in an annotation project for semantic roles on German texts. The paper briefly introduces this project and reports on experiences performing annotations with the new tool. On a comparative evaluation, our tool reaches significant speedups over WebAnno 2 for a semantic annotation task.},
  langid = {english},
  keywords = {*CITATION}
}

@book{edwards:1992:likelihood,
  title = {Likelihood},
  author = {Edwards, Anthony William Fairbank},
  date = {1992},
  edition = {Expanded ed},
  publisher = {{Johns Hopkins Univ. Press}},
  location = {{Baltimore}},
  isbn = {978-0-8018-4445-4 978-0-8018-4443-0},
  langid = {english},
  pagetotal = {275},
  keywords = {*CITATION,Log vraissemblance,Metric}
}

@article{elkosantini-gien:2009:integration-human-behavioural,
  title = {Integration of Human Behavioural Aspects in a Dynamic Model for a Manufacturing System},
  author = {Elkosantini, S. and Gien, D.},
  date = {2009-05-15},
  journaltitle = {International Journal of Production Research},
  volume = {47},
  number = {10},
  pages = {2601--2623},
  issn = {0020-7543, 1366-588X},
  doi = {10.1080/00207540701663490},
  url = {https://www.tandfonline.com/doi/full/10.1080/00207540701663490},
  urldate = {2023-06-07},
  langid = {english},
  keywords = {*CITATION,Ergonomie,Fatigue des opérateurs},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\BVES3475\Elkosantini et Gien - 2009 - Integration of human behavioural aspects in a dyna.pdf}
}

@thesis{eshkol-taravella:2015:definition-annotations-linguistiques,
  type = {HDR},
  title = {La définition des annotations linguistiques selon les corpus: de l'écrit journalistique à l'oral},
  author = {Eshkol-Taravella, Iris},
  date = {2015-10-16},
  institution = {{Université d'Orléans}},
  url = {https://hal.science/tel-01250650},
  abstract = {Confronté à Internet, le Traitement Automatique des Langues (TAL) a dû relever le défi que posait l’analyse de textes dialogiques écrits (blog, forum, chat, réseaux sociaux etc.) et oraux. Les recherches présentées ont, dans un premier temps, porté sur le développement de systèmes à même de repérer et d’analyser l’information à partir d’une annotation des ressources. L’approche retenue privilégie l’intégration d’indices inhérents à la nature de corpus « hors normes » afin d’améliorer les techniques de traitement automatique. La chaîne d’opérations comprend quatre étapes : (i) L’observation et l’analyse manuelle des données afin de recenser les variations dans les occurrences et d’évaluer l’ampleur des phénomènes à annoter, leur classification et l’identification de leurs marqueurs formels. (ii) La modélisation de l’information à partir d’une typologie sous la forme d’un jeu d’étiquettes ajusté à la nature du corpus. (iii) La définition de la technologie congrue (généralement, l’arbitrage entre le développement d’un nouvel outil et l’adaptation d’un outil existant). (iv) L’implémentation du schéma d’annotation défini afin de procéder à une analyse quantitative et qualitative des résultats. L’annotation effectuée concerne les domaines de la syntaxe (étiquetage morpho-syntaxique et chunking), sémantique et/ou pragmatique (entités nommées, indices d’identification de la personne, reformulations etc.). L’application concerne aussi bien des entretiens transcrits que des titres de cartes géographiques, des recettes d’omelette que des articles du Monde. Les méthodes utilisées varient en fonction du corpus et de la tâche traitée. L’annotation syntaxique et le repérage des segments reformulés sont fondés sur la technique d’apprentissage automatique avec les CRFs ; le repérage des entités nommées et des indices d’identification de la personne dans les transcriptions de l’oral utilise les méthodes symboliques ; la détection automatique des tours de parole contenant la reformulation emploie les méthodes heuristiques. Le travail sur le français parlé et son annotation a conduit à la modélisation des caractéristiques propres à l’oral : disfluences, marqueurs discursifs, présentateurs, segmentation, commentaires personnels etc. Un autre phénomène caractéristique de l’oral, la reformulation, a fait l’objet d’une étude particulière. Le travail sur l’annotation du corpus oral, du corpus Web ou du corpus médiatique a permis de reconsidérer la notion de subjectivité qui constitue l’une des difficultés récurrentes du traitement automatique. L’étude de la subjectivité et son expression dans le discours a été poursuivie dans plusieurs des recherches menées : la subjectivité à partir des informations personnelles livrées par le locuteur, la subjectivité dans la perception et l’appropriation des lieux, la subjectivité dans les recettes de cuisine et enfin la subjectivité exprimée à travers les noms généraux.},
  langid = {french},
  keywords = {*CITATION,Annotation},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\UHWB7Y5D\Eshkol-Taravella - La définition des annotations linguistiques selon .pdf}
}

@legislation{european-commission:2016:regulation-eu-2016,
  title = {Regulation ({{EU}}) 2016/679 of the {{European Parliament}} and of the {{Council}} of 27 {{April}} 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing {{Directive}} 95/46/{{EC}} ({{General Data Protection Regulation}}) ({{Text}} with {{EEA}} Relevance)},
  editora = {{European Commission}},
  editoratype = {collaborator},
  date = {2016-05-04},
  journaltitle = {32016R0679},
  volume = {OJ L 119},
  number = {2016/679},
  url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32016R0679},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\Q86DQLZD\European Commission - 2016 - Regulation (EU) 2016679 of the European Parliamen.pdf}
}

@legislation{european-commission:2021:proposal-regulation-european,
  title = {Proposal for a {{Regulation}} of the {{European Parliament}} and the {{Council}} Laying down Harmonised Rules on {{Artificial Intelligence}} ({{Artificial Intelligence Act}}) and Amending Certain {{Union Legislative Acts}}},
  shorttitle = {The {{Artificial Intelligence Act}}},
  editora = {{European Commission}},
  editoratype = {collaborator},
  date = {2021-04-21},
  journaltitle = {52021PC0206},
  number = {2021/0106 (COD)},
  url = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\74X9BPTL\European Commission - 2021 - Proposal for a Regulation of the European Parliame.pdf}
}

@inproceedings{falke-etal:2019:ranking-generated-summaries,
  title = {Ranking {{Generated Summaries}} by {{Correctness}}: {{An Interesting}} but {{Challenging Application}} for {{Natural Language Inference}}},
  shorttitle = {Ranking {{Generated Summaries}} by {{Correctness}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Falke, Tobias and Ribeiro, Leonardo F. R. and Utama, Prasetya Ajie and Dagan, Ido and Gurevych, Iryna},
  date = {2019},
  pages = {2214--2220},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1213},
  url = {https://www.aclweb.org/anthology/P19-1213},
  urldate = {2023-08-24},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\T4IGJ5QQ\Falke et al. - 2019 - Ranking Generated Summaries by Correctness An Int.pdf}
}

@article{finlayson-erjavec:2016:overview-annotation-creation,
  title = {Overview of {{Annotation Creation}}: {{Processes}} \& {{Tools}}},
  shorttitle = {Overview of {{Annotation Creation}}},
  author = {Finlayson, Mark A. and Erjavec, Tomaž},
  date = {2016-02-18},
  journaltitle = {ArXiv preprint},
  url = {http://arxiv.org/abs/1602.05753},
  urldate = {2021-06-14},
  abstract = {Creating linguistic annotations requires more than just a reliable annotation scheme. Annotation can be a complex endeavour potentially involving many people, stages, and tools. This chapter outlines the process of creating end-to-end linguistic annotations, identifying specific tasks that researchers often perform. Because tool support is so central to achieving high quality, reusable annotations with low cost, the focus is on identifying capabilities that are necessary or useful for annotation tools, as well as common problems these tools present that reduce their utility. Although examples of specific tools are provided in many cases, this chapter concentrates more on abstract capabilities and problems because new tools appear continuously, while old tools disappear into disuse or disrepair. The two core capabilities tools must have are support for the chosen annotation scheme and the ability to work on the language under study. Additional capabilities are organized into three categories: those that are widely provided; those that often useful but found in only a few tools; and those that have as yet little or no available tool support.},
  keywords = {*CITATION,Computation and Language,Human-Computer Interaction},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\YGEMRHIK\Finlayson and Erjavec - 2016 - Overview of Annotation Creation Processes & Tools.pdf}
}

@inproceedings{fiumara-etal:2020:languagearc-developing-language,
  title = {{{LanguageARC}}: {{Developing}} Language Resources through Citizen Linguistics},
  booktitle = {Proceedings of the {{LREC}} 2020 Workshop on “{{Citizen}} Linguistics in Language Resource Development”},
  author = {Fiumara, James and Cieri, Christopher and Wright, Jonathan and Liberman, Mark},
  date = {2020-05},
  pages = {1--6},
  publisher = {{European Language Resources Association}},
  location = {{Marseille, France}},
  url = {https://aclanthology.org/2020.cllrd-1.1},
  abstract = {This paper introduces the citizen science platform, LanguageARC, developed within the NIEUW (Novel Incentives and Workflows) project supported by the National Science Foundation under Grant No. 1730377. LanguageARC is a community-oriented online platform bringing together researchers and “citizen linguists” with the shared goal of contributing to linguistic research and language technology development. Like other Citizen Science platforms and projects, LanguageARC harnesses the power and efforts of volunteers who are motivated by the incentives of contributing to science, learning and discovery, and belonging to a community dedicated to social improvement. Citizen linguists contribute language data and judgments by participating in research tasks such as classifying regional accents from audio clips, recording audio of picture descriptions and answering personality questionnaires to create baseline data for NLP research into autism and neurodegenerative conditions. Researchers can create projects on Language ARC without any coding or HTML required using our Project Builder Toolkit.},
  isbn = {979-10-95546-59-7},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\CKSNA5YF\Fiumara et al. - 2020 - LanguageARC Developing language resources through.pdf}
}

@inproceedings{fort-etal:2009:vers-methodologie-annotation,
  title = {Vers Une Méthodologie d'annotation Des Entités Nommées En Corpus ?},
  booktitle = {Traitement Automatique Des Langues Naturelles 2009},
  author = {Fort, Karen and Ehrmann, Maud and Nazarenko, Adeline},
  date = {2009-06},
  location = {{Senlis, France}},
  url = {https://hal.science/hal-00402321},
  keywords = {*CITATION,Annotation,Named entities extraction},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\UAD26IZX\Fort et al. - 2009 - Vers une méthodologie d'annotation des entités nom.pdf}
}

@inproceedings{fort-etal:2012:modeling-complexity-manual,
  title = {Modeling the Complexity of Manual Annotation Tasks: A Grid of Analysis},
  booktitle = {Proceedings of {{COLING}} 2012},
  author = {Fort, Karen and Nazarenko, Adeline and Rosset, Sophie},
  date = {2012-12},
  pages = {895--910},
  publisher = {{The COLING 2012 Organizing Committee}},
  location = {{Mumbaï, India}},
  url = {https://hal.science/hal-00769631},
  abstract = {Manual corpus annotation is getting widely used in Natural Language Processing (NLP). While being recognized as a difficult task, no in-depth analysis of its complexity has been performed yet. We provide in this article a grid of analysis of the different complexity dimensions of an annotation task, which helps estimating beforehand the difficulties and cost of annotation campaigns. We observe the applicability of this grid on existing annotation campaigns and detail its application on a real-world example.},
  langid = {english},
  keywords = {*CITATION,Annotation campaign cost estimate,Annotation campaign management,Manual corpus annotation},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\NCX95JS3\Fort et al. - 2012 - Modeling the complexity of manual annotation tasks.pdf}
}

@inproceedings{fort-sagot:2010:influence-preannotation-postagged,
  title = {Influence of Pre-Annotation on {{POS-Tagged}} Corpus Development},
  booktitle = {Proceedings of the Fourth Linguistic Annotation Workshop},
  author = {Fort, Karën and Sagot, Benoît},
  date = {2010-07},
  pages = {56--63},
  publisher = {{Association for Computational Linguistics}},
  location = {{Uppsala, Sweden}},
  url = {https://aclanthology.org/W10-1807},
  abstract = {This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a notso accurate tagger can help improving annotation speed},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\W6CEI465\Fort et Sagot - 2010 - Influence of pre-annotation on POS-Tagged corpus d.pdf}
}

@article{fort:2017:experts-ou-foule,
  title = {Experts Ou (Foule de) Non-Experts ? La Question de l’expertise Des Annotateurs Vue de La Myriadisation (Crowdsourcing)},
  shorttitle = {Experts Ou (Foule de) Non-Experts ?},
  author = {Fort, Karën},
  date = {2017-01-19},
  journaltitle = {corela},
  issn = {1638-573X},
  doi = {10.4000/corela.4835},
  url = {http://journals.openedition.org/corela/4835},
  urldate = {2023-09-21},
  issue = {HS-21},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\XJB2GERR\Fort - 2017 - Experts ou (foule de) non-experts  la question de.pdf}
}

@article{gancarski-wemmert:2007:collaborative-multistep-monolevel,
  title = {Collaborative Multi-Step Mono-Level Multi-Strategy Classification},
  author = {Gançarski, Pierre and Wemmert, Cédric},
  date = {2007-08-30},
  journaltitle = {Multimed Tools Appl},
  volume = {35},
  number = {1},
  pages = {1--27},
  issn = {1380-7501},
  doi = {10.1007/s11042-007-0115-x},
  url = {https://univoak.eu/islandora/object/islandora:96302},
  abstract = {This article deals with the description of a new way to learn from multiple and heterogeneous data sets, and with the integration of this method in a multi-agent hybrid learning system. This system integrates different kinds of unsupervised classification methods and gives a set of clusterings as the result and a unifying result, representing all the other one. In this new approach, the method occurrences compare their results and automatically refine them to try to make them converge towards a unique clustering that unifies all the results. Thus, the data are not really merged but the results from their classification are compared and refined according to the results from all the other data sets. This enables to produce a set of classification hierarchies which classes are very similar, although these hierarchies were extracted from different data sets. Then it is easy to build a unifying result from all of them.},
  langid = {english},
  keywords = {*CITATION,Clustering,Collaborative clustering,Constrained clustering,INTERACTIVE\_CLUSTERING},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\CFXDP786\Gançarski and Wemmert - 2007 - Collaborative multi-step mono-level multi-strategy.pdf}
}

@book{garside-etal:1997:corpus-annotation-linguistic,
  title = {Corpus Annotation: Linguistic Information from Computer Text Corpora},
  shorttitle = {Corpus Annotation},
  editor = {Garside, Roger and Leech, Geoffrey N. and McEnery, Tony},
  date = {1997},
  publisher = {{Longman}},
  location = {{London \& New York}},
  abstract = {Corpus Annotation gives an up-to-date picture of this fascinating new area of research, and will provide essential reading for newcomers to the field as well as those already involved in corpus annotation. Early chapters introduce the different levels and techniques of corpus annotation. Later chapters deal with software developments, applications, and the development of standards for the evaluation of corpus annotation. While the book takes detailed account of research world-wide, its focus is particularly on the work of the UCREL (University Centre for Computer Corpus Research on Language) team at Lancaster University, which has been at the forefront of developments in the field of corpus annotation since its beginnings in the 1970s.},
  isbn = {978-0-582-29837-8},
  langid = {english},
  pagetotal = {281},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\QWV7ZQE2\Garside et al. - 1997 - Corpus annotation linguistic information from com.pdf}
}

@book{girden:1992:anova,
  title = {{{ANOVA}}},
  author = {Girden, Ellen},
  date = {1992},
  publisher = {{SAGE Publications, Inc.}},
  location = {{2455 Teller Road,~Thousand Oaks~California~91320~United States of America}},
  url = {https://methods.sagepub.com/book/anova},
  urldate = {2023-07-06},
  isbn = {978-0-8039-4257-8 978-1-4129-8341-9},
  keywords = {*CITATION}
}

@article{givoni-frey:2009:semisupervised-affinity-propagation,
  title = {Semi-{{Supervised Aﬃnity Propagation}} with {{Instance-Level Constraints}}},
  author = {Givoni, Inmar E and Frey, Brendan J},
  date = {2009},
  abstract = {Recently, affinity propagation (AP) was introduced as an unsupervised learning algorithm for exemplar based clustering. Here we extend the AP model to account for semisupervised clustering. AP, which is formulated as inference in a factor-graph, can be naturally extended to account for ‘instancelevel’ constraints: pairs of data points that cannot belong to the same cluster (cannotlink), or must belong to the same cluster (must-link). We present a semi-supervised AP algorithm (SSAP) that can use instancelevel constraints to guide the clustering. We demonstrate the applicability of SSAP to interactive image segmentation by using SSAP to cluster superpixels while taking into account user instructions regarding which superpixels belong to the same object. We demonstrate SSAP can achieve better performance compared to other semi-supervised methods.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Affinity propagation,Clustering,Constrained clustering},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\SMP6WJN6\Givoni et Frey - 2009 - Semi-Supervised Aﬃnity Propagation with Instance-L.pdf}
}

@article{goasduff:2019:chatbots-will-appeal,
  entrysubtype = {newspaper},
  title = {Chatbots {{Will Appeal}} to {{Modern Workers}}},
  author = {Goasduff, Laurence},
  date = {2019-07-31},
  journaltitle = {Gartner, Inc},
  url = {https://www.gartner.com/smarterwithgartner/chatbots-will-appeal-to-modern-workers/},
  urldate = {2020-10-23},
  abstract = {The proliferation of chatbots in the modern workplace calls for IT leaders to create a conversational platform strategy that ensures an effective solution for employees, customers and key partners.},
  langid = {american},
  keywords = {*CITATION}
}

@software{google:2016:google-assistant-your,
  title = {Google Assistant, Your Own Personal Google},
  author = {{Google}},
  date = {2016},
  url = {https://assistant.google.com/},
  organization = {{Google}},
  keywords = {*CITATION}
}

@software{google:2023:bard-chat-based,
  title = {Bard - Chat Based {{AI}} Tool from {{Google}}},
  author = {{Google}},
  date = {2023-07-13},
  url = {https://bard.google.com/chat},
  keywords = {*CITATION}
}

@article{goyal-etal:2018:recent-named-entity,
  title = {Recent {{Named Entity Recognition}} and {{Classification}} Techniques: {{A}} Systematic Review},
  shorttitle = {Recent {{Named Entity Recognition}} and {{Classification}} Techniques},
  author = {Goyal, Archana and Gupta, Vishal and Kumar, Manish},
  date = {2018-08},
  journaltitle = {Computer Science Review},
  volume = {29},
  pages = {21--43},
  issn = {15740137},
  doi = {10.1016/j.cosrev.2018.06.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013717302782},
  urldate = {2023-09-18},
  abstract = {Textual information is becoming available in abundance on the web, arising the requirement of techniques and tools to extract the meaningful information. One of such an important information extraction task is Named Entity Recognition and Classification. It is the problem of finding the members of various predetermined classes, such as person, organization, location, date/time, quantities, numbers etc. The concept of named entity extraction was first proposed in Sixth Message Understanding Conference in 1996. Since then, a number of techniques have been developed by many researchers for extracting diversity of entities from different languages and genres of text. Still, there is a growing interest among research community to develop more new approaches to extract diverse named entities which are helpful in various natural language applications. Here we present a survey of developments and progresses made in Named Entity Recognition and Classification research.},
  langid = {english},
  keywords = {*CITATION}
}

@inproceedings{guillaume-etal:2016:crowdsourcing-complex-language,
  title = {Crowdsourcing Complex Language Resources: {{Playing}} to Annotate Dependency Syntax},
  booktitle = {International Conference on Computational Linguistics ({{COLING}})},
  author = {Guillaume, Bruno and Fort, Karën and Lefèbvre, Nicolas},
  date = {2016-12},
  series = {Proceedings of the 26th International Conference on Computational Linguistics ({{COLING}})},
  location = {{Osaka, Japan}},
  url = {https://inria.hal.science/hal-01378980},
  abstract = {This article presents the results we obtained on a complex annotation task (that of dependency syntax) using a specifically designed Game with a Purpose, ZombiLingo. We show that with suitable mechanisms (decomposition of the task, training of the players and regular control of the annotation quality during the game), it is possible to obtain annotations whose quality is significantly higher than that obtainable with a parser, provided that enough players participate. The source code of the game and the resulting annotated corpora (for French) are freely available.},
  keywords = {*CITATION,Annotation,crowdsourcing,game with a purpose},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\LDHE9V7J\Guillaume et al. - 2016 - Crowdsourcing complex language resources Playing .pdf}
}

@article{gupta-gupta:2019:dealing-noise-problem,
  title = {Dealing with {{Noise Problem}} in {{Machine Learning Data-sets}}: {{A Systematic Review}}},
  shorttitle = {Dealing with {{Noise Problem}} in {{Machine Learning Data-sets}}},
  author = {Gupta, Shivani and Gupta, Atul},
  date = {2019},
  journaltitle = {Procedia Computer Science},
  volume = {161},
  pages = {466--474},
  issn = {18770509},
  doi = {10.1016/j.procs.2019.11.146},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050919318575},
  urldate = {2023-10-04},
  abstract = {The occurrences of noisy data in data set can significantly impact prediction of any meaningful information. Many empirical studies have shown that noise in data set dramatically led to decreased classification accuracy and poor prediction results. Therefore, the problem of identifying and handling noise in prediction application has drawn considerable attention over past many years. In our study, we performed a systematic literature review of noise identification and handling studies published in various conferences and journals between January 1993 to July 2018. We have identified 79 primary studies are of noise identification and noise handling techniques. After investigating these studies, we found that among the noise identification schemes, the accuracy of identification of noisy instances by using ensemble-based techniques are better than other techniques. But regarding efficiency, usually single based techniques method is better; it is more suitable for noisy data sets. Among noise handling techniques, polishing techniques generally improve classification accuracy than filtering and robust techniques, but it introduced some errors in the data sets.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\BDBRFCBH\Gupta et Gupta - 2019 - Dealing with Noise Problem in Machine Learning Dat.pdf}
}

@article{gut-bayerl:2004:measuring-reliability-manual,
  title = {Measuring the {{Reliability}} of {{Manual Annotations}} of {{Speech Corpora}}},
  author = {Gut, Ulrike and Bayerl, Petra Saskia},
  date = {2004-01},
  url = {https://api.semanticscholar.org/CorpusID:27970161},
  abstract = {The quality of manual annotations of speech corpora depends on the ability of human annotators to cope with phonetic and prosodic coding schemas such as SAMPA and ToBI. It has been proposed widely that an acceptable amount of reliability among and within individual annotators is impossible to achieve. In this paper, we present an extensive evaluation of annotator reliability in a multilevel phonetically annotated speech corpus, using two methods for measuring annotator reliability. The results show that manual annotations can be very reliable, but that reliability is correlated with the complexity of the coding schema.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\FDKDZD7X\Gut et Bayerl - Measuring the Reliability of Manual Annotations of.pdf}
}

@incollection{hart-staveland:1988:development-nasatlx-task,
  title = {Development of {{NASA-TLX}} ({{Task Load Index}}): {{Results}} of {{Empirical}} and {{Theoretical Research}}},
  booktitle = {Human {{Mental Workload}}},
  author = {Hart, Sandra G. and Staveland, Lowell E.},
  editor = {Hancock, Peter A. and Meshkati, Najmedin},
  date = {1988},
  series = {Advances in {{Psychology}}},
  volume = {52},
  pages = {139--183},
  publisher = {{North-Holland}},
  url = {https://www.sciencedirect.com/science/article/pii/S0166411508623869},
  abstract = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload.},
  keywords = {*CITATION,Charge mentale}
}

@incollection{hernandez-olivan-beltran:2023:music-composition-deep,
  title = {Music {{Composition}} with {{Deep Learning}}: {{A Review}}},
  shorttitle = {Music {{Composition}} with {{Deep Learning}}},
  booktitle = {Advances in {{Speech}} and {{Music Technology}}},
  author = {Hernandez-Olivan, Carlos and Beltran, Jose R.},
  date = {2023},
  pages = {25--50},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  url = {https://link.springer.com/10.1007/978-3-031-18444-4_2},
  urldate = {2023-09-18},
  abstract = {Generating a complex work of art such as a musical composition requires exhibiting true creativity that depends on a variety of factors that are related to the hierarchy of musical language. Music generation have been faced with Algorithmic methods and recently, with Deep Learning models that are being used in other fields such as Computer Vision. In this paper we want to put into context the existing relationships between AI-based music composition models and human musical composition and creativity processes. We give an overview of the recent Deep Learning models for music composition and we compare these models to the music composition process from a theoretical point of view. We have tried to answer some of the most relevant open questions for this task by analyzing the ability of current Deep Learning models to generate music with creativity or the similarity between AI and human composition processes, among others.},
  isbn = {978-3-031-18443-7 978-3-031-18444-4},
  langid = {english},
  keywords = {*CITATION,Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\4SBA57WH\Hernandez-Olivan et Beltran - 2021 - Music Composition with Deep Learning A Review.pdf}
}

@article{honnibal-montani:2017:spacy-natural-language,
  title = {{{spaCy}} 2 : {{Natural}} Language Understanding with {{Bloom}} Embeddings, Convolutional Neural Networks and Incremental Parsing},
  author = {Honnibal, Matthew and Montani, Ines},
  date = {2017},
  keywords = {*CITATION,Spacy}
}

@book{howe:2008:crowdsourcing-how-power,
  title = {Crowdsourcing: How the Power of the Crowd Is Driving the Future of Business},
  shorttitle = {Crowdsourcing},
  author = {Howe, Jeff},
  date = {2008},
  edition = {Random House Books},
  publisher = {{RH Business Books}},
  location = {{London}},
  abstract = {Jeff Howe coined the word Crowdsourcing in a 2006 article for Wired magazine to describe the way in which the Internet has broken down traditional employer/employee relationships to create vibrant new enterprises that are staffed by informal, often large gatherings of enthusiasts. A few weeks before the article hit the newsstands, a Google search for the word Crowdsourcing returned zero results. One month after the article appeared, the same search returned nearly 500,000 hits. These days anyone and everyone can write book reviews on Amazon, post videos on Youtube, come up with new uses for Google maps or design T-shirts for Threadless. What makes this phenomenon so remarkable is that it is starting to transform the way many companies operate and to change their relationship with their customers- iStockPhoto.com has revolutionised the world of digital photography; Cambrian House is having a profound impact on the way films get made; Second Life has created a vast, profitable business with only a few formal employees but thousands of dedicated contributors. Moreover this revolution is rapidly changing our culture, introducing a consumer democracy that has never existed befo},
  isbn = {978-1-905211-12-8 978-1-905211-11-1},
  langid = {english},
  pagetotal = {312},
  keywords = {*CITATION}
}

@article{hoyt-etal:2016:ibm-watson-analytics,
  title = {{{IBM Watson Analytics}}: {{Automating Visualization}}, {{Descriptive}}, and {{Predictive Statistics}}},
  author = {Hoyt, Robert Eugene and Snider, Dallas and Thompson, Carla and Mantravadi, Sarita},
  date = {2016-10-11},
  journaltitle = {JMIR Public Health Surveill},
  volume = {2},
  number = {2},
  issn = {2369-2960},
  doi = {10.2196/publichealth.5810},
  url = {https://doi.org/10.2196/publichealth.5810},
  abstract = {Background: We live in an era of explosive data generation that will continue to grow and involve all industries. One of the results of this explosion is the need for newer and more efficient data analytics procedures. Traditionally, data analytics required a substantial background in statistics and computer science. In 2015, International Business Machines Corporation (IBM) released the IBM Watson Analytics (IBMWA) software that delivered advanced statistical procedures based on the Statistical Package for the Social Sciences (SPSS). The latest entry of Watson Analytics into the field of analytical software products provides users with enhanced functions that are not available in many existing programs. For example, Watson Analytics automatically analyzes datasets, examines data quality, and determines the optimal statistical approach. Users can request exploratory, predictive, and visual analytics. Using natural language processing (NLP), users are able to submit additional questions for analyses in a quick response format. This analytical package is available free to academic institutions (faculty and students) that plan to use the tools for noncommercial purposes. Objective: To report the features of IBMWA and discuss how this software subjectively and objectively compares to other data mining programs. Methods: The salient features of the IBMWA program were examined and compared with other common analytical platforms, using validated health datasets. Results: Using a validated dataset, IBMWA delivered similar predictions compared with several commercial and open source data mining software applications. The visual analytics generated by IBMWA were similar to results from programs such as Microsoft Excel and Tableau Software. In addition, assistance with data preprocessing and data exploration was an inherent component of the IBMWA application. Sensitivity and specificity were not included in the IBMWA predictive analytics results, nor were odds ratios, confidence intervals, or a confusion matrix. Conclusions: IBMWA is a new alternative for data analytics software that automates descriptive, predictive, and visual analytics. This program is very user-friendly but requires data preprocessing, statistical conceptual understanding, and domain expertise.},
  keywords = {*CITATION,Data analysis,Data mining,IBM Watson,Machine learning,NLP,Statistical data analysis}
}

@article{huang-etal:2022:are-large-pretrained,
  title = {Are {{Large Pre-Trained Language Models Leaking Your Personal Information}}?},
  author = {Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  date = {2022},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.2205.12628},
  url = {https://arxiv.org/abs/2205.12628},
  urldate = {2023-07-20},
  abstract = {Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner's name. We find that PLMs do leak personal information due to memorization. However, since the models are weak at association, the risk of specific personal information being extracted by attackers is low. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe.},
  keywords = {*CITATION,Artificial Intelligence,Computation and Language,Cryptography and Security,FOS: Computer and information sciences},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\YTDTZXU8\Huang et al. - 2022 - Are Large Pre-Trained Language Models Leaking Your.pdf}
}

@dataset{hugging-face:2016:hugging-face-ai,
  title = {Hugging {{Face}} - the {{AI}} Community Building the Future.},
  author = {{Hugging Face}},
  date = {2016},
  url = {https://huggingface.co/datasets},
  keywords = {*CITATION}
}

@article{iman-etal:2023:review-deep-transfer,
  title = {A {{Review}} of {{Deep Transfer Learning}} and {{Recent Advancements}}},
  author = {Iman, Mohammadreza and Arabnia, Hamid Reza and Rasheed, Khaled},
  date = {2023-03-14},
  journaltitle = {Technologies},
  volume = {11},
  number = {2},
  pages = {40},
  issn = {2227-7080},
  doi = {10.3390/technologies11020040},
  url = {https://www.mdpi.com/2227-7080/11/2/40},
  urldate = {2023-09-29},
  abstract = {Deep learning has been the answer to many machine learning problems during the past two decades. However, it comes with two significant constraints: dependency on extensive labeled data and training costs. Transfer learning in deep learning, known as Deep Transfer Learning (DTL), attempts to reduce such reliance and costs by reusing obtained knowledge from a source data/task in training on a target data/task. Most applied DTL techniques are network/model-based approaches. These methods reduce the dependency of deep learning models on extensive training data and drastically decrease training costs. Moreover, the training cost reduction makes DTL viable on edge devices with limited resources. Like any new advancement, DTL methods have their own limitations, and a successful transfer depends on specific adjustments and strategies for different scenarios. This paper reviews the concept, definition, and taxonomy of deep transfer learning and well-known methods. It investigates the DTL approaches by reviewing applied DTL techniques in the past five years and a couple of experimental analyses of DTLs to discover the best practice for using DTL in different scenarios. Moreover, the limitations of DTLs (catastrophic forgetting dilemma and overly biased pre-trained models) are discussed, along with possible solutions and research trends.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\IE6I9DG4\Iman et al. - 2023 - A Review of Deep Transfer Learning and Recent Adva.pdf}
}

@standard{international-organization-for-standardization:2007:codes-representation-names,
  type = {Standard},
  title = {Codes for the Representation of Names of Languages – {{Part}} 3: {{Alpha-3}} Code for Comprehensive Coverage of Languages},
  date = {2007-02-16},
  number = {ISO 639-3:2007},
  publisher = {{International Organization for Standardization}},
  location = {{Geneva, CH}},
  url = {https://www.iso.org/standard/39534.html},
  abstract = {ISO 639-3:2007 provides a code, published by the Registration Authority of ISO 639-3, consisting of language code elements comprising three-letter language identifiers for the representation of languages. The language identifiers according to this ISO 639-3:2007 were devised for use in a wide range of applications, especially in computer systems, where there is potential need to support a large number of the languages that are known to have ever existed. Whereas ISO 639-1 and ISO 639-2 are intended to focus on the major languages of the world that are most frequently represented in the total body of the world's literature, ISO 639-3:2007 attempts to provide as complete an enumeration of languages as possible, including living, extinct, ancient and constructed languages, whether major or minor, written or unwritten. As a result, ISO 639-3:2007 deals with a very large number of lesser-known languages. Languages designed exclusively for machine use, such as computer-programming languages and reconstructed languages, are not included in this code.},
  pubstate = {Published},
  keywords = {*CITATION}
}

@online{isoz:2017:decouvrir-metiers-data,
  title = {Découvrir les métiers de la data science},
  author = {Isoz, Vincent},
  date = {2017-12-20},
  url = {https://fr.linkedin.com/learning/decouvrir-les-metiers-de-la-data-science/decouvrir-la-data-science},
  urldate = {2023-09-26},
  abstract = {Avec Vincent Isoz, plongez-vous dans le thème à la fois actuel, étonnant et futuriste de la data science, ou science des données. Après en avoir jeté les bases, votre formateur vous présente les 7 métiers majeurs de la data science. Vous y découvrirez les rôles, les compétences mais également toutes les interactions et la synergie entre eux. À l'issue de cette formation, vous aurez une bonne idée de ce qu'est la data science et vous serez apte à mieux suivre l'actualité, l'information et les débats à ce sujet.},
  langid = {french},
  organization = {{LinkedIn Learning}},
  keywords = {*CITATION}
}

@inproceedings{jaipuria-etal:2020:deflating-dataset-bias,
  title = {Deflating Dataset Bias Using Synthetic Data Augmentation},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}}) Workshops},
  author = {Jaipuria, Nikita and Zhang, Xianling and Bhasin, Rohan and Arafa, Mayar and Chakravarty, Punarjay and Shrivastava, Shubham and Manglani, Sagar and Murali, Vidya N.},
  date = {2020-06},
  abstract = {Deep Learning has seen an unprecedented increase in vision applications since the publication of large-scale object recognition datasets and introduction of scalable compute hardware. State-of-the-art methods for most vision tasks for Autonomous Vehicles (AVs) rely on supervised learning and often fail to generalize to domain shifts and/or outliers. Dataset diversity is thus key to successful real-world deployment. No matter how big the size of the dataset, capturing long tails of the distribution pertaining to task-specific environmental factors is impractical. The goal of this paper is to investigate the use of targeted synthetic data augmentation - combining the benefits of gaming engine simulations and sim2real style transfer techniques - for filling gaps in real datasets for vision tasks. Empirical studies on three different computer vision tasks of practical use to AVs - parking slot detection, lane detection and monocular depth estimation - consistently show that having synthetic data in the training mix provides a significant boost in cross-dataset generalization performance as compared to training on real data only, for the same size of the training set.},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\RBYWKHHJ\Jaipuria et al. - 2020 - Deflating dataset bias using synthetic data augmen.pdf}
}

@article{jones-etal:2015:demographic-occupational-predictors,
  title = {Demographic and Occupational Predictors of Stress and Fatigue in {{French}} Intensive-Care Registered Nurses and Nurses' Aides: {{A}} Cross-Sectional Study},
  shorttitle = {Demographic and Occupational Predictors of Stress and Fatigue in {{French}} Intensive-Care Registered Nurses and Nurses' Aides},
  author = {Jones, Gabrielle and Hocine, Mounia and Salomon, Jérôme and Dab, William and Temime, Laura},
  date = {2015-01},
  journaltitle = {International Journal of Nursing Studies},
  volume = {52},
  number = {1},
  pages = {250--259},
  issn = {00207489},
  doi = {10.1016/j.ijnurstu.2014.07.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0020748914002016},
  urldate = {2023-06-07},
  langid = {english},
  keywords = {*CITATION,Ergonomie,Fatigue des opérateurs}
}

@article{kaddour-etal:2023:challenges-applications-large,
  title = {Challenges and {{Applications}} of {{Large Language Models}}},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  date = {2023},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.2307.10169},
  url = {https://arxiv.org/abs/2307.10169},
  urldate = {2023-07-20},
  abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
  keywords = {*CITATION,Artificial Intelligence,Computation and Language,FOS: Computer and information sciences,Machine learning,Problèmes},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\R7GUR649\Kaddour et al. - 2023 - Challenges and Applications of Large Language Mode.pdf}
}

@book{kahneman:2011:thinking-fast-slow,
  title = {Thinking, Fast and Slow.},
  author = {Kahneman, Daniel},
  date = {2011},
  series = {Thinking, Fast and Slow.},
  publisher = {{Farrar, Straus and Giroux}},
  location = {{New York,  NY,  US}},
  abstract = {In the highly anticipated Thinking, Fast and Slow, Kahneman takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. Kahneman exposes the extraordinary capabilities—and also the faults and biases—of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behavior. The impact of loss aversion and overconfidence on corporate strategies, the difficulties of predicting what will make us happy in the future, the challenges of properly framing risks at work and at home, the profound effect of cognitive biases on everything from playing the stock market to planning the next vacation—each of these can be understood only by knowing how the two systems shape our judgments and decisions. Engaging the reader in a lively conversation about how we think, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives—and how we can use different techniques to guard against the mental glitches that often get us into trouble. Thinking, Fast and Slow will transform the way you think about thinking. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {0-374-27563-7 (Hardcover); 1-4299-6935-0 (PDF); 978-0-374-27563-1 (Hardcover); 978-1-4299-6935-2 (PDF)},
  pagetotal = {499},
  keywords = {*CITATION,Choice Behavior,Cognitive Processes,Decision Making,Intuition,Judgment,Mind,Thinking}
}

@article{kamvar-etal:2003:spectral-learning,
  title = {Spectral {{Learning}}},
  author = {Kamvar, Sepandar D and Klein, Dan and Manning, Christopher D},
  date = {2003},
  journaltitle = {Proceedings of the international joint conference on artificial intelligence},
  pages = {561--566},
  abstract = {We present a simple, easily implemented spectral learning algorithm that applies equally whether we have no supervisory information, pairwise link constraints, or labeled examples. In the unsupervised case, it performs consistently with other spectral clustering algorithms. In the supervised case, our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the 20 Newsgroups data set. Furthermore, its classification accuracy increases with the addition of unlabeled documents, demonstrating effective use of unlabeled data.},
  langid = {english},
  keywords = {*CITATION,Clustering,Constrained clustering,Spectral clustering},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\GX9HEA6W\Kamvar et al. - 2003 - Spectral Learning.pdf}
}

@inproceedings{keung-etal:2020:multilingual-amazon-reviewsa,
  title = {The {{Multilingual Amazon Reviews Corpus}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Keung, Phillip and Lu, Yichao and Szarvas, György and Smith, Noah A.},
  date = {2020},
  pages = {4563--4568},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.369},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.369},
  urldate = {2023-10-03},
  abstract = {We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., ‘books’, ‘appliances’, etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20\% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.},
  eventtitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\QQ9HP3I4\Keung et al. - 2020 - The Multilingual Amazon Reviews Corpus.pdf}
}

@article{khan-etal:2012:multiple-parameter-based,
  title = {Multiple {{Parameter Based Clustering}} ({{MPC}}): {{Prospective Analysis}} for {{Effective Clustering}} in {{Wireless Sensor Network}} ({{WSN}}) {{Using K-Means Algorithm}}},
  shorttitle = {Multiple {{Parameter Based Clustering}} ({{MPC}})},
  author = {Khan, Md. Asif and Tamim, Israfil and Ahmed, Emdad and Awal, M. Abdul},
  date = {2012},
  journaltitle = {WSN},
  volume = {04},
  number = {01},
  pages = {18--24},
  issn = {1945-3078, 1945-3086},
  doi = {10.4236/wsn.2012.41003},
  url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/wsn.2012.41003},
  urldate = {2023-01-16},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Constrained clustering,K-Means},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\LQ5DZJVB\Khan et al. - 2012 - Multiple Parameter Based Clustering (MPC) Prospec.pdf}
}

@incollection{kirch:2008:pearson-correlation-coefficient,
  title = {Pearson’s {{Correlation Coefficient}}},
  booktitle = {Encyclopedia of {{Public Health}}},
  editor = {Kirch, Wilhelm},
  date = {2008},
  pages = {1090--1091},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  url = {https://link.springer.com/10.1007/978-1-4020-5614-7_2569},
  urldate = {2023-07-06},
  isbn = {978-1-4020-5613-0 978-1-4020-5614-7},
  langid = {english},
  keywords = {*CITATION,Correlation,Metric}
}

@inproceedings{klie-etal:2018:inception-platform-machineassisted,
  title = {The {{INCEpTION}} Platform: {{Machine-assisted}} and Knowledge-Oriented Interactive Annotation},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics: {{System}} Demonstrations},
  author = {Klie, Jan-Christoph and Bugert, Michael and Boullosa, Beto and family=Castilho, given=Richard Eckart, prefix=de, useprefix=true and Gurevych, Iryna},
  date = {2018-06},
  pages = {5--9},
  publisher = {{Association for Computational Linguistics}},
  url = {https://inception-project.github.io/},
  abstract = {We introduce INCEpTION, a new annotation platform for tasks including interactive and semantic annotation (e.g., concept linking, fact linking, knowledge base population, semantic frame annotation). These tasks are very time consuming and demanding for annotators, especially when knowledge bases are used. We address these issues by developing an annotation platform that incorporates machine learning capabilities which actively assist and guide annotators. The platform is both generic and modular. It targets a range of research domains in need of semantic annotation, such as digital humanities, bioinformatics, or linguistics. INCEpTION is publicly available as open-source software.},
  langid = {english},
  keywords = {*CITATION,UKP\_a\_LangTech4eHum,UKP\_p\_INCEpTION,UKP\_reviewed}
}

@article{kothadiya-etal:2020:different-methods-review,
  title = {Different {{Methods Review}} for {{Speech}} to {{Text}} and {{Text}} to {{Speech Conversion}}},
  author = {Kothadiya, Deep and Pise, Nitin and Bedekar, Mangesh},
  date = {2020-09-17},
  journaltitle = {IJCA},
  volume = {175},
  number = {20},
  pages = {9--12},
  issn = {09758887},
  doi = {10.5120/ijca2020920727},
  url = {http://www.ijcaonline.org/archives/volume175/number20/kothadiya-2020-ijca-920727.pdf},
  urldate = {2023-09-18},
  abstract = {In the instant corporation, transmission is the primary fundamental to momentum. Transitory information, to the correct person with the correct aspect is very essential, not just on an industry level, but also on an individual position. Nature is inspiring in the direction of digitization and the mechanisms of intercommunication. Telephone calling, emails, text memorandums belong to a fundamental element of signal communication in this tech-intellect nature. In procedures to distribute the intention of adequate transmission intervening two endpoints without obstacles, numerous utilizations have shown up the impression, which operates as an intermediary and helps in efficiently transmitting signals in the scheme of text or speech messages accomplished huge structure of webs. Most of these implementations discover the Usage of tasks essentially articulatory and acoustic-positioned speech recognition, reorganization from audio messages to text, and then text to artificial speech signals, vocabulary interpretation amidst individual leftovers. Researchers will be penetrating distinct algorithms and techniques that are enforced to obtain the specified utilitarian.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\8JQQGUQT\Kothadiya et al. - 2020 - Different Methods Review for Speech to Text and Te.pdf}
}

@article{kotsiantis-etal:2006:machine-learning-review,
  title = {Machine Learning: A Review of Classification and Combining Techniques},
  shorttitle = {Machine Learning},
  author = {Kotsiantis, S. B. and Zaharakis, I. D. and Pintelas, P. E.},
  date = {2006-11},
  journaltitle = {Artif Intell Rev},
  volume = {26},
  number = {3},
  pages = {159--190},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-007-9052-3},
  url = {http://link.springer.com/10.1007/s10462-007-9052-3},
  urldate = {2023-09-18},
  abstract = {Supervised classification is one of the tasks most frequently carried out by so-called Intelligent Systems. Thus, a large number of techniques have been developed based on Artificial Intelligence (Logic-based techniques, Perceptron-based techniques) and Statistics (Bayesian Networks, Instance-based techniques). The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various classification algorithms and the recent attempt for improving classification accuracy—ensembles of classifiers.},
  langid = {english},
  keywords = {*CITATION}
}

@book{krippendorff:2004:content-analysis-introduction,
  title = {Content Analysis: An Introduction to Its Methodology},
  shorttitle = {Content Analysis},
  author = {Krippendorff, Klaus},
  date = {2004},
  edition = {2nd ed},
  publisher = {{Sage}},
  location = {{Thousand Oaks, Calif}},
  isbn = {978-0-7619-1544-7 978-0-7619-1545-4},
  langid = {english},
  pagetotal = {413},
  keywords = {*CITATION,Content analysis},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\SBK8YVFI\Krippendorff - 2004 - Content analysis an introduction to its methodolo.pdf}
}

@article{kruskal-mosteller:1979:representative-sampling-ii,
  title = {Representative {{Sampling}}, {{II}}: {{Scientific Literature}}, {{Excluding Statistics}}},
  shorttitle = {Representative {{Sampling}}, {{II}}},
  author = {Kruskal, William and Mosteller, Frederick},
  date = {1979-08},
  journaltitle = {International Statistical Review / Revue Internationale de Statistique},
  volume = {47},
  number = {2},
  eprint = {1402564},
  eprinttype = {jstor},
  pages = {111},
  issn = {03067734},
  doi = {10.2307/1402564},
  url = {https://www.jstor.org/stable/1402564?origin=crossref},
  urldate = {2023-10-03},
  abstract = {This paper describes and illustrates the meanings of 'representative sample' and 'representative sampling' as used in the extrastatistical scientific literature. Six categories seem adequate to describe these uses. The first five appeared also in the review of non-scientific uses in 'Representative Sampling, I' (Kruskal and Mosteller, 1979). We define and illustrate these in Sections 1-5 of the paper. Meaning 1 General, unjustified acclaim for the data. Here the investigator gives the data a pat on the back by using a seemingly scientific term to raise its stature. No grounds, either in study design or empirical information, are given to support this usage, and hence we recommend that it be abandoned in scientific discourse. Meaning 2 Absence (or presence) of selective forces. If grounds for the absence are given, this meaning may revert to Meaning 6, a vague term to be made specific. Otherwise it reverts to Meaning 1. In proclaiming the presence of selective forces, one is denying Meaning 6. Meaning 3 Mirror or miniature of the population. The sample has the same distribution as the population. This rare feature carries with it severe constraints. We would avoid calling this a 'representative sample' because it seems much more. Meaning 4 Typical or ideal case. An item from the population that represents it on the average or modally (or ideally). We call it a representative, but not a 'representative sample'. Meaning 5 Coverage of the population. Samples designed to reflect variation, especially among strata. A sample containing at least one item from each stratum of a partition of the population is said to cover it for that partition. We would not use the term 'representative sample' for such samples. The new meaning found in the scientific literature is that of a vague term; we describe it in Section 0. Meaning 6 A vague term to be made precise. It often takes a good deal of space to describe a formal sampling scheme and it may be a convenience to have a term like 'representative sample' or 'representative sampling' to refer to the sample or the process generating it. We recommend that this usage be preserved for probability samples, and that the sampling method be described in the same work. --- Cet article d'ecrit et explique à l'aide d'exemples les sens des termes 'échantillon représentatif' et 'échantillonnage représentatif' tels qu'ils sont utilisés dans la littérature scientifique extrastatistique. Six catégories semblent adéquates pour décrire ces emplois. Les cinq premières ont également paru dans la revue des emplois non-scientifiques dans 'Echantillonnage Représentatif I', Kruskal et Mosteller (1979). Nous les définissons et les expliquons à l'aide d'exemples dans les parties 1 à 5 du présent article. - Sens 1. Un enthousiasme général et non justifié pour les données. Le chercheur 'gonfle' les données en utilisant un terme apparemment scientifique pour rehausser leur stature. Aucune raison valable, soit dans le plan de l'étude soit dans l'information empirique, n'est apportée pour justifier cet emploi, et c'est pourquoi nous suggérons qu'il soit rayé du langage scientifique. - Sens 2. Absence (ou présence) de forces discriminatoires. Si des raisons valables pour leur absence sont données, ce sens peut revenir au sens numéro six, un terme vague qu'il reste à rendre scientifique. Sinon, il revient au sens numéro un. En soulignant la présence de forces discriminatoires, on dément le sens numéro six. - Sens 3. Reflet ou miniature de la population. L'échantillon a la même répartition que la population. Cette caractéristique rare s'accompagne de contraintes sévères. Nous éviterions d'appeler ceci un échantillon représentatif car il semble être beaucoup plus. - Sens 4. Cas typique ou idéal. Un élément de la population qui la représente en moyenne ou de façon la plus probable (ou idéale). Nous appelons ceci un représentant et non un 'échantillon représentatif'. - Sens 5. Recouvrement de la population. Echantillons conçus pour refléter la variation, particulièrement entre les strates. Un échantillon contenant au moins un élément de chaque strate d'une partition de la population est dit la recouvrir pour cette partition. Nous n'emploierions pas le terme 'échantillon représentatif' pour de tels échantillons. (Le nouveau sens trouvé dans la littérature scientifique est celui d'un terme vague. Nous le décrivons dans la partie 0.) - Sens 6. Une terme vague qu'il reste à préciser. La description d'une méthode d'échantillonnage demande de longues explications et il peut être pratique d'avoir à sa disposition un terme tel qu' 'échantillon représentatif' ou 'échantillonnage représentatif' pour se rapporter à l'échantillon ou au procédé qui l'engendre. Nous suggérons que cet usage soit réservé aux échantillons probabilistes et que la méthode d'échantillonnage soit décrite dans la même étude.},
  keywords = {*CITATION}
}

@article{kruskal-mosteller:1979:representative-sampling-iii,
  title = {Representative {{Sampling}}, {{III}}: {{The Current Statistical Literature}}},
  shorttitle = {Representative {{Sampling}}, {{III}}},
  author = {Kruskal, William and Mosteller, Frederick},
  date = {1979-12},
  journaltitle = {International Statistical Review / Revue Internationale de Statistique},
  volume = {47},
  number = {3},
  eprint = {1402647},
  eprinttype = {jstor},
  pages = {245},
  issn = {03067734},
  doi = {10.2307/1402647},
  url = {https://www.jstor.org/stable/1402647?origin=crossref},
  urldate = {2023-10-03},
  abstract = {The meanings of 'representative sample' and 'representative sampling' in the statistical literature are classified, illustrated, and discussed. The categories are those of our prior papers, together with three new ones: specific sampling methods, permitting good estimation, and good enough for a particular purpose. The paper closes with a sketch of relevant technical or mathematical directions. --- Les différents sens dans la littérature statistique, de: 'échantillon représentatif' et 'sondage représentatif', ont été classés, illustrés et discutés. Les catégories retenues sont celles de nos précédents articles, ainsi que trois nouvelles: méthodes spécifiques de sondage, méthodes permettant de bonnes estimations, méthodes suffisamment bonnes pour un objectif particulier. L'article s'achève sur un aperçu des orientations techniques ou mathématiques correspondantes.},
  keywords = {*CITATION}
}

@article{kruskal-mosteller:1979:representative-sampling-nonscientific,
  title = {Representative {{Sampling}}, {{I}}: {{Non-Scientific Literature}}},
  shorttitle = {Representative {{Sampling}}, {{I}}},
  author = {Kruskal, William and Mosteller, Frederick},
  date = {1979-04},
  journaltitle = {International Statistical Review / Revue Internationale de Statistique},
  volume = {47},
  number = {1},
  eprint = {1403202},
  eprinttype = {jstor},
  pages = {13},
  issn = {03067734},
  doi = {10.2307/1403202},
  url = {https://www.jstor.org/stable/1403202?origin=crossref},
  urldate = {2023-10-03},
  abstract = {By classifying and illustrating non-scientific uses of the term 'representative sample', its meanings can be clarified. The principal meanings seem to be: generalized if unjustified acclaim for data; absence of selective forces; miniature of the population; typical case or cases, the ideal case; and coverage of the population. Because of its ambiguities and imprecision, we recommend great caution in the use of the expression 'representative sample'. Usually a more specific expression will add clarity. --- Les différents sens du terme 'échantillon représentatif' peuvent être clarifiés en classant et illustrant ses emplois non-scientifiques. Les principaux sens semblent être: un enthousiasme généralisé et non justifié pour les données l'absence de forces déformantes dans les choix une miniaturisation de la population un ou des exemples typiques; un exemple idéal une situation où l'ensemble de la population est couvert. A cause de son ambiguité et de son manque de précision, nous recommandons la plus grande précaution dans l'emploi de l'expression 'échantillon représentatif'. En général, utiliser une expression plus spécifique ajoutera de la clarté.},
  keywords = {*CITATION}
}

@inproceedings{lamirel-etal:2016:new-efficient-clustering,
  title = {New Efficient Clustering Quality Indexes},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Lamirel, Jean-Charles and Dugue, Nicolas and Cuxac, Pascal},
  date = {2016-07},
  pages = {3649--3657},
  publisher = {{IEEE}},
  location = {{Vancouver, BC, Canada}},
  doi = {10/gfs3xx},
  url = {http://ieeexplore.ieee.org/document/7727669/},
  urldate = {2018-11-23},
  abstract = {This paper deals with a major challenge in clustering that is optimal model selection. It presents new efficient clustering quality indexes relying on feature maximization, which is an alternative measure to usual distributional measures relying on entropy, Chi-square metric or vector-based measures such as Euclidean distance or correlation distance. First Experiments compare the behavior of these new indexes with usual cluster quality indexes based on Euclidean distance on different kinds of test datasets for which ground truth is available. This comparison clearly highlights altogether the superior accuracy and stability of the new method on these datasets, its efficiency from low to high dimensional range and its tolerance to noise. Further experiments are then conducted on ”real life” textual data extracted from a multisource bibliographic database for which ground truth is unknown. These experiments show that the accuracy and stability of these new indexes allow to deal efficiently with diachronic analysis, when other indexes do not fit the requirements for this task.},
  eventtitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  isbn = {978-1-5090-0620-5},
  langid = {english},
  keywords = {*CITATION,FMC},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\I9SXE6GM\Lamirel et al. - 2016 - New efficient clustering quality indexes.pdf}
}

@incollection{lamirel-etal:2017:novel-approach-feature,
  title = {A {{Novel Approach}} to {{Feature Selection Based}} on {{Quality Estimation Metrics}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Management}}},
  author = {Lamirel, Jean-Charles and Cuxac, Pascal and Hajlaoui, Kafil},
  editor = {Guillet, Fabrice and Pinaud, Bruno and Venturini, Gilles},
  date = {2017},
  series = {Studies in {{Computational Intelligence}}},
  volume = {665},
  pages = {121--140},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  url = {http://link.springer.com/10.1007/978-3-319-45763-5_7},
  urldate = {2018-11-23},
  abstract = {Feature maximization (F-max) is an unbiased quality estimation metric of unsupervised classification (clustering) that favours clusters with a maximal feature F-measure value. In this article we show that an adaptation of this metric within the framework of supervised classification allows efficient feature selection and feature contrasting to be performed. We experiment the method on different types of textual data. In this context, we demonstrate that this technique significantly improves the performance of classification methods as compared with the use of state-of-the art feature selection techniques, notably in the case of the classification of unbalanced, highly multidimensional and noisy textual data gathered in similar classes.},
  isbn = {978-3-319-45762-8 978-3-319-45763-5},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Evaluation metrics,Features selection,Filters,FMC,Multidimensional data},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\QHURTRCV\Lamirel et al. - 2017 - A Novel Approach to Feature Selection Based on Qua.pdf}
}

@article{lampert-etal:2018:constrained-distance-based,
  title = {Constrained Distance Based Clustering for Time-Series: A Comparative and Experimental Study},
  shorttitle = {Constrained Distance Based Clustering for Time-Series},
  author = {Lampert, Thomas and Dao, Thi-Bich-Hanh and Lafabregue, Baptiste and Serrette, Nicolas and Forestier, Germain and Crémilleux, Bruno and Vrain, Christel and Gançarski, Pierre},
  date = {2018-11},
  journaltitle = {Data Min Knowl Disc},
  volume = {32},
  number = {6},
  pages = {1663--1707},
  issn = {1384-5810, 1573-756X},
  doi = {10/gfbpj8},
  url = {http://link.springer.com/10.1007/s10618-018-0573-y},
  urldate = {2020-06-02},
  abstract = {Constrained clustering is becoming an increasingly popular approach in data mining. It offers a balance between the complexity of producing a formal definition of thematic classes—required by supervised methods—and unsupervised approaches, which ignore expert knowledge and intuition. Nevertheless, the application of constrained clustering to time-series analysis is relatively unknown. This is partly due to the unsuitability of the Euclidean distance metric, which is typically used in data mining, to time-series data. This article addresses this divide by presenting an exhaustive review of constrained clustering algorithms and by modifying publicly available implementations to use a more appropriate distance measure—dynamic time warping. It presents a comparative study, in which their performance is evaluated when applied to time-series. It is found that k-means based algorithms become computationally expensive and unstable under these modifications. Spectral approaches are easily applied and offer state-of-the-art performance, whereas declarative approaches are also easily applied and guarantee constraint satisfaction. An analysis of the results raises several influencing factors to an algorithm’s performance when constraints are introduced.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Comparaison,Constrained clustering,Dynamic time warping,INTERACTIVE\_CLUSTERING,Partition clustering,Semi-supervised,Time-series},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\UMVKY7ZH\Lampert et al. - 2018 - Constrained distance based clustering for time-ser.pdf}
}

@inproceedings{lampert-etal:2019:constrained-distance-based,
  title = {Constrained {{Distance}} Based {{K-Means Clustering}} for {{Satellite Image Time-Series}}},
  booktitle = {{{IGARSS}} 2019 - 2019 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Lampert, Thomas and Lafabregue, Baptiste and Gancarski, Pierre},
  date = {2019-07},
  pages = {2419--2422},
  publisher = {{IEEE}},
  location = {{Yokohama, Japan}},
  doi = {10/ggx3tj},
  url = {https://ieeexplore.ieee.org/document/8900147/},
  urldate = {2020-06-02},
  abstract = {The advent of high-resolution instruments for time-series sampling poses added complexity for the formal definition of thematic classes in the remote sensing domain—required by supervised methods—while unsupervised methods ignore expert knowledge and intuition. Constrained clustering is becoming an increasingly popular approach in data mining because it offers a solution to these problems, however, its application in remote sensing is relatively unknown. This article addresses this divide by adapting publicly available k-Means constrained clustering implementations to use the dynamic time warping (DTW) dissimilarity measure, which is thought to be more appropriate for time-series analysis. Adding constraints to the clustering problem increases accuracy when compared to unconstrained clustering. The output of such algorithms are homogeneous in spatially defined regions.},
  eventtitle = {{{IGARSS}} 2019 - 2019 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  isbn = {978-1-5386-9154-0},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Constrained clustering,INTERACTIVE\_CLUSTERING,K-Means},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\KWERLC9W\\Lampert et al. - 2019 - PRÉSENTATION.pdf;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\UNDG3ZHC\\Lampert et al. - 2019 - Constrained Distance based K-Means Clustering for .pdf}
}

@software{latitude-inc.-oasis-tech-inc.:2019:ai-dungeon,
  title = {{{AI}} Dungeon},
  author = {{Latitude Inc.} and {Oasis Tech Inc.}},
  date = {2019},
  url = {https://play.aidungeon.com/},
  organization = {{Latitude Inc.}},
  keywords = {*CITATION}
}

@online{lee-sengupta:2022:introducing-ai-research,
  type = {Blog},
  title = {Introducing the Ai Research Supercluster - Meta’s Cutting-Edge Ai Supercomputer for Ai Research},
  author = {Lee, Kevin and Sengupta, Shubho},
  date = {2022},
  url = {https://ai.meta.com/blog/ai-rsc/},
  abstract = {Developing the next generation of advanced AI will require powerful new computers capable of quintillions of operations per second. Today, Meta is announcing that we’ve designed and built the AI Research SuperCluster (RSC) — which we believe is among the fastest AI supercomputers running today and will be the fastest AI supercomputer in the world when it’s fully built out in mid-2022. Our researchers have already started using RSC to train large models in natural language processing (NLP) and computer vision for research, with the aim of one day training models with trillions of parameters. RSC will help Meta’s AI researchers build new and better AI models that can learn from trillions of examples; work across hundreds of different languages; seamlessly analyze text, images, and video together; develop new augmented reality tools; and much more. Our researchers will be able to train the largest models needed to develop advanced AI for computer vision, NLP, speech recognition, and more. We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together. Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.},
  organization = {{Meta AI}},
  keywords = {*CITATION}
}

@incollection{leech:2004:adding-linguistic-annotation,
  title = {Adding Linguistic Annotation.},
  booktitle = {Developing Linguistic Corpora : A Guide to Good Practice},
  author = {Leech, Geoffrey},
  editor = {Wynne, M.},
  date = {2004},
  edition = {Oxbow Books},
  pages = {17--29},
  publisher = {{AHDS: Literature, Languages, and Linguistics}},
  location = {{Oxford}},
  url = {http://ahds.ac.uk/creating/guides/linguistic-corpora/chapter2.htm},
  isbn = {978-1-84217-205-6},
  langid = {english},
  keywords = {*CITATION}
}

@inproceedings{lefeuvre-etal:2014:annotation-temporalite-corpus,
  title = {Annotation de La Temporalité En Corpus : Contribution à l'amélioration de La Norme {{TimeML}}},
  booktitle = {{{TALN}}'2014},
  author = {Lefeuvre, Anaïs and Antoine, Jean-Yves and Savary, Agata and Schang, Emmanuel and Abouda, Lotfi and Maurel, Denis and Eshkol, Iris},
  editor = {{ATALA}},
  date = {2014-07},
  volume = {2},
  pages = {F14-2029},
  location = {{Marseille, France}},
  url = {https://hal.science/hal-01075207},
  keywords = {*CITATION,Annotation temporelle,eventualités,Expressions temporelles,MATTER (revise),relations temporelles,temporal annotation,TimeML}
}

@article{les-echos:2023:ia-auteur-game,
  entrysubtype = {newspaper},
  title = {IA : L'auteur de "Game of Thrones" et d'autres écrivains portent plainte contre le créateur de ChatGPT},
  author = {{Les Echos}},
  date = {2023-09-21},
  journaltitle = {Les Echos},
  url = {https://www.lesechos.fr/tech-medias/intelligence-artificielle/ia-lauteur-de-game-of-thrones-et-dautres-ecrivains-portent-plainte-contre-le-createur-de-chatgpt-1980235},
  urldate = {2023-09-29},
  abstract = {Ils accusent OpenAI d'avoir entraîné son intelligence artificielle avec leurs oeuvres, permettant ainsi à ChatGPT d'être en mesure de produire des contenus dérivés, imitant le style des écrivains.},
  journalsubtitle = {Tech-Médias},
  langid = {french},
  keywords = {*CITATION}
}

@article{lewis-etal:2019:bart-denoising-sequencetosequence,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2019},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.1910.13461},
  url = {https://arxiv.org/abs/1910.13461},
  urldate = {2023-07-17},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  keywords = {*CITATION,Computation and Language,FOS: Computer and information sciences,Machine learning},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\HMB79SIK\Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf}
}

@inproceedings{li-etal:2009:constrained-clustering-spectral,
  title = {Constrained Clustering via Spectral Regularization},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Zhenguo and Liu, Jianzhuang and Tang, Xiaoou},
  date = {2009-06},
  pages = {421--428},
  publisher = {{IEEE}},
  location = {{Miami, FL}},
  doi = {10/dsh439},
  url = {https://ieeexplore.ieee.org/document/5206852/},
  urldate = {2020-07-29},
  abstract = {We propose a novel framework for constrained spectral clustering with pairwise constraints which specify whether two objects belong to the same cluster or not. Unlike previous methods that modify the similarity matrix with pairwise constraints, we adapt the spectral embedding towards an ideal embedding as consistent with the pairwise constraints as possible. Our formulation leads to a small semidefinite program whose complexity is independent of the number of objects in the data set and the number of pairwise constraints, making it scalable to large-scale problems. The proposed approach is applicable directly to multi-class problems, handles both must-link and cannotlink constraints, and can effectively propagate pairwise constraints. Extensive experiments on real image data and UCI data have demonstrated the efficacy of our algorithm.},
  eventtitle = {2009 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPR Workshops}})},
  isbn = {978-1-4244-3992-8},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Constrained clustering,INTERACTIVE\_CLUSTERING,Spectral clustering},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\DUKRVM3F\Li et al. - 2009 - Constrained clustering via spectral regularization.pdf}
}

@article{li-etal:2022:survey-deep-learning,
  title = {A {{Survey}} on {{Deep Learning}} for {{Named Entity Recognition}}},
  author = {Li, Jing and Sun, Aixin and Han, Jianglei and Li, Chenliang},
  date = {2022-01-01},
  journaltitle = {IEEE Trans. Knowl. Data Eng.},
  volume = {34},
  number = {1},
  pages = {50--70},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2020.2981314},
  url = {https://ieeexplore.ieee.org/document/9039685/},
  urldate = {2023-09-18},
  abstract = {Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area.},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\KKWA3GMY\Li et al. - 2022 - A Survey on Deep Learning for Named Entity Recogni.pdf}
}

@article{lin-etal:2014:microsoft-coco-common,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2014},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.1405.0312},
  url = {https://cocodataset.org},
  urldate = {2023-10-06},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  langid = {english},
  keywords = {*CITATION,Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\8Y5FLDIK\Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf}
}

@article{loignon:2023:ia-medias-francais,
  entrysubtype = {newspaper},
  title = {IA : Les médias français s'organisent face à la collecte de données par les robots},
  author = {Loignon, Stéphane},
  date = {2023-08-28},
  journaltitle = {Les Echos},
  url = {https://www.lesechos.fr/tech-medias/medias/ia-les-medias-francais-sorganisent-face-a-la-collecte-de-donnees-par-les-robots-1973079},
  urldate = {2023-10-04},
  abstract = {A l'image du « New York Times », quelques sites français, dont ceux de Radio France et TF1, empêchent le robot GPTBot d'OpenAI de se nourrir de leurs contenus. Refusant d'être « pillé », le secteur réfléchit à la manière d'obtenir une juste rémunération de la part des géants de l'IA.},
  journalsubtitle = {Tech-Médias},
  langid = {french},
  keywords = {*CITATION}
}

@article{maalouf:2011:logistic-regression-data,
  title = {Logistic Regression in Data Analysis: An Overview},
  shorttitle = {Logistic Regression in Data Analysis},
  author = {Maalouf, Maher},
  date = {2011},
  journaltitle = {IJDATS},
  volume = {3},
  number = {3},
  pages = {281},
  issn = {1755-8050, 1755-8069},
  doi = {10.1504/IJDATS.2011.041335},
  url = {http://www.inderscience.com/link.php?id=41335},
  urldate = {2023-09-15},
  abstract = {Logistic regression (LR) continues to be one of the most widely used methods in data mining in general and binary data classification in particular. This paper is focused on providing an overview of the most important aspects of LR when used in data analysis, specifically from an algorithmic and machine learning perspective and how LR can be applied to imbalanced and rare events data.},
  langid = {english},
  keywords = {*CITATION}
}

@article{macqueen:1967:methods-classification-analysis,
  title = {Some Methods for Classification and Analysis of Multivariate Observations.},
  author = {MacQueen, J},
  date = {1967},
  journaltitle = {Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
  volume = {1},
  number = {14},
  pages = {281--297},
  langid = {english},
  keywords = {*CITATION,Clustering,K-Means},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\NUMFXV2C\MacQueen - 1967 - Some methods for classification and analysis of mu.pdf}
}

@article{maharana-etal:2022:review-data-preprocessing,
  title = {A Review: {{Data}} Pre-Processing and Data Augmentation Techniques},
  shorttitle = {A Review},
  author = {Maharana, Kiran and Mondal, Surajit and Nemade, Bhushankumar},
  date = {2022-06},
  journaltitle = {Global Transitions Proceedings},
  volume = {3},
  number = {1},
  pages = {91--99},
  issn = {2666285X},
  doi = {10.1016/j.gltp.2022.04.020},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666285X22000565},
  urldate = {2023-09-29},
  abstract = {This review paper provides an overview of data pre-processing in Machine learning, focusing on all types of problems while building the machine learning problems. It deals with two significant issues in the pre-processing process (i). issues with data and (ii). Steps to follow to do data analysis with its best approach. As raw data are vulnerable to noise, corruption, missing, and inconsistent data, it is necessary to perform pre-processing steps, which is done using classification, clustering, and association and many other pre-processing techniques available. Poor data can primarily affect the accuracy and lead to false prediction, so it is necessary to improve the dataset's quality. So, data pre-processing is the best way to deal with such problems. It makes the knowledge extraction from the data set much easier with cleaning, Integration, transformation, and reduction methods. The issue with Data missing and significant differences in the variety of data always exists as the information is collected through multiple sources and from a real-world application. So, the data augmentation approach generates data for machine learning models. To decrease the dependency on training data and to improve the performance of the machine learning model. This paper discusses flipping, rotating with slight degrees and others to augment the image data and shows how to perform data augmentation methods without distorting the original data.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\IE547ETH\Maharana et al. - 2022 - A review Data pre-processing and data augmentatio.pdf}
}

@book{manning-schutze:2000:foundations-statistical-natural,
  title = {Foundations of Statistical Natural Language Processing},
  author = {Manning, Christopher D. and Schütze, Hinrich},
  date = {2000},
  edition = {2e éd. avec des corrections},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\WINIYV9I\Manning et Schütze - 2000 - Foundations of statistical natural language proces.pdf}
}

@report{mccowan-etal:2005:use-information-retrieval,
  title = {On the {{Use}} of {{Information Retrieval Measures}} for {{Speech Recognition Evaluation}}},
  author = {McCowan, Iain and Moore, Daren and Dines, John and Gatica-Perez, Daniel and Flynn, Mike and Wellner, Pierre and Bourlard, Hervé},
  date = {2005-03},
  number = {IDIAP-RR 04-73},
  institution = {{IDIAP Research Institute}},
  location = {{Martigny, Switzerland}},
  abstract = {This paper discusses the evaluation of automatic speech recognition (ASR) systems developed for practical applications, suggesting a set of criteria for application-oriented performance measures. The commonly used word error rate (WER), which poses ASR evaluation as a string editing process, is shown to have a number of limitations with respect to these criteria, motivating alternative or additional measures. This paper suggests that posing speech recognition evaluation as an information retrieval problem, where each word is one unit of information, outliers a flexible framework for application-oriented performance analysis based on the concepts of recall and precision.},
  langid = {english},
  keywords = {*CITATION,WER},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\4GUNCZIT\McCowan et al. - 2005 - On the Use of Information Retrieval Measures for S.pdf}
}

@software{microsoft-corporation:2018:microsoft-excel,
  title = {Microsoft {{Excel}}},
  author = {{Microsoft Corporation}},
  date = {2018},
  url = {https://office.microsoft.com/excel},
  organization = {{Microsoft Corporation}},
  keywords = {*CITATION}
}

@article{miller-charles:1991:contextual-correlates-semantic,
  title = {Contextual Correlates of Semantic Similarity},
  author = {Miller, George A. and Charles, Walter G.},
  date = {1991},
  journaltitle = {Language and Cognitive Processes},
  volume = {6},
  number = {1},
  pages = {1--28},
  doi = {10.1080/01690969108406936},
  url = {https://doi.org/10.1080/01690969108406936},
  keywords = {*CITATION}
}

@software{montani-honnibal:2017:prodigy-modern-scriptable,
  title = {Prodigy: {{A}} Modern and Scriptable Annotation Tool for Creating Training Data for Machine Learning Models},
  author = {Montani, Ines and Honnibal, Matthew},
  date = {2017-12-18},
  url = {https://prodi.gy/},
  abstract = {Prodigy is a modern annotation tool for creating training and evaluation data for machine learning models. You can also use Prodigy to help you inspect and clean your data, do error analysis and develop rule-based systems to use in combination with your statistical models. The Python library includes a range of pre-built workflows and command-line commands for various tasks, and well-documented components for implementing your own workflow scripts. Your scripts can specify how the data is loaded and saved, change which questions are asked in the annotation interface, and can even define custom HTML and JavaScript to change the behavior of the front-end. The web application is optimized for fast, intuitive and efficient annotation. Prodigy’s mission is to help you do more of all those manual or semi-automatic processes that we all know we don’t do enough of. To most data scientists, the advice to spend more time looking at your data is sort of like the advice to floss or get more sleep: it’s easy to recognize that it’s sound advice, but not always easy to put into practice. Prodigy helps by giving you a practical, flexible tool that fits easily into your workflow. With concrete steps to follow instead of a vague goal, annotation and data inspection will change from something you should do, to something you will do.},
  organization = {{Explosion}},
  keywords = {*CITATION}
}

@book{morris-goscinny:1950:rodeo,
  title = {Rodeo},
  author = {{Morris} and Goscinny, René},
  date = {1950},
  series = {Lucky Luke},
  number = {2},
  publisher = {{Dupuis}},
  isbn = {978-2-8001-0141-5},
  langid = {fre},
  keywords = {*CITATION}
}

@book{morris-goscinny:1952:sous-ciel-ouest,
  title = {Sous le ciel de l'ouest},
  author = {{Morris} and Goscinny, René},
  date = {1952},
  series = {Lucky Luke},
  number = {4},
  publisher = {{Dupuis}},
  isbn = {978-2-8001-0143-9},
  langid = {fre},
  keywords = {*CITATION}
}

@book{morris-goscinny:1958:cousins-dalton,
  title = {Les Cousins Dalton},
  author = {Morris and Goscinny, René},
  date = {1958},
  series = {Lucky Luke},
  number = {12},
  publisher = {{Dupuis}},
  isbn = {978-2-8001-1452-1},
  langid = {fre},
  keywords = {*CITATION}
}

@article{mu-etal:2021:review-endtoend-speech,
  title = {Review of End-to-End Speech Synthesis Technology Based on Deep Learning},
  author = {Mu, Zhaoxi and Yang, Xinyu and Dong, Yizhuo},
  date = {2021-04-20},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/arXiv.2104.09995},
  url = {http://arxiv.org/abs/2104.09995},
  urldate = {2023-09-18},
  abstract = {As an indispensable part of modern human-computer interaction system, speech synthesis technology helps users get the output of intelligent machine more easily and intuitively, thus has attracted more and more attention. Due to the limitations of high complexity and low efficiency of traditional speech synthesis technology, the current research focus is the deep learning-based end-to-end speech synthesis technology, which has more powerful modeling ability and a simpler pipeline. It mainly consists of three modules: text front-end, acoustic model, and vocoder. This paper reviews the research status of these three parts, and classifies and compares various methods according to their emphasis. Moreover, this paper also summarizes the open-source speech corpus of English, Chinese and other languages that can be used for speech synthesis tasks, and introduces some commonly used subjective and objective speech quality evaluation method. Finally, some attractive future research directions are pointed out.},
  keywords = {*CITATION,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\RMUDP3MB\Mu et al. - 2021 - Review of end-to-end speech synthesis technology b.pdf}
}

@article{murtagh-contreras:2012:algorithms-hierarchical-clustering,
  title = {Algorithms for Hierarchical Clustering: {{An}} Overview},
  author = {Murtagh, Fionn and Contreras, Pedro},
  date = {2012},
  journaltitle = {Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery},
  volume = {2},
  pages = {86--97},
  doi = {10.1002/widm.53},
  abstract = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. © 2011 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Hierarchcal clustering}
}

@article{nedellec-etal:2006:annotation-guidelines-machine,
  title = {Annotation Guidelines for Machine Learning-Based Named Entity Recognition in Microbiology},
  author = {Nédellec, Claire and Bessieres, Philippe and Bossy, Robert and Kotoujansky, Alain},
  date = {2006-01},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\7243BLRY\Nédellec et al. - 2006 - Annotation guidelines for machine learning-based n.pdf}
}

@article{nelder-wedderburn:1972:generalized-linear-models,
  title = {Generalized {{Linear Models}}},
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {135},
  number = {3},
  eprint = {10.2307/2344614},
  eprinttype = {jstor},
  pages = {370},
  issn = {00359238},
  doi = {10.2307/2344614},
  url = {https://www.jstor.org/stable/10.2307/2344614?origin=crossref},
  urldate = {2023-07-06},
  keywords = {*CITATION,GLM}
}

@incollection{neuendorf:2009:reliability-content-analysis,
  title = {Reliability for Content Analysis.},
  booktitle = {Media Messages and Public Health: {{A}} Decisions Approach to Content Analysis},
  author = {Neuendorf, Kimberly},
  editor = {Jordan, Amy B. and Kunkel, Dale and Manganello, Jennifer and Fishbein, Martin},
  date = {2009-01},
  edition = {Routledge},
  pages = {67--87},
  abstract = {This entry reviews the methodological options for and issues important to quantitative content analysis, including human coding versus computer-aided text analysis, reliability, validity (including external validity), and the integration of content analysis data with additional, extra-message data. A general overview of qualitative approaches to content analysis is also provided. Further, the applications of content analysis in the field of political communication are reviewed, with two main focuses: the study of messages fulfilling a news function, and those supporting a campaign function.},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\AID99P82\Neuendorf - 2009 - Reliability for content analysis..pdf}
}

@incollection{ng-etal:2002:spectral-clustering-analysis,
  title = {On {{Spectral Clustering}}: {{Analysis}} and an Algorithm},
  shorttitle = {On {{Spectral Clustering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
  editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
  date = {2002},
  pages = {849--856},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf},
  urldate = {2020-10-22},
  abstract = {Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
  keywords = {*CITATION,Clustering,Spectral clustering},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\DTJJCJL4\Ng et al. - 2002 - On Spectral Clustering Analysis and an algorithm.pdf}
}

@article{ni-etal:2022:recent-advances-deep,
  title = {Recent {{Advances}} in {{Deep Learning Based Dialogue Systems}}: {{A Systematic Survey}}},
  shorttitle = {Recent {{Advances}} in {{Deep Learning Based Dialogue Systems}}},
  author = {Ni, Jinjie and Young, Tom and Pandelea, Vlad and Xue, Fuzhao and Cambria, Erik},
  date = {2022-03-29},
  journaltitle = {ArXiv preprint},
  url = {http://arxiv.org/abs/2105.04387},
  urldate = {2023-07-26},
  abstract = {Dialogue systems are a popular natural language processing (NLP) task as it is promising in real-life applications. It is also a complicated task since many NLP tasks deserving study are involved. As a result, a multitude of novel works on this task are carried out, and most of them are deep learning based due to the outstanding performance. In this survey, we mainly focus on the deep learning based dialogue systems. We comprehensively review state-of-the-art research outcomes in dialogue systems and analyze them from two angles: model type and system type. Specifically, from the angle of model type, we discuss the principles, characteristics, and applications of different models that are widely used in dialogue systems. This will help researchers acquaint these models and see how they are applied in state-of-the-art frameworks, which is rather helpful when designing a new dialogue system. From the angle of system type, we discuss task-oriented and open-domain dialogue systems as two streams of research, providing insight into the hot topics related. Furthermore, we comprehensively review the evaluation methods and datasets for dialogue systems to pave the way for future research. Finally, some possible research trends are identified based on the recent research outcomes. To the best of our knowledge, this survey is the most comprehensive and up-to-date one at present for deep learning based dialogue systems, extensively covering the popular techniques1. We speculate that this work is a good starting point for academics who are new to the dialogue systems or those who want to quickly grasp up-to-date techniques in this area.},
  langid = {english},
  keywords = {*ALIRE/AIMPLÉMENTER,*CITATION,Chatbot,Dialogue},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\VPJSKSAR\Ni et al. - 2022 - Recent Advances in Deep Learning Based Dialogue Sy.pdf}
}

@article{nicoletti-bass:2023:generative-ai-takes,
  entrysubtype = {newspaper},
  title = {Generative AI takes stereotypes and bias from bad to worse},
  author = {Nicoletti, Leonardo and Bass, Dina},
  date = {2023-06},
  journaltitle = {Bloomberg.com},
  url = {https://www.bloomberg.com/graphics/2023-generative-ai-bias/},
  urldate = {2023-10-02},
  abstract = {The world according to Stable Diffusion is run by White male CEOs. Women are rarely doctors, lawyers or judges. Men with dark skin commit crimes, while women with dark skin flip burgers.},
  langid = {french},
  keywords = {*CITATION}
}

@book{nivre:2006:inductive-dependency-parsing,
  title = {Inductive {{Dependency Parsing}}},
  author = {Nivre, Joakim},
  editorb = {Ide, Nancy and Véronis, Jean},
  editorbtype = {redactor},
  date = {2006},
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {34},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  url = {http://link.springer.com/10.1007/1-4020-4889-0},
  urldate = {2023-07-06},
  isbn = {978-1-4020-4888-3 978-1-4020-4889-0},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\YZEE8JPF\Nivre - 2006 - Inductive Dependency Parsing.pdf}
}

@inproceedings{nothman-etal:2018:stop-word-lists,
  title = {Stop {{Word Lists}} in {{Free Open-source Software Packages}}},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
  date = {2018},
  pages = {7--12},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/W18-2502},
  url = {http://aclweb.org/anthology/W18-2502},
  urldate = {2023-07-06},
  eventtitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Preprocessing,Stopwords},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\VGBMUBTE\Nothman et al. - 2018 - Stop Word Lists in Free Open-source Software Packa.pdf}
}

@article{oneill-connor:2023:amplifying-limitations-harms,
  title = {Amplifying {{Limitations}}, {{Harms}} and {{Risks}} of {{Large Language Models}}},
  author = {O'Neill, Michael and Connor, Mark},
  date = {2023},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.2307.04821},
  url = {https://arxiv.org/abs/2307.04821},
  urldate = {2023-07-20},
  abstract = {We present this article as a small gesture in an attempt to counter what appears to be exponentially growing hype around Artificial Intelligence (AI) and its capabilities, and the distraction provided by the associated talk of science-fiction scenarios that might arise if AI should become sentient and super-intelligent. It may also help those outside of the field to become more informed about some of the limitations of AI technology. In the current context of popular discourse AI defaults to mean foundation and large language models (LLMs) such as those used to create ChatGPT. This in itself is a misrepresentation of the diversity, depth and volume of research, researchers, and technology that truly represents the field of AI. AI being a field of research that has existed in software artefacts since at least the 1950's. We set out to highlight a number of limitations of LLMs, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations. Along the way we also highlight some of the associated risks for individuals and organisations in using this technology.},
  keywords = {*CITATION,Artificial Intelligence,Computation and Language,Computers and Society,FOS: Computer and information sciences},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\T3MMECGA\O'Neill et Connor - 2023 - Amplifying Limitations, Harms and Risks of Large L.pdf}
}

@software{openai:2023:chatgpt,
  title = {{{ChatGPT}}},
  author = {{OpenAI}},
  date = {2023},
  url = {https://chat.openai.com},
  keywords = {*CITATION}
}

@article{parasuraman-etal:2000:model-types-levels,
  title = {A Model for Types and Levels of Human Interaction with Automation},
  author = {Parasuraman, R. and Sheridan, T.B. and Wickens, C.D.},
  date = {2000-05},
  journaltitle = {IEEE Trans. Syst., Man, Cybern. A},
  volume = {30},
  number = {3},
  pages = {286--297},
  issn = {10834427},
  doi = {10.1109/3468.844354},
  url = {http://ieeexplore.ieee.org/document/844354/},
  urldate = {2023-09-01},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\JNZR94XP\Parasuraman et al. - 2000 - A model for types and levels of human interaction .pdf}
}

@article{parnami-lee:2022:learning-few-examples,
  title = {Learning from {{Few Examples}}: {{A Summary}} of {{Approaches}} to {{Few-Shot Learning}}},
  shorttitle = {Learning from {{Few Examples}}},
  author = {Parnami, Archit and Lee, Minwoo},
  date = {2022},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.2203.04291},
  url = {https://arxiv.org/abs/2203.04291},
  urldate = {2023-10-06},
  abstract = {Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem).},
  keywords = {*CITATION,Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\LE2RR2SZ\Parnami et Lee - 2022 - Learning from Few Examples A Summary of Approache.pdf}
}

@article{pedregosa-etal:2011:scikitlearn-machine-learning,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  keywords = {*CITATION,Sklearn}
}

@article{perrigo-zorthian:2023:exclusive-openai-used,
  entrysubtype = {newspaper},
  title = {Exclusive: {{OpenAI}} Used {{Kenyan}} Workers on Less than \$2 per Hour to Make {{ChatGPT}} Less Toxic},
  author = {Perrigo, Billy and Zorthian, Julia},
  date = {2023-01-18},
  journaltitle = {Time},
  location = {{New York}},
  url = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
  urldate = {2023-09-29},
  journalsubtitle = {Business, Technology},
  langid = {english},
  keywords = {*CITATION}
}

@inproceedings{perrotin-etal:2018:annotation-actes-dialogue,
  title = {Annotation En Actes de Dialogue Pour Les Conversations d'{{Assistance}} En Ligne},
  booktitle = {25e Conférence Sur Le Traitement Automatique Des Langues Naturelles ({{TALN}})},
  author = {Perrotin, Robin and Nasr, Alexis and Auguste, Jeremy},
  date = {2018},
  location = {{Rennes, France}},
  url = {https://hal.science/hal-01943345},
  abstract = {Les conversations techniques en ligne sont un type de productions linguistiques qui par de nombreux aspects se démarquent des objets plus usuellement étudiés en traitement automatique des langues : il s’agit de dialogues écrits entre deux locuteurs qui servent de support à la résolution coopérative des problèmes des usagers. Nous proposons de décrire ici ces conversations par un étiquetage en actes de dialogue spécifiquement conçu pour les conversations en ligne. Différents systèmes de prédictions ont été évalués ainsi qu’une méthode permettant de s’abstraire des spécificités lexicales du corpus d’apprentissage.},
  keywords = {*CITATION,Actes de Dialogues,Big Data,Conditional Random Fields,Conversations en Ligne,CRF,Dialog Acts,Natural language processing,Neural Networks,NLP,Online Chats,Réseaux neuronaux,TAL,Traitement Automatique des Langues},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\UG79YI8I\Perrotin et al. - 2018 - Annotation en actes de dialogue pour les conversat.pdf}
}

@inproceedings{pradhan-etal:2007:semeval2007-task-17,
  title = {{{SemEval-2007}} Task 17: {{English}} Lexical Sample, {{SRL}} and All Words},
  shorttitle = {{{SemEval-2007}} Task 17},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Semantic Evaluations}} - {{SemEval}} '07},
  author = {Pradhan, Sameer S. and Loper, Edward and Dligach, Dmitriy and Palmer, Martha},
  date = {2007},
  pages = {87--92},
  publisher = {{Association for Computational Linguistics}},
  location = {{Prague, Czech Republic}},
  doi = {10.3115/1621474.1621490},
  url = {http://portal.acm.org/citation.cfm?doid=1621474.1621490},
  urldate = {2023-06-27},
  abstract = {This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 – Lexical Sample, Semantic Role Labeling (SRL) and All-Words respectively. We tabulate and analyze the results of participating systems.},
  eventtitle = {The 4th {{International Workshop}}},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\WXHN2Z2U\Pradhan et al. - 2007 - SemEval-2007 task 17 English lexical sample, SRL .pdf}
}

@book{purves-brannon:2013:principles-cognitive-neuroscience,
  title = {Principles of Cognitive Neuroscience},
  editor = {Purves, Dale and Brannon, Elizabeth M.},
  date = {2013},
  edition = {2. ed},
  publisher = {{Sinauer}},
  location = {{Sunderland, Mass}},
  isbn = {978-0-87893-573-4},
  langid = {english},
  pagetotal = {601},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\XKKHDBM8\Purves et Brannon - 2013 - Principles of cognitive neuroscience.pdf}
}

@inproceedings{pustejovsky-stubbs:2012:natural-language-annotation,
  title = {Natural Language Annotation for Machine Learning},
  author = {Pustejovsky, James and Stubbs, Amber},
  date = {2012-10-10},
  publisher = {{O'Reilly Media, Inc.}},
  url = {https://api.semanticscholar.org/CorpusID:60457717},
  abstract = {This book is intended as a resource for people who are interested in using computers to help process natural language. A natural language refers to any language spoken by humans, either currently (e.g., English, Chinese, Spanish) or in the past (e.g., Latin, ancient Greek, Sanskrit). Annotation refers to the process of adding metadata informa tion to the text in order to augment a computer’s capability to perform Natural Language Processing (NLP). In particular, we examine how information can be added to natural language text through annotation in order to increase the performance of machine learning algorithms—computer programs designed to extrapolate rules from the infor mation provided over texts in order to apply those rules to unannotated texts later on.},
  isbn = {978-1-4493-0666-3},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\YTFHRWN9\Pustejovsky et Stubbs - 2012 - Natural language annotation for machine learning.pdf}
}

@report{r-core-team:2017:language-environment-statistical,
  title = {R: {{A}} Language and Environment for Statistical   Computing},
  author = {R Core Team},
  date = {2017},
  institution = {{R Foundation for Statistical Computing}},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  langid = {english},
  keywords = {*CITATION}
}

@article{radford-etal:2019:language-models-are,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  journaltitle = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\ARNHQL4P\Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@article{radovilsky-etal:2018:skills-requirements-business,
  title = {Skills {{Requirements}} of {{Business Data Analytics}} and {{Data Science Jobs}}: {{A Comparative Analysis}}},
  author = {Radovilsky, Zinovy and Hegde, Vishwanath and Acharya, Anuja},
  date = {2018},
  volume = {16},
  number = {1},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\TPG8D56J\Radovilsky et al. - 2018 - Skills Requirements of Business Data Analytics and.pdf}
}

@article{rahm-etal:2000:data-cleaning-problems,
  title = {Data Cleaning: {{Problems}} and Current Approaches},
  author = {Rahm, Erhard and Do, Hong Hai and others},
  date = {2000},
  journaltitle = {IEEE Data Eng. Bull.},
  volume = {23},
  number = {4},
  pages = {3--13},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\IIPJ7FGH\Rahm et al. - 2000 - Data cleaning Problems and current approaches.pdf}
}

@article{rajbahadur-etal:2022:can-use-this,
  title = {Can {{I}} Use This Publicly Available Dataset to Build Commercial {{AI}} Software? -- {{A Case Study}} on {{Publicly Available Image Datasets}}},
  shorttitle = {Can {{I}} Use This Publicly Available Dataset to Build Commercial {{AI}} Software?},
  author = {Rajbahadur, Gopi Krishnan and Tuck, Erika and Zi, Li and Lin, Dayi and Chen, Boyuan and Ming, Zhen and Jiang and German, Daniel M.},
  date = {2022-04-11},
  journaltitle = {ArXiv preprint},
  url = {http://arxiv.org/abs/2111.02374},
  urldate = {2023-09-29},
  abstract = {Publicly available datasets are one of the key drivers for commercial AI software. The use of publicly available datasets (particularly for commercial purposes) is governed by dataset licenses. These dataset licenses outline the rights one is entitled to on a given dataset and the obligations that one must fulfil to enjoy such rights without any license compliance violations. However, unlike standardized Open Source Software (OSS) licenses, existing dataset licenses are defined in an ad-hoc manner and do not clearly outline the rights and obligations associated with their usage. This makes checking for potential license compliance violations difficult. Further, a public dataset may be hosted in multiple locations and created from multiple data sources each of which may have different licenses. Hence, existing approaches on checking OSS license compliance cannot be used. In this paper, we propose a new approach to assess the potential license compliance violations if a given publicly available dataset were to be used for building commercial AI software. We conduct trials of our approach on two product groups within Huawei on 6 commonly used publicly available datasets. Our results show that there are risks of license violations on 5 of these 6 studied datasets if they were used for commercial purposes. Consequently, we provide recommendations for AI engineers on how to better assess publicly available datasets for license compliance violations.},
  langid = {english},
  keywords = {*CITATION,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\SVEZGLXJ\Rajbahadur et al. - 2022 - Can I use this publicly available dataset to build.pdf}
}

@article{ramesh-etal:2021:zeroshot-texttoimage-generation,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.2102.12092},
  url = {https://arxiv.org/abs/2102.12092},
  urldate = {2023-07-25},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  keywords = {*CITATION,Computer Vision and Pattern Recognition,FOS: Computer and information sciences,Machine learning},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\BAFHIAGD\Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf}
}

@article{ramos:2003:using-tfidf-determine,
  title = {Using {{TF-IDF}} to {{Determine Word Relevance}} in {{Document Queries}}},
  author = {Ramos, Juan},
  date = {2003},
  journaltitle = {Proceedings of the first instructional conference on machine learning},
  abstract = {In this paper, we examine the results of applying Term Frequency Inverse Document Frequency (TF-IDF) to determine what words in a corpus of documents might be more favorable to use in a query. As the term implies, TF-IDF calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in. Words with high TF-IDF numbers imply a strong relationship with the document they appear in, suggesting that if that word were to appear in a query, the document could be of interest to the user. We provide evidence that this simple algorithm efficiently categorizes relevant words that can enhance query retrieval.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\Q7PCWF7S\Ramos - Using TF-IDF to Determine Word Relevance in Docume.pdf}
}

@book{raschka-mirjalili:2019:python-machine-learning,
  title = {Python Machine Learning: Machine Learning and Deep Learning with {{Python}}, Scikit-Learn, and {{TensorFlow}} 2},
  shorttitle = {Python Machine Learning},
  author = {Raschka, Sebastian and Mirjalili, Vahid},
  date = {2019},
  series = {Expert Insight},
  edition = {Third edition},
  publisher = {{Packt}},
  location = {{Birmingham Mumbai}},
  isbn = {978-1-78995-575-0},
  langid = {english},
  pagetotal = {741},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\IZWAM845\Raschka et Mirjalili - 2019 - Python machine learning machine learning and deep.pdf}
}

@dataset{re3data.org:2013:zenodo,
  title = {Zenodo},
  author = {{Re3data.Org}},
  date = {2013},
  doi = {10.17616/R3QP53},
  url = {https://www.re3data.org/repository/r3d100010468},
  urldate = {2023-10-04},
  abstract = {ZENODO builds and operates a simple and innovative service that enables researchers, scientists, EU projects and institutions to share and showcase multidisciplinary research results (data and publications) that are not part of the existing institutional or subject-based repositories of the research communities. ZENODO enables researchers, scientists, EU projects and institutions to: easily share the long tail of small research results in a wide variety of formats including text, spreadsheets, audio, video, and images across all fields of science. display their research results and get credited by making the research results citable and integrate them into existing reporting lines to funding agencies like the European Commission. easily access and reuse shared research results.},
  langid = {english},
  keywords = {*CITATION,1 Humanities and Social Sciences,2 Life Sciences,3 Natural Sciences,4 Engineering Sciences}
}

@online{roach:2023:how-microsoft-bet,
  type = {Blog},
  title = {How {{Microsoft}}’s Bet on {{Azure}} Unlocked an {{AI}} Revolution},
  author = {Roach, John},
  date = {2023},
  url = {https://news.microsoft.com/},
  organization = {{Microsoft}},
  keywords = {*CITATION}
}

@article{rosenberg-hirschberg:2007:vmeasure-conditional-entropybased,
  title = {V-{{Measure}}: {{A Conditional Entropy-Based External Cluster Evaluation Measure}}},
  author = {Rosenberg, Andrew and Hirschberg, Julia},
  date = {2007},
  abstract = {We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the “problem of matching”, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\HK8LKIDM\Rosenberg et Hirschberg - V-Measure A Conditional Entropy-Based External Cl.pdf}
}

@article{rowe:2023:it-destroyed-me,
  entrysubtype = {newspaper},
  title = {"{{It}}'s Destroyed Me Completely": {{Kenyan}} Moderators Decry Toll of Training of {{AI}} Models},
  author = {Rowe, Niamh},
  date = {2023-08-02},
  journaltitle = {The Guardian},
  url = {https://www.theguardian.com/technology/2023/aug/02/ai-chatbot-training-human-toll-content-moderator-meta-openai},
  urldate = {2023-09-29},
  abstract = {Employees describe the psychological trauma of reading and viewing graphic content, low pay and abrupt dismissals},
  journalsubtitle = {Artificial Intelligence},
  langid = {english},
  keywords = {*CITATION}
}

@article{ruiz-etal:2010:densitybased-semisupervised-clustering,
  title = {Density-Based Semi-Supervised Clustering},
  author = {Ruiz, Carlos and Spiliopoulou, Myra and Menasalvas, Ernestina},
  date = {2010-11},
  journaltitle = {Data Min Knowl Disc},
  volume = {21},
  number = {3},
  pages = {345--370},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-009-0157-y},
  url = {http://link.springer.com/10.1007/s10618-009-0157-y},
  urldate = {2023-01-12},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Constrained clustering,DBScan},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\T78I7SNJ\Ruiz et al. - 2010 - Density-based semi-supervised clustering.pdf}
}

@inproceedings{sagot-etal:2011:turc-mecanique-pour,
  title = {Un Turc Mécanique Pour Les Ressources Linguistiques : Critique de La Myriadisation Du Travail Parcellisé},
  booktitle = {{{TALN}}'2011 - Traitement Automatique Des Langues Naturelles},
  author = {Sagot, Benoît and Fort, Karen and Adda, Gilles and Mariani, Joseph and Lang, Bernard},
  date = {2011-06},
  location = {{Montpellier, France}},
  url = {https://inria.hal.science/inria-00617067},
  abstract = {Cet article est une prise de position concernant les plate-formes de type Amazon Mechanical Turk, dont l'utilisation est en plein essor depuis quelques années dans le traitement automatique des langues. Ces plateformes de travail en ligne permettent, selon le discours qui prévaut dans les articles du domaine, de faire développer toutes sortes de ressources linguistiques de qualité, pour un prix imbattable et en un temps très réduit, par des gens pour qui il s'agit d'un passe-temps. Nous allons ici démontrer que la situation est loin d'être aussi idéale, que ce soit sur le plan de la qualité, du prix, du statut des travailleurs ou de l'éthique. Nous rappellerons ensuite les solutions alternatives déjà existantes ou proposées. Notre but est ici double : informer les chercheurs, afin qu'ils fassent leur choix en toute connaissance de cause, et proposer des solutions pratiques et organisationnelles pour améliorer le développement de nouvelles ressources linguistiques en limitant les risques de dérives éthiques et légales, sans que cela se fasse au prix de leur coût ou de leur qualité.},
  keywords = {*CITATION,Amazon Mechanical Turk,Language resources,Ressources linguistiques},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\X5MRGBD3\Sagot et al. - 2011 - Un turc mécanique pour les ressources linguistique.pdf}
}

@article{santorini:1990:partofspeech-tagging-guidelines,
  title = {Part-of-{{Speech Tagging Guidelines}} for the {{Penn Treebank Project}}  3rd {{Revision}}, 2nd Printing},
  author = {Santorini, Beatrice},
  date = {1990},
  url = {https://api.semanticscholar.org/CorpusID:60354878},
  abstract = {This manual addresses the linguistic issues that arise in connection with annotating texts by part of speech ("tagging"). Section 2 is an alphabetical list of the parts of speech encoded in the annotation systems of the Penn Treebank Project, along with their corresponding abbreviations ("tags") and some information concerning their definition. This section allows you to find an unfamiliar tag by looking up a familiar part of speech. Section 3 recapitulates the information in Section 2, but this time the information is alphabetically ordered by tags. This is the section to consult in order to find out what an unfamiliar tag means. Since the parts of speech are probably familiar to you from high school English, you should have little difficulty in assimilating the tags themselves. However, it is often quite difficult to decide which tag is appropriate in a particular context. The two sections 4 and 5 therefore include examples and guidelines on how to tag problematic cases. If you are uncertain about whether a given tag is correct or not, refer to these sections in order to ensure a consistently annotated text. Section 4 discusses parts of speech that are easily confused and gives guidelines on how to tag such cases, while Section 5 contains an alphabetical list of specific problematic words and collocations. Finally, Section 6 discusses some general tagging conventions. One general rule, however, is so important that we state it here. Many texts are not models of good prose, and some contain outright errors and slips of the pen. Do not be tempted to correct a tag to what it would be if the text were correct; rather, it is the incorrect word that should be tagged correctly. Disciplines Computer Sciences Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-47. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis\_reports/570 Part-of-S peech Tagging Guidelines For The Penn Treebank Project (3rd Revision) MS-CIS-90-47 LINC LAB 178},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\57VDC2LS\Santorini - Part-of-Speech Tagging Guidelines for the Penn Tre.pdf}
}

@article{sasaki:2007:truth-fmeasure,
  title = {The Truth of the {{F-measure}}},
  author = {Sasaki, Yutaka},
  date = {2007-10-26},
  abstract = {It has been past more than 15 years since the F-measure was first introduced to evaluation tasks of information extraction technology at the Fourth Message Understanding Conference (MUC-4) in 1992. Recently, sometimes I see some confusion with the definition of the Fmeasure, which seems to be triggered by lack of background knowledge about how the F-measure was derived. Since I was not involved in the process of the introduction or device of the F-measure, I might not be the best person to explain this but I hope this note would be a little help for those who are wondering what the F-measure really is. This introduction is devoted to provide brief but sufficient information on the F-measure.},
  langid = {english},
  keywords = {*CITATION,fscore},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\GEYNFWLW\Sasaki - 2007 - The truth of the F-measure.pdf}
}

@dataset{schild-adler:2023:subset-mlsum-multilingual,
  title = {Subset of 'MLSUM: The Multilingual Summarization Corpus' for constraints annotation experiment},
  shorttitle = {Subset of 'MLSUM},
  author = {Schild, Erwan and Adler, Marie},
  date = {2023-10-02},
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.8399301},
  url = {https://zenodo.org/record/8399301},
  urldate = {2023-10-02},
  abstract = {{$<$}strong{$>$}[EN] Subset of 'MLSUM: The Multilingual Summarization Corpus' for constraints annotation experiment.{$<$}/strong{$>$} {$<$}strong{$>$}Description{$<$}/strong{$>$}: MLSUM is a dataset of newspappers articles aimed at training summaring model. We use it for a constraints annotation experiment on newspapper titles according to their topic classification. {$<$}strong{$>$}Content{$<$}/strong{$>$}: For constraints annotation experiment based on data similarity, this dataset have been subsetted (randomly pick 75 articles in the following 14 most used topics: 'economie', 'politique', 'sport', 'planete' (renamed in 'ecologie'), 'sciences', 'police-justice', 'disparitions', 'emploi', 'sante', 'musiques', 'arts', 'educations', 'climat' (renamed in 'meteo'), 'immobilier') and filtered (keep articles that have an obvious topics regarding their titles, without their bodies). Two reviewers have working on this task in order to limit the subjectivity of the filtering. This subsetted dataset is used (1) to estimate needed time to annotate titles similarity with constraints (MUST-LINK, CANNOT-LINK) and (2) to test interactive clustering methodology (constraints annotation and constrained clustering). {$<$}strong{$>$}Origin{$<$}/strong{$>$}: The dataset is bassed on the original 'MLSUM: The Multilingual Summarization Corpus' dataset (https://doi.org/10.48550/arXiv.2004.14900). {$<$}br{$>$} {$<$}strong{$>$}[FR] Echantillon de 'MLSUM: The Multilingual Summarization Corpus' pour une expériemnt d'annotation de contraintes.{$<$}/strong{$>$} {$<$}strong{$>$}Description {$<$}/strong{$>$}: MLSUM est un ensemble de données d'articles de journaux destinés à l'entraînement d'un modèle de résumé automatique. Nous l'utilisons pour une expérience d'annotation de contraintes sur des titres de journaux en fonction de leur classification thématique. {$<$}strong{$>$}Contenu {$<$}/strong{$>$}: Pour une expérience d'annotation de contraintes basée sur la similarité des données, cet ensemble de données a été échantillonné (sélectionner au hasard de 75 articles dans les 14 sujets les plus utilisés : 'économie', 'politique', 'sport', 'planète' (renommé en « écologie »). ), 'sciences', 'police-justice', 'disparitions', 'emploi', 'sante', 'musiques', 'arts', 'éducations', 'climat' (renommé en 'meteo'), 'immobilier' ) et filtré (conserver les articles qui ont un sujet évident par rapport à leur titre, sans leur corps). Deux relecteurs ont travaillé sur cette tâche afin de limiter la subjectivité du filtrage. Ce sous-ensemble de données est utilisé (1) pour estimer le temps nécessaire pour annoter la similarité des titres avec des contraintes (MUST-LINK, CANNOT-LINK) et (2) pour tester la méthodologie de clustering interactif (annotation de contraintes et clustering contraint). {$<$}strong{$>$}Origine {$<$}/strong{$>$}: L'ensemble de données est basé sur l'ensemble de données original 'MLSUM : The Multilingual Summarization Corpus' (https://doi.org/10.48550/arXiv.2004.1490).},
  langid = {french},
  version = {1.0.0 [subset: fr+train+filtered]},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,Computation and Language,Natural Language Processing,Newspapper article,Trainset},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\8MEAE6CJ\\mlsum_fr_train_subset_v1.0.0.schild.xlsx;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\VN46BQAB\\mlsum_fr_train_subset_v1.0.0.schild_UNLABELLED.xlsx}
}

@inproceedings{schild-etal:2021:conception-iterative-semisupervisee,
  title = {Conception itérative et semi-supervisée d'assistants conversationnels par regroupement interactif des questions},
  author = {Schild, Erwan and Durantin, Gautier and Lamirel, Jean-Charles and Miconi, Florian},
  date = {2021-01-25},
  volume = {RNTI E-37},
  publisher = {{Edition RNTI}},
  url = {https://hal.inria.fr/hal-03133007},
  urldate = {2021-06-14},
  abstract = {La création d’un jeu de données pour l’entrainement d’un chatbot repose sur un a priori de connaissance du domaine. En conséquence, cette étape est le plus souvent manuelle, fastidieuse et soumise aux biais. Pour garantir l’efficacité et l’objectivité de l’annotation, nous proposons une méthodologie d’apprentissage actif par annotation de contraintes. Il s’agit d’une approche itérative, reposant sur un algorithme de clustering pour segmenter les données et tirant parti de la connaissance de l’annotateur pour guider le regroupement des questions en une structure d’intentions. Dans cet article, nous étudions les paramètres optimaux de modélisation pour réaliser une segmentation exploitable en un minimum d’annotations, et montrons que cette approche permet d’aboutir à une structure cohérente pour l’entrainement d’un assistant conversationnel.},
  eventtitle = {EGC 2021 - 21èmes Journées Francophones Extraction et Gestion des Connaissances},
  langid = {french},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,INTERACTIVE\_CLUSTERING},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\T7NYJRY8\Schild et al. - 2021 - Conception itérative et semi-supervisée d'assistan.pdf}
}

@inproceedings{schild-etal:2021:concevoir-assistant-conversationnel,
  title = {Concevoir un assistant conversationnel de manière itérative et semi-supervisée avec le clustering interactif},
  shorttitle = {Concevoir un assistant conversationnel avec le clustering interactif},
  booktitle = {TextMine 2021 (TM'2021) - En conjonction avec EGC 2021},
  author = {Schild, Erwan and Durantin, Gautier and Lamirel, Jean-Charles},
  date = {2021-01-26},
  pages = {11--14},
  publisher = {{Pascal Cuxac, Vincent Lemaire, Cédric Lopez}},
  url = {https://hal.inria.fr/hal-03133060},
  abstract = {La création d’un jeu de données nécessaire à la conception d’un assistant conversationnel résulte le plus souvent d’une étape manuelle et fastidieuse qui manque de techniques destinées à l’assister. Pour accélérer cette étape d’annotation, nous proposons une méthode de clustering interactif : il s’agit d’une approche itérative inspirée de l’apprentissage actif, reposant sur un algorithme de clustering et tirant parti d’une annotation de contraintes pour guider le regroupement des questions en une structure d’intentions. Dans cet article, nous exposons la méthodologie à mettre en oeuvre pour concevoir un assistant conversationnel opérationnel à l’aide du clustering interactif.},
  eventtitle = {Atelier - Fouille de Textes - Text Mine 2021 - En conjonction avec EGC 2021},
  langid = {french},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,INTERACTIVE\_CLUSTERING},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\3GD3XG3E\\Schild et al. - 2021 - Concevoir un assistant conversationnel de manière .pdf;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\IHVRXA2V\\Schild et al - 2021 - PRESENTATION.pdf}
}

@software{schild-etal:2022:cognitivefactory-interactiveclusteringgui,
  title = {Cognitivefactory/Interactive-Clustering-Gui},
  shorttitle = {Cognitivefactory/Interactive-Clustering-Gui},
  author = {SCHILD, Erwan and TTremble and Clementine-Msk},
  date = {2022-09-01},
  url = {https://zenodo.org/record/4775270},
  urldate = {2023-02-13},
  abstract = {Release {$<$}code{$>$}0.4.0{$<$}/code{$>$} of {$<$}code{$>$}cognitivefactory/interactive-clustering-gui{$<$}/code{$>$} package. {$<$}em{$>$}GitHub repository{$<$}/em{$>$} : https://github.com/cognitivefactory/interactive-clustering-gui/tree/0.4.0 {$<$}em{$>$}Main documentation{$<$}/em{$>$} : https://cognitivefactory.github.io/interactive-clustering-gui/ {$<$}em{$>$}Pypi distribution{$<$}/em{$>$} : https://pypi.org/project/cognitivefactory-interactive-clustering-gui/0.4.0/},
  organization = {{Zenodo}},
  version = {0.4.0},
  keywords = {*CITATION}
}

@article{schild-etal:2022:iterative-semisupervised-design,
  title = {Iterative and {{Semi-Supervised Design}} of {{Chatbots Using Interactive Clustering}}},
  shorttitle = {Iterative and {{Semi-Supervised Design}} of {{Chatbots Using Interactive Clustering}}},
  author = {Schild, Erwan and Durantin, Gautier and Lamirel, Jean-Charles and Miconi, Florian},
  date = {2022-04-01},
  journaltitle = {International Journal of Data Warehousing and Mining (IJDWM)},
  volume = {18},
  number = {2},
  pages = {1--19},
  issn = {1548-3924},
  doi = {10.4018/IJDWM.298007},
  url = {https://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJDWM.298007},
  abstract = {Chatbots represent a promising tool to automate the processing of requests in a business context. However, despite major progress in natural language processing technologies, constructing a dataset deemed relevant by business experts is a manual, iterative and error-prone process. To assist these experts during modelling and labelling, the authors propose an active learning methodology coined Interactive Clustering. It relies on interactions between computer-guided segmentation of data in intents, and response-driven human annotations imposing constraints on clusters to improve relevance.This article applies Interactive Clustering on a realistic dataset, and measures the optimal settings required for relevant segmentation in a minimal number of annotations. The usability of the method is discussed in terms of computation time, and the achieved compromise between business relevance and classification performance during training.In this context, Interactive Clustering appears as a suitable methodology combining human and computer initiatives to efficiently develop a useable chatbot.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,INTERACTIVE\_CLUSTERING},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\QPFCLFA7\\Schild et al. - 2022 - Iterative and Semi-Supervised Design of Chatbots U.pdf;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\TW3MBXVX\\Schild et al. - 2022 - Iterative and Semi-Supervised Design of Chatbots U.pdf}
}

@software{schild:2021:cognitivefactory-interactiveclusteringcomparativestudy,
  title = {Cognitivefactory/Interactive-Clustering-Comparative-Study},
  author = {SCHILD, Erwan},
  date = {2022-11-05},
  url = {https://zenodo.org/record/5648255},
  urldate = {2023-02-13},
  abstract = {Release {$<$}code{$>$}0.1.0{$<$}/code{$>$} of {$<$}code{$>$}cognitivefactory/interactive-clustering-comparative-study{$<$}/code{$>$} repository. {$<$}em{$>$}GitHub repository{$<$}/em{$>$} : https://github.com/cognitivefactory/interactive-clustering-comparative-study/tree/0.1.0},
  organization = {{Zenodo}},
  version = {0.1.0},
  keywords = {*CITATION,Clustering,Comparative-study,Constraints,Interactive-clustering,Natural language processing,Python}
}

@software{schild:2022:cognitivefactory-interactiveclustering,
  title = {Cognitivefactory/Interactive-Clustering},
  shorttitle = {Cognitivefactory/Interactive-Clustering},
  author = {SCHILD, Erwan},
  date = {2022-08-22},
  url = {https://zenodo.org/record/4775251},
  urldate = {2023-02-13},
  abstract = {Release {$<$}code{$>$}0.5.2{$<$}/code{$>$} of {$<$}code{$>$}cognitivefactory/interactive-clustering{$<$}/code{$>$} package. {$<$}em{$>$}GitHub repository{$<$}/em{$>$} : https://github.com/cognitivefactory/interactive-clustering/tree/0.5.2 {$<$}em{$>$}Main documentation{$<$}/em{$>$} : https://cognitivefactory.github.io/interactive-clustering/ {$<$}em{$>$}Pypi distribution{$<$}/em{$>$} : https://pypi.org/project/cognitivefactory-interactive-clustering/0.5.2/},
  organization = {{Zenodo}},
  version = {0.5.2},
  keywords = {*CITATION}
}

@dataset{schild:2022:french-trainset-chatbots,
  title = {French trainset for chatbots dealing with usual requests on bank cards},
  author = {Schild, Erwan},
  date = {2022-11-09},
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.4769949},
  url = {https://zenodo.org/record/4769949},
  urldate = {2023-02-13},
  abstract = {{$<$}strong{$>$}[EN] French training dataset for chatbots dealing with usual requests on bank cards.{$<$}/strong{$>$} {$<$}strong{$>$}Description{$<$}/strong{$>$}: This dataset represents examples of common customer requests relating to bank cards management. It can be used as a training set for a small chatbot intended to process these usual requests. {$<$}strong{$>$}Content{$<$}/strong{$>$}: The questions are asked in French. The dataset is divided into 10 intents of 100 questions each, for a total of 1 000 questions. {$<$}strong{$>$}Intents scope{$<$}/strong{$>$}: Intents are constructed in such a way that all questions arising from the same intention have the same response or action. The scope covered concerns: loss or theft of cards; the swallowed card; the card order; consultation of the bank balance; insurance provided by a card; card unlocking; virtual card management; management of bank overdraft; management of payment limits; management of contactless mode. {$<$}strong{$>$}Origin{$<$}/strong{$>$}: Intents scope is inspired by a chatbot currently in production, and the wording of the questions are inspired by the usual customers requests. {$<$}br{$>$} {$<$}strong{$>$}[FR] Jeu d'entraînement en français d'assistants conversationnels traitant des demandes courantes sur les cartes bancaires.{$<$}/strong{$>$} {$<$}strong{$>$}Description {$<$}/strong{$>$}: Cet ensemble de données représente des exemples de demandes usuelles des clients concernant la gestion des cartes bancaires. Il peut être utilisé comme jeu d'entraînement pour un assistant conversationnel destiné à traiter ces demandes courantes. {$<$}strong{$>$}Contenu {$<$}/strong{$>$}: Les questions sont formulées en français. L'ensemble de données est divisé en 10 intentions de 100 questions chacune, pour un total de 1 000 questions. {$<$}strong{$>$}Périmètre des intentions{$<$}/strong{$>$} : Les intentions sont construites de telle manière que toutes les questions issues d'une même intention ont la même réponse ou action. Le périmètre couvert concerne : la perte ou le vol de cartes ; la carte avalée ; la commande des cartes ; la consultation du solde bancaire ; l'assurance fournie par une carte ; le déverrouillage de la carte ; la gestion de cartes virtuelles ; la gestion du découvert bancaire ; la gestion des plafonds de paiement ; la gestion du mode sans contact. {$<$}strong{$>$}Origine {$<$}/strong{$>$}: Le périmètre des intentions est inspiré par un chatbot actuellement en production, et la formulation des questions est inspirée de demandes courantes de clients.},
  langid = {french},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,Bank cards management,Chatbot,Natural language processing,Trainset},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\2YJEM4BD\\French_trainset_for_chatbots_dealing_with_usual_requests_on_bank_cards_v1.0.0.xlsx;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\FGW9EV2H\\French_trainset_for_chatbots_dealing_with_usual_requests_on_bank_cards_v2.0.0_UNLABELED.xlsx;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\KYG43CY9\\French_trainset_for_chatbots_dealing_with_usual_requests_on_bank_cards_v2.0.0.xlsx}
}

@software{schild:2023:cognitivefactory-featuresmaximizationmetric,
  title = {Cognitivefactory/Features-Maximization-Metric},
  shorttitle = {Cognitivefactory/Features-Maximization-Metric},
  author = {SCHILD, Erwan},
  date = {2023-02-16},
  url = {https://zenodo.org/record/7646382},
  urldate = {2023-02-16},
  abstract = {Release {$<$}code{$>$}0.1.1{$<$}/code{$>$} of {$<$}code{$>$}cognitivefactory/features-maximization-metric{$<$}/code{$>$} package. {$<$}em{$>$}GitHub repository{$<$}/em{$>$} : https://github.com/cognitivefactory/features-maximization-metric/tree/0.1.1 {$<$}em{$>$}Main documentation{$<$}/em{$>$} : https://cognitivefactory.github.io/features-maximization-metric/ {$<$}em{$>$}Pypi distribution{$<$}/em{$>$} : https://pypi.org/project/cognitivefactory-features-maximization-metric/0.1.1/},
  organization = {{Zenodo}},
  version = {0.1.1},
  keywords = {*CITATION}
}

@article{schuurmans-frasincar:2020:intent-classification-dialogue,
  title = {Intent {{Classification}} for {{Dialogue Utterances}}},
  author = {Schuurmans, Jetze and Frasincar, Flavius},
  date = {2020-01-01},
  journaltitle = {IEEE Intell. Syst.},
  volume = {35},
  number = {1},
  pages = {82--88},
  issn = {1541-1672, 1941-1294},
  doi = {10.1109/MIS.2019.2954966},
  url = {https://ieeexplore.ieee.org/document/8910417/},
  urldate = {2023-07-26},
  abstract = {In this work we investigate several machine learning methods to tackle the problem of intent classification for dialogue utterances. We start with Bag-of-Words (BoW) in combination with Na¨ıve Bayes (NB). After that, we employ Continuous Bag-of-Words (CBoW) coupled with Support Vector Machines (SVM). Then follow Long Short-Term Memory (LSTM) networks, which are made bidirectional. The best performing model is hierarchical, such that it can take advantage of the natural taxonomy within classes. The main experiments are a comparison between these methods on an open sourced academic dataset. In the first experiment we consider the full dataset. We also consider the given subsets of data separately, in order to compare our results with state-of-the-art vendor solutions. In general we find that the SVM models outperform the LSTM models. The former models achieve the highest macro-F1 for the full dataset, and in most of the individual datasets. We also found out that the incorporation of the hierarchical structure in the intents improves the performance.},
  langid = {english},
  keywords = {*CITATION,Chatbot,Intention},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\FF7R2HHE\Schuurmans et Frasincar - 2020 - Intent Classification for Dialogue Utterances.pdf}
}

@dataset{scialom-etal:2020:mlsum-multilingual-summarization,
  title = {{{MLSUM}}: {{The Multilingual Summarization Corpus}}},
  shorttitle = {{{MLSUM}}},
  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},
  date = {2020-04-30},
  number = {arXiv:2004.14900},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2004.14900},
  urldate = {2023-06-07},
  abstract = {We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Computation and Language},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\SJDKVTSQ\Scialom et al. - 2020 - MLSUM The Multilingual Summarization Corpus.pdf}
}

@inproceedings{seabold-perktold:2010:statsmodels-econometric-statistical,
  title = {Statsmodels: {{Econometric}} and {{Statistical Modeling}} with {{Python}}},
  shorttitle = {Statsmodels},
  author = {Seabold, Skipper and Perktold, Josef},
  date = {2010},
  pages = {92--96},
  location = {{Austin, Texas}},
  doi = {10.25080/Majora-92bf1922-011},
  url = {https://conference.scipy.org/proceedings/scipy2010/seabold.html},
  urldate = {2023-07-07},
  eventtitle = {Python in {{Science Conference}}},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\N79637BD\Seabold et Perktold - 2010 - Statsmodels Econometric and Statistical Modeling .pdf}
}

@article{settles:2010:active-learning-literature,
  title = {Active {{Learning Literature Survey}}},
  author = {Settles, Burr},
  date = {2010},
  pages = {67},
  abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
  langid = {english},
  keywords = {*CITATION,Active learning},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\SRWU4YSI\Settles - 2010 - Active Learning Literature Survey.pdf}
}

@report{sheridan-verplank:1978:human-computer-control,
  title = {Human and {{Computer Control}} of {{Undersea Teleoperators}}},
  shorttitle = {Human and {{Computer Control}} of {{Undersea Teleoperators}}},
  author = {Sheridan, Thomas B. and Verplank, William L.},
  date = {1978-07-15},
  institution = {{Defense Technical Information Center}},
  location = {{Fort Belvoir, VA}},
  url = {http://www.dtic.mil/docs/citations/ADA057655},
  urldate = {2023-09-01},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\HIXFEHEF\Sheridan et Verplank - 1978 - Human and Computer Control of Undersea Teleoperato.pdf}
}

@article{shorten-etal:2021:text-data-augmentation,
  title = {Text {{Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M. and Furht, Borko},
  date = {2021-12},
  journaltitle = {J Big Data},
  volume = {8},
  number = {1},
  pages = {101},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00492-0},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00492-0},
  urldate = {2023-09-29},
  abstract = {Natural Language Processing (NLP) is one of the most captivating applications of Deep Learning. In this survey, we consider how the Data Augmentation training strategy can aid in its development. We begin with the major motifs of Data Augmentation summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form. We follow these motifs with a concrete list of augmentation frameworks that have been developed for text data. Deep Learning generally struggles with the measurement of generalization and characterization of overfitting. We highlight studies that cover how augmentations can construct test sets for generalization. NLP is at an early stage in applying Data Augmentation compared to Computer Vision. We highlight the key differences and promising ideas that have yet to be tested in NLP. For the sake of practical implementation, we describe tools that facilitate Data Augmentation such as the use of consistency regularization, controllers, and offline and online augmentation pipelines, to preview a few. Finally, we discuss interesting topics around Data Augmentation in NLP such as task-specific augmentations, the use of prior knowledge in self-supervised learning versus Data Augmentation, intersections with transfer and multi-task learning, and ideas for AI-GAs (AI-Generating Algorithms). We hope this paper inspires further research interest in Text Data Augmentation.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\JJ3JB4X5\Shorten et al. - 2021 - Text Data Augmentation for Deep Learning.pdf}
}

@article{shorten-khoshgoftaar:2019:survey-image-data,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  date = {2019-12},
  journaltitle = {J Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
  urldate = {2023-09-29},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\FL796M3N\Shorten et Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf}
}

@incollection{sinclair:2004:corpus-text-basic,
  title = {Corpus and {{Text}}: {{Basic Principles}}},
  booktitle = {Developing {{Linguistic Corpora}}: {{A Guide}} to {{Good Practice}}},
  author = {Sinclair, John},
  editor = {Wynne, Martin},
  date = {2004},
  edition = {Oxbow Books},
  pages = {1--16},
  publisher = {{AHDS: Literature, Languages, and Linguistics}},
  location = {{Oxford}},
  url = {http://ahds.ac.uk/creating/guides/linguistic-corpora/chapter1.htm},
  abstract = {A corpus is a remarkable thing, not so much because it is a collection of language text, but because of the properties that it acquires if it is well-designed and carefully-constructed. The guiding principles that relate corpus and text are concepts that are not strictly definable, but rely heavily on the good sense and clear thinking of the people involved, and feedback from a consensus of users. However unsteady is the notion of representativeness, it is an unavoidable one in corpus design, and others such as sample and balance need to be faced as well. It is probably time for linguists to be less squeamish about matters which most scientists take completely for granted. I propose to defer offering a definition of a corpus until after these issues have been aired, so that the definition, when it comes, rests on as stable foundations as possible. For this reason, the definition of a corpus will come at the end of this paper, rather than at the beginning.},
  isbn = {978-1-84217-205-6},
  langid = {english},
  keywords = {*CITATION}
}

@software{sncf:2018:agent-virtuel-sncf,
  title = {Agent Virtuel {{SNCF}}},
  author = {{SNCF}},
  date = {2018},
  url = {https://bot.assistant.sncf/index.html},
  keywords = {*CITATION}
}

@article{snow-etal:2008:cheap-fast-it,
  title = {Cheap and {{Fast}} - {{But}} Is It {{Good}}? {{Evaluating Non-Expert Annotations}} for {{Natural Language Tasks}}},
  author = {Snow, Rion and O'Connor, Brendan and Jurafsky, Daniel and Ng, Andrew},
  date = {2008-10},
  journaltitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  pages = {254--263},
  abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\7UTBTBGX\Snow et al. - 2008 - Cheap and Fast - But is it Good Evaluating Non-Ex.pdf}
}

@article{sparck-jones:1972:statistical-interpretation-term,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Sparck Jones, Karen},
  date = {1972-01},
  journaltitle = {Journal of Documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  issn = {0022-0418},
  doi = {10.1108/eb026526},
  url = {https://www.emerald.com/insight/content/doi/10.1108/eb026526/full/html},
  urldate = {2023-07-06},
  langid = {english},
  keywords = {*CITATION,TF-IDF,Vectorization}
}

@article{sperandio:1972:charge-travail-regulation,
  title = {Charge de Travail et Régulation Des Processus Opératoires},
  author = {Sperandio, Jean-Claude},
  date = {1972},
  journaltitle = {Le travail humain},
  pages = {85--98},
  keywords = {*CITATION}
}

@article{sperandio:1978:regulation-working-methods,
  title = {The {{Regulation}} of {{Working Methods}} as a {{Function}} of {{Work-load}} among {{Air Traffic Controllers}}},
  author = {Sperandio, Jean-Claude},
  date = {1978-03},
  journaltitle = {Ergonomics},
  volume = {21},
  number = {3},
  pages = {195--202},
  issn = {0014-0139, 1366-5847},
  doi = {10.1080/00140137808931713},
  url = {https://www.tandfonline.com/doi/full/10.1080/00140137808931713},
  urldate = {2023-09-01},
  langid = {english},
  keywords = {*CITATION}
}

@book{sperandio:1987:ergonomie-travail-mental,
  title = {L'ergonomie Du Travail Mental},
  author = {Sperandio, Jean-Claude},
  date = {1987},
  publisher = {{FeniXX}},
  keywords = {*CITATION}
}

@thesis{stubbs:2013:methodology-using-professional,
  type = {phdthesis},
  title = {A {{Methodology}} for {{Using Professional Knowledge}} in {{Corpus}}},
  author = {Stubbs, Amber C.},
  date = {2013},
  institution = {{Brandeis University}},
  abstract = {It is a well-known problem that performing linguistic annotation over a corpus can be an expensive and time-consuming task. The problem of annotation becomes even more difficult to solve when the task is based around a domain-specific corpus or specification. For example, extracting diagnosis information from clinical notes must be done by someone with sufficient medical training, who can understand all of the medical jargon and determine if a diagnosis can be made. However, hiring medical professionals to perform syntactic or semantic annotations can be extremely expensive, and few domain-expert annotators will have the time to create such an annotation.  This dissertation aims at finding a way to capture expert domain knowledge quickly and easily as annotations, and in a format where the information can then be used for more advanced natural language processing (NLP) tasks. To that end, this dissertation proposes the use of light annotation tasks: linguistically under-specified, task- and domain-specific annotation models that can quickly capture expert knowledge in a corpus as it relates to a research question. The corpora created from light annotation tasks can then be augmented with additional, denser annotations (such as part-of-speech tagging), or used directly with an NLP system.  In addition to defining the light annotation task, this dissertation presents a set of principles that can be used to create annotation tasks for domain experts. These principles are based on examining other “light” annotations, as well as the existing standards and methodologies used in more traditional annotation research. Software designed for light annotation projects is also presented.  Finally, in order to illustrate the utility of light annotations, a case study based around the medical research task of finding patients qualified to participate in a clinical study is presented. The medical settings that influence the case study\&\#39;s design are discussed, and the light annotation task\&\#39;s implementation is analyzed. The resulting corpus (called the Patient Evaluation Resource for Medical Information in Text (PERMIT) corpus) is then leveraged into a preliminary NLP system, which demonstrates the versatility of the light annotation methodology.},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\T47FZS2F\Stubbs - 2013 - A Methodology for Using Professional Knowledge in .pdf}
}

@online{team-datascientest:2022:metiers-data-mieux,
  title = {Les métiers de la data : mieux comprendre leurs différences},
  author = {{Team Datascientest}},
  date = {2022-09},
  url = {https://datascientest.com/les-metiers-de-la-data},
  urldate = {2023-09-26},
  abstract = {Les métiers relatifs aux données sont souvent sources d’incompréhension et sont également parfois soumis à une hiérarchie, à tort, puisqu’il s’agit de métiers bien différents. Les non-initiés, également appelés Moldus dans le milieu, peuvent y voir certaines zones d’ombre. En effet, l’erreur souvent commise est de parler de “Data Scientist” sans distinction pour englober les métiers de la data, alors qu’il s’agit bel et bien de rôles différents. Le but de cet article est de lever le voile sur les principaux métiers de la data, et de mieux comprendre leurs différences et spécificités. De plus, vous trouverez à la fin de cet article un diagramme illustrant les différents métiers..},
  langid = {french},
  organization = {{Datascientest.com}}
}

@article{thorndike:1953:who-belongs-family,
  title = {Who Belongs in the Family?},
  author = {Thorndike, Robert L.},
  date = {1953-12},
  journaltitle = {Psychometrika},
  volume = {18},
  number = {4},
  pages = {267--276},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/BF02289263},
  url = {http://link.springer.com/10.1007/BF02289263},
  urldate = {2023-07-06},
  langid = {english},
  keywords = {*CITATION,Elbow}
}

@article{touvron-etal:2023:llama-open-foundation,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  date = {2023},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.2307.09288},
  url = {https://arxiv.org/abs/2307.09288},
  urldate = {2023-07-20},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  keywords = {*CITATION,Artificial Intelligence,Computation and Language,FOS: Computer and information sciences},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\VFE8SRBF\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf}
}

@article{tukey:1949:comparing-individual-means,
  title = {Comparing {{Individual Means}} in the {{Analysis}} of {{Variance}}},
  author = {Tukey, John W.},
  date = {1949-06},
  journaltitle = {Biometrics},
  volume = {5},
  number = {2},
  eprint = {3001913},
  eprinttype = {jstor},
  pages = {99},
  issn = {0006341X},
  doi = {10.2307/3001913},
  url = {https://www.jstor.org/stable/3001913?origin=crossref},
  urldate = {2023-07-06},
  keywords = {*CITATION}
}

@online{uszkoreit:2017:transformer-novel-neural,
  title = {Transformer: {{A Novel Neural Network Architecture}} for {{Language Understanding}}},
  shorttitle = {Transformer},
  author = {Uszkoreit, Jakob},
  date = {2017-08-31},
  url = {http://ai.googleblog.com/2017/08/transformer-novel-neural-network.html},
  urldate = {2020-06-10},
  abstract = {Posted by Jakob Uszkoreit, Software Engineer, Natural Language Understanding   Neural networks, in particular recurrent neural networks  (RN...},
  langid = {english},
  organization = {{Google AI Blog}},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Attention,Computation and Language,Decoder,Encoder,Transformers},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\ZY5F5X6A\Uszkoreit - 2017 - Transformer A Novel Neural Network Architecture f.html}
}

@article{valette:2016:analyse-statistique-donnees,
  title = {Analyse statistique des données textuelles et traitement automatique des langues. Une étude comparée},
  author = {Valette, Mathieu},
  date = {2016-06},
  journaltitle = {International conference on statistical analysis of textual data (JADT2016)},
  series = {Proceedings of 13th international conference on statistical analysis of textual data, 7-10 june 2016, nice (france)},
  volume = {2},
  pages = {697--706},
  url = {https://inalco.hal.science/hal-01335084},
  abstract = {Notre propos dans cet article est de comparer le TAL et l’ADT selon des points de vue gnoséologique, méthodologique, applicatif et des objets d’étude (texte, corpus). Il s’agit d’expliciter, au moyen de cinq positions antagonistes, les relations entretenues par ces deux sous-disciplines pour envisager les terrains de conciliation possibles : (i) automatisation vs herméneutique ; (ii) tekhnè vs épistémè ; (iii) test vs jugement d’acceptabilité ; (v) algorithmique vs ergonomie ; (v) corpus comme ressources vs corpus comme sources.},
  langid = {french},
  keywords = {discussion,Methods,NLP,Statistical Analysis of Textual Data,Textometrics},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\M7JCR35V\Valette - 2016 - Analyse statistique des données textuelles et trai.pdf}
}

@inproceedings{van-der-goot:2021:we-need-talk,
  title = {We {{Need}} to {{Talk About}} Train-Dev-Test {{Splits}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Van Der Goot, Rob},
  date = {2021},
  pages = {4485--4494},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.368},
  url = {https://aclanthology.org/2021.emnlp-main.368},
  urldate = {2023-09-22},
  abstract = {Standard train-dev-test splits used to benchmark multiple models against each other are ubiquitously used in Natural Language Processing (NLP). In this setup, the train data is used for training the model, the development set for evaluating different versions of the proposed model(s) during development, and the test set to confirm the answers to the main research question(s). However, the introduction of neural networks in NLP has led to a different use of these standard splits; the development set is now often used for model selection during the training procedure.Because of this, comparing multiple versions of the same model during development leads to overestimation on the development data. As an effect, people have started to compare an increasing amount of models on the test data, leading to faster overfitting and “expiration” of our test sets. We propose to use a tune-set when developing neural network methods, which can be used for model picking so that comparing the different versions of a new model can safely be done on the development data.},
  eventtitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\693AGW26\Van Der Goot - 2021 - We Need to Talk About train-dev-test Splits.pdf}
}

@book{van-rossum-drake:2009:python-reference-manual,
  title = {Python 3 {{Reference Manual}}},
  author = {Van Rossum, Guido and Drake, Fred L.},
  date = {2009},
  edition = {CreateSpace},
  location = {{Scotts Valley, CA}},
  isbn = {1-4414-1269-7},
  keywords = {*CITATION}
}

@article{von-ahn:2006:games-purpose,
  title = {Games with a {{Purpose}}},
  author = {Von Ahn, L.},
  date = {2006-06},
  journaltitle = {Computer},
  volume = {39},
  number = {6},
  pages = {92--94},
  issn = {0018-9162},
  doi = {10.1109/MC.2006.196},
  url = {http://ieeexplore.ieee.org/document/1642623/},
  urldate = {2023-10-09},
  abstract = {Through online games, people can collectively solve large-scale computational problems. Such games constitute a general mechanism for using brain power to solve open problems. In fact, designing such a game is much like designing an algorithm - it must be proven correct, its efficiency can be analyzed, a more efficient version can supersede a less efficient one, and so on. "Games with a purpose" have a vast range of applications in areas as diverse as security, computer vision, Internet accessibility, adult content filtering, and Internet search. Any game designed to address these and other problems must ensure that game play results in a correct solution and, at the same time, is enjoyable. People will play such games to be entertained, not to solve a problem - no matter how laudable the objective},
  langid = {english},
  keywords = {*CITATION}
}

@inproceedings{voormann-gut:2008:agile-corpus-creationa,
  title = {Agile Corpus Creation},
  author = {Voormann, Holger and Gut, Ulrike},
  date = {2008},
  url = {https://api.semanticscholar.org/CorpusID:56885448},
  keywords = {*CITATION}
}

@article{wagstaff-cardie:2000:clustering-instancelevel-constraints,
  title = {Clustering with {{Instance-level Constraints}}},
  author = {Wagstaff, Kiri and Cardie, Claire},
  date = {2000},
  journaltitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
  pages = {1103--1110},
  abstract = {Clustering algorithms conduct a search through the space of possible organizations of a data set. In this paper, we propose two types of instance-level clustering constraints – must-link and cannot-link constraints – and show how they can be incorporated into a clustering algorithm to aid that search. For three of the four data sets tested, our results indicate that the incorporation of surprisingly few such constraints can increase clustering accuracy while decreasing runtime. We also investigate the relative effects of each type of constraint and find that the type that contributes most to accuracy improvements depends on the behavior of the clustering algorithm without constraints.},
  langid = {english},
  keywords = {*CITATION,Clustering,Constrained clustering,Contraintes,INTERACTIVE\_CLUSTERING,Must-link and Cannot-Link},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\8WNCENKV\Wagstaﬀ et Cardie - 2000 - Clustering with Instance-level Constraints.pdf}
}

@inproceedings{wagstaff-etal:2001:constrained-kmeans-clustering,
  title = {Constrained {{K-means}} Clustering with Background Knowledge},
  author = {Wagstaff, Kiri and Cardie, Claire and Rogers, Seth and Schrödl, Stefan},
  date = {2001-01},
  series = {Proceedings of 18th {{International Conference}} on {{Machine Learning}}},
  pages = {577--584},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\TGGJZZWD\Wagstaff et al. - 2001 - Constrained K-means clustering with background kno.pdf}
}

@article{wallach-goffinet:1987:mean-squared-error,
  title = {Mean Squared Error of Prediction in Models for Studying Ecological and Agronomic Systems},
  author = {Wallach, Daniel and Goffinet, Bruno},
  date = {1987},
  journaltitle = {Biometrics},
  pages = {561--573},
  keywords = {*CITATION,Mean squared error}
}

@article{weizenbaum:1966:eliza-computer-program,
  title = {{{ELIZA}} - a Computer Program for the Study of Natural Language Communication between Man and Machine},
  author = {Weizenbaum, Joseph},
  date = {1966-01},
  journaltitle = {Commun. ACM},
  volume = {9},
  number = {1},
  pages = {36--45},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/365153.365168},
  url = {https://dl.acm.org/doi/10.1145/365153.365168},
  urldate = {2023-10-09},
  langid = {english},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\KLKJI4YY\Weizenbaum - 1966 - ELIZA - a computer program for the study of natura.pdf}
}

@audio{woods:1971:poor-lonesome-cowboy,
  title = {I'm {{A Poor Lonesome Cowboy}}},
  editora = {Bolling, Claude and Woods, Pat},
  editoratype = {collaborator},
  date = {1971},
  url = {https://youtu.be/Ao6yD3bkyAU},
  abstract = {[Intro]   D          D7     G          G7   Lonesome cowboy   Lonesome cowboy D          Bm   Bm7  E        A   You're a long long way from home D                 G   Lonesome cowboy   Lonesome cowboy D          Bm   Bm7  E   A7 D   You've a long long way to roam   [Verse]       D I'm a poor lonesome cowboy, I'm a long long way from home                        F\#m And this poor lonesome cowboy           Em               A Has got a long long way to roam       D                  D7 Over mountains and over prairies      G                Em From dawn 'til day is done    Bm                F\#m       G      A       D My horse and me keep ridin', into the settin' sun   [Chorus]   D          D7     G          G7   Lonesome cowboy   Lonesome cowboy D          Bm   Bm7  E        A   You're a long long way from home D                 G   Lonesome cowboy   Lonesome cowboy D          Bm   Bm7  E   A7 D   You've a long long way to roam   [Verse]             D There are guys who just figure every problem with a gun                      F\#m And a finger on the trigger        Em               A Can be dangerous hurt someone     D                   D7 But problems solve much better    G By keeping calm and cool    Bm                F\#m My horse and me keep ridin'   G       A      D I ain't nobody's fool   [Chorus]   D          D7     G          G7   Lonesome cowboy   Lonesome cowboy D          Bm   Bm7  E        A   You're a long long way from home D                 G   Lonesome cowboy   Lonesome cowboy D          Bm   Bm7  E   A7 D   You've a long long way to roam   [Verse]         D I'm a poor lonesome cowboy, but it doesn't bother me                           F\#m 'Cause this poor lonesome cowboy           Em              A Prefers a horse for company     D               D7 Got nothing against women       G But I wave them all goodbye    Bm                F\#m My horse and me keep riding    G          A     D We don't like being tied   [Chorus]   D          D7     G          G7   Lonesome cowboy   Lonesome cowboy D          Bm   Bm7  E        A   You're a long long way from home D                 G   Lonesome cowboy   Lonesome cowboy D          Bm   Bm7  E   A7 D   You've a long long way to roam   [Outro]   D     G       D   To roam wou ou},
  keywords = {*CITATION}
}

@book{wynne:2004:developing-linguistic-corpora,
  title = {Developing Linguistic Corpora: A Guide to Good Practice},
  shorttitle = {Developing Linguistic Corpora},
  editor = {Wynne, Martin},
  date = {2004},
  series = {{{AHDS}} Literature, Languages and Linguistics},
  edition = {Oxbow Books},
  publisher = {{AHDS: Literature, Languages, and Linguistics}},
  location = {{Oxford}},
  url = {http://www.ahds.ac.uk/creating/guides/linguistic-corpora/index.htm},
  abstract = {A linguistic corpus is a collection of texts which have been selected and brought together so that language can be studied on the computer. Today, corpus linguistics offers some of the most powerful new procedures for the analysis of language, and the impact of this dynamic and expanding sub-discipline is making itself felt in many areas of language study. In this volume, a selection of leading experts in various key areas of corpus construction offer advice in a readable and largely non-technical style to help the reader to ensure that their corpus is well designed and fit for the intended purpose. This Guide is aimed at those who are at some stage of building a linguistic corpus. Little or no knowledge of corpus linguistics or computational procedures is assumed, although it is hoped that more advanced users will also find the guidelines here useful. It also has relevance for those who are not building a corpus, but who need to know something about the issues involved in the design of corpora in order to choose between available resources and to help draw conclusions from their analysis. Increasing numbers of researchers are seeing the potential benefits of the use of an electronic corpus as a source of empirical language data for their research. Until now, where did they find out about how to build a corpus? There is a great deal of useful information available which covers principles of corpus design and development, but it is dispersed in handbooks, reports, monographs, journal articles and sometimes only in the heads of experienced practitioners. This Guide is an attempt to draw together the experience of corpus builders into a single source, as a starting point for obtaining advice and guidance on good practice in this field. It aims to bring together some key elements of the experience learned, over many decades, by leading practitioners in the field and to make it available to those developing corpora today. The modest aim of this Guide is to take readers through the basic first steps involved in creating a corpus of language data in electronic form for the purpose of linguistic research. While some technical issues are covered, this Guide does not aim to offer the latest information on digitisation techniques. Rather, the emphasis is on the principles, and readers are invited to refer to other sources, such as the latest AHDS information papers, for the latest advice on technologies. In addition to the first chapter on the principles of corpus design, Professor Sinclair has also provided a more practical guide to building a corpus, which is added as an appendix to the Guide. This should help guide the user through some of the more specific decisions that are likely to be involved in building a corpus. Alert readers will see that there are areas where the authors are not in accord with each other. It is for the reader to weigh up the advantages of each approach for his own particular project, and to decide which course to follow. This Guide not aim to synthesize the advice offered by the various practitioners into a single approach to creating corpora. The information on good practice which is sampled here comes from a variety of sources, reflecting different research goals, intellectual traditions and theoretical orientations. The individual authors were asked to state their opinion on what they think is the best way to deal with the relevant aspects of developing a corpus, and neither the authors nor the editor have tried to hide the differences in approaches which inevitably exist. It is anticipated that readers of this document will have differing backgrounds, will have very diverse aims and objectives, will be dealing with a variety of different languages and varieties, and that one single approach would not fit them all. I would like to thank the authors of this volume for their goodwill and support to this venture, and for their patience through the long period it has taken to bring the Guide to publication. I would like to acknowledge the extremely helpful advice and editorial work from my colleague Ylva Berglund, which has improved many aspects of this guide.},
  isbn = {978-1-84217-205-6},
  langid = {english},
  pagetotal = {87},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\FZ6PT65D\Wynne - 2005 - Developing linguistic corpora a guide to good pra.pdf}
}

@article{xu-tian:2015:comprehensive-survey-clustering,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  author = {Xu, Dongkuan and Tian, Yingjie},
  date = {2015},
  journaltitle = {Annals of Data Science},
  volume = {2},
  pages = {165--193},
  abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table 22.},
  keywords = {*CITATION,Clustering,Comparaison}
}

@incollection{zdaniuk:2014:ordinary-leastsquares-ols,
  title = {Ordinary {{Least-Squares}} ({{OLS}}) {{Model}}},
  booktitle = {Encyclopedia of {{Quality}} of {{Life}} and {{Well-Being Research}}},
  author = {Zdaniuk, Bozena},
  editor = {Michalos, Alex C.},
  date = {2014},
  pages = {4515--4517},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  url = {http://link.springer.com/10.1007/978-94-007-0753-5_2008},
  urldate = {2023-09-15},
  abstract = {Ordinary least-squares (OLS) models assume that the analysis is fitting a model of a relationship between one or more explanatory variables and a continuous or at least interval outcome variable that minimizes the sum of square errors, where an error is the difference between the actual and the predicted value of the outcome variable. The most common analytical method that utilizes OLS models is linear regression (with a single or multiple predictor variables).},
  isbn = {978-94-007-0752-8 978-94-007-0753-5},
  langid = {english},
  keywords = {*CITATION}
}

@article{zhang-etal:2019:pegasus-pretraining-extracted,
  title = {{{PEGASUS}}: {{Pre-training}} with {{Extracted Gap-sentences}} for {{Abstractive Summarization}}},
  shorttitle = {{{PEGASUS}}},
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
  date = {2019},
  journaltitle = {ArXiv preprint},
  doi = {10.48550/ARXIV.1912.08777},
  url = {https://arxiv.org/abs/1912.08777},
  urldate = {2023-07-17},
  abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
  keywords = {*CITATION,Computation and Language,FOS: Computer and information sciences},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\UWWCFCEF\Zhang et al. - 2019 - PEGASUS Pre-training with Extracted Gap-sentences.pdf}
}

@book{zhou:2021:machine-learning,
  title = {Machine Learning},
  author = {Zhou, Zhi-Hua},
  translator = {Liu, Shaowu},
  date = {2021},
  publisher = {{Springer}},
  location = {{Singapore}},
  url = {https://books.google.fr/books?id=Zd5hywEACAAJ},
  abstract = {Machine Learning, a vital and core area of artificial intelligence (AI), is propelling the AI field ever further and making it one of the most compelling areas of computer science research. This textbook offers a comprehensive and unbiased introduction to almost all aspects of machine learning, from the fundamentals to advanced topics. It consists of 16 chapters divided into three parts: Part 1 (Chapters 1-3) introduces the fundamentals of machine learning, including terminology, basic principles, evaluation, and linear models; Part 2 (Chapters 4-10) presents classic and commonly used machine learning methods, such as decision trees, neural networks, support vector machines, Bayesian classifiers, ensemble methods, clustering, dimension reduction and metric learning; Part 3 (Chapters 11-16) introduces some advanced topics, covering feature selection and sparse learning, computational learning theory, semi-supervised learning, probabilistic graphical models, rule learning, and reinforcement learning. Each chapter includes exercises and further reading, so that readers can explore areas of interest. The book can be used as an undergraduate or postgraduate textbook for computer science, computer engineering, electrical engineering, data science, and related majors. It is also a useful reference resource for researchers and practitioners of machine learning.},
  isbn = {9789811519673 9789811519666},
  langid = {english},
  pagetotal = {458},
  keywords = {*CITATION}
}

@article{zhuang-etal:2021:comprehensive-survey-transfer,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  date = {2021-01},
  journaltitle = {Proc. IEEE},
  volume = {109},
  number = {1},
  pages = {43--76},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2020.3004555},
  url = {https://ieeexplore.ieee.org/document/9134370/},
  urldate = {2023-09-29},
  abstract = {Deep learning has been the answer to many machine learning problems during the past two decades. However, it comes with two significant constraints: dependency on extensive labeled data and training costs. Transfer learning in deep learning, known as Deep Transfer Learning (DTL), attempts to reduce such reliance and costs by reusing obtained knowledge from a source data/task in training on a target data/task. Most applied DTL techniques are network/model-based approaches. These methods reduce the dependency of deep learning models on extensive training data and drastically decrease training costs. Moreover, the training cost reduction makes DTL viable on edge devices with limited resources. Like any new advancement, DTL methods have their own limitations, and a successful transfer depends on specific adjustments and strategies for different scenarios. This paper reviews the concept, definition, and taxonomy of deep transfer learning and well-known methods. It investigates the DTL approaches by reviewing applied DTL techniques in the past five years and a couple of experimental analyses of DTLs to discover the best practice for using DTL in different scenarios. Moreover, the limitations of DTLs (catastrophic forgetting dilemma and overly biased pre-trained models) are discussed, along with possible solutions and research trends.},
  keywords = {*CITATION},
  file = {C:\Users\SCHILDEW\Documents\Zotero\storage\PRYF889G\Zhuang et al. - 2021 - A Comprehensive Survey on Transfer Learning.pdf}
}
