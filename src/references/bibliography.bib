@software{alammar-grefenstette:2022:cohere-sandbox,
  title = {Cohere {{Sandbox}}},
  author = {Alammar, Jay and Grefenstette, Edward},
  date = {2022},
  url = {https://github.com/cohere-ai/sandbox-topically},
  organization = {{Cohere.ai}},
  keywords = {*CITATION}
}

@article{alexa-internet:2018:keyword-research-competitor,
  title = {Keyword {{Research}}, {{Competitor Analysis}}, \& {{Website Ranking}} | {{Alexa}}},
  author = {Alexa Internet},
  date = {2018-01-27},
  url = {https://www.alexa.com},
  keywords = {*CITATION}
}

@book{anderson:2013:architecture-cognition,
  title = {The {{Architecture}} of {{Cognition}}},
  author = {Anderson, John R.},
  date = {2013-11-19},
  edition = {0},
  publisher = {{Psychology Press}},
  url = {https://www.taylorfrancis.com/books/9781317759539},
  urldate = {2023-06-07},
  isbn = {978-1-317-75953-9},
  langid = {english},
  keywords = {*CITATION,Ergonomie,Temps adaptation}
}

@thesis{baledent:2022:complexite-annotation-manuelle,
  title = {De la complexité de l'annotation manuelle : méthodologie, biais et recommandations},
  author = {Baledent, Anaelle},
  date = {2023-12-01},
  institution = {{Normandie Université}},
  url = {https://theses.hal.science/tel-04011353},
  abstract = {Les corpus de référence annotés constituent des éléments primordiaux de nombreuses tâches du Traitement Automatique des Langues. Leur construction fait l'objet d’une attention particulière, notamment lors de campagnes d’annotation manuelle. Ces dernières impliquent de multiples aspects, déjà étudiés dans la littérature mais souvent de manière séparée. Nous présentons une synthèse des problèmes rencontrés lors des différentes étapes d'une campagne, attirant l’attention des gestionnaires sur des points de vigilance, afin qu'ils fassent preuve de prudence durant leur campagne.Cette thèse donne une première définition des biais d’annotation, qui sont des phénomènes perturbateurs et variés pouvant avoir une incidence sur les annotations. Nous proposons une méthode et des moyens d'observation pour détecter et analyser la présence de biais d’annotation. Deux campagnes d’annotation, menées spécialement dans le but d'étudier des biais particuliers, servent d'illustration et nous ont permis de constater l'influence tangible de certains paramètres sur l’annotation. Dans cette optique, nous avons aussi introduit la notion de consensualité, qui permet en particulier de situer un annotateur par rapport à un groupe. Nous montrons un premier lien entre les annotateurs les moins consensuels et les moins performants.},
  langid = {french},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\PK64N4R8\\Baledent - 2022 - De la complexité de l'annotation manuelle  méthod.pdf}
}

@article{blei-etal:2003:latent-dirichlet-allocation,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M and Ng, Andrew Y. and Jordan, Michael I.},
  date = {2003},
  journaltitle = {Journal of machine Learning research},
  number = {3},
  pages = {993--1022},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  langid = {english},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\WW295LFP\\Blei - 2003 - Latent Dirichlet Allocation.pdf}
}

@unpublished{bocklisch-etal:2017:rasa-open-source,
  title = {Rasa: {{Open Source Language Understanding}} and {{Dialogue Management}}},
  shorttitle = {Rasa},
  author = {Bocklisch, Tom and Faulkner, Joey and Pawlowski, Nick and Nichol, Alan},
  date = {2017-12-15},
  eprint = {1712.05181},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1712.05181},
  urldate = {2020-10-23},
  abstract = {We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source python libraries for building conversational software. Their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers. In terms of design philosophy, we aim for ease of use, and bootstrapping from minimal (or no) initial training data. Both packages are extensively documented and ship with a comprehensive suite of tests. The code is available at https://github.com/RasaHQ/},
  langid = {english},
  keywords = {*CITATION,Artificial Intelligence,Computation and Language,Computer Science - Machine Learning,Rasa},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\4ME935TW\\Bocklisch et al. - 2017 - Rasa Open Source Language Understanding and Dialo.pdf}
}

@unpublished{bojanowski-etal:2016:enriching-word-vectors,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  date = {2016},
  eprint = {1607.04606},
  eprinttype = {arxiv},
  keywords = {*CITATION,⛔ No DOI found,Computation and Language,Machine learning},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\BZMD9R5Y\\Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf}
}

@article{brown-etal:2020:language-models-are,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  urldate = {2023-07-17},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  keywords = {*CITATION,Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\X8K8BLEW\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@article{brysbaert:2019:how-many-words,
  title = {How Many Words Do We Read per Minute? {{A}} Review and Meta-Analysis of Reading Rate},
  shorttitle = {How Many Words Do We Read per Minute?},
  author = {Brysbaert, Marc},
  date = {2019-12},
  journaltitle = {Journal of Memory and Language},
  volume = {109},
  pages = {104047},
  issn = {0749596X},
  doi = {10.1016/j.jml.2019.104047},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X19300786},
  urldate = {2023-06-27},
  langid = {english},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\6HQS5M2Y\\Brysbaert - 2019 - How many words do we read per minute A review and.pdf}
}

@article{costello:2019:gartner-top-technologies,
  title = {Gartner {{Top Technologies}} and {{Trends Driving}} the {{Digital Workplace}}},
  author = {Costello, Katie},
  date = {2019-03-18},
  journaltitle = {Gartner, Inc},
  url = {//www.gartner.com/smarterwithgartner/top-10-technologies-driving-the-digital-workplace/},
  urldate = {2020-10-23},
  abstract = {How artificial intelligence, smart workspaces and talent markets will boost employee digital dexterity in future digital workplaces.},
  langid = {american},
  keywords = {*CITATION}
}

@inproceedings{dagan-etal:2005:pascal-recognising-textual,
  title = {The {{PASCAL Recognising Textual Entailment Challenge}}},
  booktitle = {Machine {{Learning Challenges}}. {{Evaluating Predictive Uncertainty}}, {{Visual Object Classification}}, and {{Recognising Tectual Entailment}}},
  author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  date = {2005-01},
  volume = {3944},
  pages = {177--190},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11736790_9},
  url = {http://link.springer.com/10.1007/11736790_9},
  abstract = {This paper describes the Second PASCAL Recognising Textual Entailment Challenge (RTE-2).1 We describe the RTE2 dataset and overview the submissions for the challenge. One of the main goals for this year’s dataset was to provide more “realistic” text-hypothesis examples, based mostly on outputs of actual systems. The 23 submissions for the challenge present diverse approaches and research directions, and the best results achieved this year are considerably higher than last year’s state of the art.},
  isbn = {978-3-540-33427-9},
  langid = {english},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\NXZHY47L\\Dagan et al. - 2006 - The PASCAL Recognising Textual Entailment Challeng.pdf}
}

@article{davidson-ravi:2005:agglomerative-hierarchical-clustering,
  title = {Agglomerative {{Hierarchical Clustering}} with {{Constraints}}: {{Theoretical}} and {{Empirical Results}}},
  shorttitle = {Agglomerative {{Hierarchical Clustering}} with {{Constraints}}},
  author = {Davidson, Ian and Ravi, S. S.},
  editor = {Jorge, Alípio Mário and Torgo, Luís and Brazdil, Pavel and Camacho, Rui and Gama, João},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {collaborator},
  date = {2005},
  journaltitle = {Knowledge Discovery in Databases: PKDD 2005},
  volume = {3721},
  pages = {59--70},
  url = {http://link.springer.com/10.1007/11564126_11},
  urldate = {2020-10-22},
  abstract = {We explore the use of instance and cluster-level constraints with agglomerative hierarchical clustering. Though previous work has illustrated the benefits of using constraints for non-hierarchical clustering, their application to hierarchical clustering is not straight-forward for two primary reasons. First, some constraint combinations make the feasibility problem (Does there exist a single feasible solution?) NP-complete. Second, some constraint combinations when used with traditional agglomerative algorithms can cause the dendrogram to stop prematurely in a dead-end solution even though there exist other feasible solutions with a significantly smaller number of clusters. When constraints lead to efficiently solvable feasibility problems and standard agglomerative algorithms do not give rise to dead-end solutions, we empirically illustrate the benefits of using constraints to improve cluster purity and average distortion. Furthermore, we introduce the new γ constraint and use it in conjunction with the triangle inequality to considerably improve the efficiency of agglomerative clustering.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Constrained clustering,Hierarchcal clustering},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\QEHQH3QT\\Davidson et Ravi - 2005 - Agglomerative Hierarchical Clustering with Constra.pdf}
}

@article{diamond-etal:1990:analysis-binary-data,
  title = {Analysis of {{Binary Data}}. 2nd {{Edn}}.},
  author = {Diamond, Ian and Cox, D. R. and Snell, E. J.},
  date = {1990},
  journaltitle = {Applied Statistics},
  volume = {39},
  number = {2},
  eprint = {10.2307/2347766},
  eprinttype = {jstor},
  pages = {260},
  issn = {00359254},
  doi = {10.2307/2347766},
  url = {https://www.jstor.org/stable/10.2307/2347766?origin=crossref},
  urldate = {2023-07-06},
  keywords = {*CITATION,R²}
}

@book{edwards:1992:likelihood,
  title = {Likelihood},
  author = {Edwards, Anthony William Fairbank},
  date = {1992},
  edition = {Expanded ed},
  publisher = {{Johns Hopkins Univ. Press}},
  location = {{Baltimore}},
  isbn = {978-0-8018-4445-4 978-0-8018-4443-0},
  langid = {english},
  pagetotal = {275},
  keywords = {*CITATION,Log vraissemblance,Metric}
}

@article{elkosantini-gien:2009:integration-human-behavioural,
  title = {Integration of Human Behavioural Aspects in a Dynamic Model for a Manufacturing System},
  author = {Elkosantini, S. and Gien, D.},
  date = {2009-05-15},
  journaltitle = {International Journal of Production Research},
  volume = {47},
  number = {10},
  pages = {2601--2623},
  issn = {0020-7543, 1366-588X},
  doi = {10.1080/00207540701663490},
  url = {https://www.tandfonline.com/doi/full/10.1080/00207540701663490},
  urldate = {2023-06-07},
  langid = {english},
  keywords = {*CITATION,Ergonomie,Fatigue des opérateurs},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\BVES3475\\Elkosantini et Gien - 2009 - Integration of human behavioural aspects in a dyna.pdf}
}

@article{gancarski-wemmert:2007:collaborative-multistep-monolevel,
  title = {Collaborative Multi-Step Mono-Level Multi-Strategy Classification},
  author = {Gançarski, Pierre and Wemmert, Cédric},
  date = {2007-08-30},
  journaltitle = {Multimed Tools Appl},
  volume = {35},
  number = {1},
  pages = {1--27},
  issn = {1380-7501},
  doi = {10.1007/s11042-007-0115-x},
  url = {https://univoak.eu/islandora/object/islandora:96302},
  abstract = {This article deals with the description of a new way to learn from multiple and heterogeneous data sets, and with the integration of this method in a multi-agent hybrid learning system. This system integrates different kinds of unsupervised classification methods and gives a set of clusterings as the result and a unifying result, representing all the other one. In this new approach, the method occurrences compare their results and automatically refine them to try to make them converge towards a unique clustering that unifies all the results. Thus, the data are not really merged but the results from their classification are compared and refined according to the results from all the other data sets. This enables to produce a set of classification hierarchies which classes are very similar, although these hierarchies were extracted from different data sets. Then it is easy to build a unifying result from all of them.},
  langid = {english},
  keywords = {*CITATION,Clustering,Collaborative clustering,Constrained clustering,INTERACTIVE\_CLUSTERING},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\CFXDP786\\Gançarski and Wemmert - 2007 - Collaborative multi-step mono-level multi-strategy.pdf}
}

@book{girden:1992:anova,
  title = {{{ANOVA}}},
  author = {Girden, Ellen},
  date = {1992},
  publisher = {{SAGE Publications, Inc.}},
  location = {{2455 Teller Road,~Thousand Oaks~California~91320~United States of America}},
  url = {https://methods.sagepub.com/book/anova},
  urldate = {2023-07-06},
  isbn = {978-0-8039-4257-8 978-1-4129-8341-9},
  keywords = {*CITATION}
}

@article{givoni-frey:2009:semisupervised-affinity-propagation,
  title = {Semi-{{Supervised Aﬃnity Propagation}} with {{Instance-Level Constraints}}},
  author = {Givoni, Inmar E and Frey, Brendan J},
  date = {2009},
  abstract = {Recently, affinity propagation (AP) was introduced as an unsupervised learning algorithm for exemplar based clustering. Here we extend the AP model to account for semisupervised clustering. AP, which is formulated as inference in a factor-graph, can be naturally extended to account for ‘instancelevel’ constraints: pairs of data points that cannot belong to the same cluster (cannotlink), or must belong to the same cluster (must-link). We present a semi-supervised AP algorithm (SSAP) that can use instancelevel constraints to guide the clustering. We demonstrate the applicability of SSAP to interactive image segmentation by using SSAP to cluster superpixels while taking into account user instructions regarding which superpixels belong to the same object. We demonstrate SSAP can achieve better performance compared to other semi-supervised methods.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Affinity propagation,Clustering,Constrained clustering},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\SMP6WJN6\\Givoni et Frey - 2009 - Semi-Supervised Aﬃnity Propagation with Instance-L.pdf}
}

@article{goasduff:2019:chatbots-will-appeal,
  title = {Chatbots {{Will Appeal}} to {{Modern Workers}}},
  author = {Goasduff, Laurence},
  date = {2019-07-31},
  journaltitle = {Gartner, Inc},
  url = {//www.gartner.com/smarterwithgartner/chatbots-will-appeal-to-modern-workers/},
  urldate = {2020-10-23},
  abstract = {The proliferation of chatbots in the modern workplace calls for IT leaders to create a conversational platform strategy that ensures an effective solution for employees, customers and key partners.},
  langid = {american},
  keywords = {*CITATION}
}

@incollection{hart-staveland:1988:development-nasatlx-task,
  title = {Development of {{NASA-TLX}} ({{Task Load Index}}): {{Results}} of {{Empirical}} and {{Theoretical Research}}},
  booktitle = {Human {{Mental Workload}}},
  author = {Hart, Sandra G. and Staveland, Lowell E.},
  editor = {Hancock, Peter A. and Meshkati, Najmedin},
  date = {1988},
  series = {Advances in {{Psychology}}},
  volume = {52},
  pages = {139--183},
  publisher = {{North-Holland}},
  url = {https://www.sciencedirect.com/science/article/pii/S0166411508623869},
  abstract = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload.},
  keywords = {*CITATION,Charge mentale}
}

@article{honnibal-montani:2017:spacy-natural-language,
  title = {{{spaCy}} 2 : {{Natural}} Language Understanding with {{Bloom}} Embeddings, Convolutional Neural Networks and Incremental Parsing},
  author = {Honnibal, Matthew and Montani, Ines},
  date = {2017},
  keywords = {*CITATION,Spacy}
}

@article{hoyt-etal:2016:ibm-watson-analytics,
  title = {{{IBM Watson Analytics}}: {{Automating Visualization}}, {{Descriptive}}, and {{Predictive Statistics}}},
  author = {Hoyt, Robert Eugene and Snider, Dallas and Thompson, Carla and Mantravadi, Sarita},
  date = {2016-10-11},
  journaltitle = {JMIR Public Health Surveill},
  volume = {2},
  number = {2},
  issn = {2369-2960},
  doi = {10.2196/publichealth.5810},
  url = {https://doi.org/10.2196/publichealth.5810},
  abstract = {Background: We live in an era of explosive data generation that will continue to grow and involve all industries. One of the results of this explosion is the need for newer and more efficient data analytics procedures. Traditionally, data analytics required a substantial background in statistics and computer science. In 2015, International Business Machines Corporation (IBM) released the IBM Watson Analytics (IBMWA) software that delivered advanced statistical procedures based on the Statistical Package for the Social Sciences (SPSS). The latest entry of Watson Analytics into the field of analytical software products provides users with enhanced functions that are not available in many existing programs. For example, Watson Analytics automatically analyzes datasets, examines data quality, and determines the optimal statistical approach. Users can request exploratory, predictive, and visual analytics. Using natural language processing (NLP), users are able to submit additional questions for analyses in a quick response format. This analytical package is available free to academic institutions (faculty and students) that plan to use the tools for noncommercial purposes. Objective: To report the features of IBMWA and discuss how this software subjectively and objectively compares to other data mining programs. Methods: The salient features of the IBMWA program were examined and compared with other common analytical platforms, using validated health datasets. Results: Using a validated dataset, IBMWA delivered similar predictions compared with several commercial and open source data mining software applications. The visual analytics generated by IBMWA were similar to results from programs such as Microsoft Excel and Tableau Software. In addition, assistance with data preprocessing and data exploration was an inherent component of the IBMWA application. Sensitivity and specificity were not included in the IBMWA predictive analytics results, nor were odds ratios, confidence intervals, or a confusion matrix. Conclusions: IBMWA is a new alternative for data analytics software that automates descriptive, predictive, and visual analytics. This program is very user-friendly but requires data preprocessing, statistical conceptual understanding, and domain expertise.},
  keywords = {*CITATION,data analysis,data mining,IBM Watson,machine learning,NLP,statistical data analysis}
}

@article{huang-etal:2022:are-large-pretrained,
  title = {Are {{Large Pre-Trained Language Models Leaking Your Personal Information}}?},
  author = {Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  date = {2022},
  doi = {10.48550/ARXIV.2205.12628},
  url = {https://arxiv.org/abs/2205.12628},
  urldate = {2023-07-20},
  abstract = {Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner's name. We find that PLMs do leak personal information due to memorization. However, since the models are weak at association, the risk of specific personal information being extracted by attackers is low. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe.},
  keywords = {*CITATION,Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Cryptography and Security (cs.CR),FOS: Computer and information sciences},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\YTDTZXU8\\Huang et al. - 2022 - Are Large Pre-Trained Language Models Leaking Your.pdf}
}

@article{jones-etal:2015:demographic-occupational-predictors,
  title = {Demographic and Occupational Predictors of Stress and Fatigue in {{French}} Intensive-Care Registered Nurses and Nurses' Aides: {{A}} Cross-Sectional Study},
  shorttitle = {Demographic and Occupational Predictors of Stress and Fatigue in {{French}} Intensive-Care Registered Nurses and Nurses' Aides},
  author = {Jones, Gabrielle and Hocine, Mounia and Salomon, Jérôme and Dab, William and Temime, Laura},
  date = {2015-01},
  journaltitle = {International Journal of Nursing Studies},
  volume = {52},
  number = {1},
  pages = {250--259},
  issn = {00207489},
  doi = {10.1016/j.ijnurstu.2014.07.015},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0020748914002016},
  urldate = {2023-06-07},
  langid = {english},
  keywords = {*CITATION,Ergonomie,Fatigue des opérateurs}
}

@book{kahneman:2011:thinking-fast-slow,
  title = {Thinking, Fast and Slow.},
  author = {Kahneman, Daniel},
  date = {2011},
  series = {Thinking, Fast and Slow.},
  publisher = {{Farrar, Straus and Giroux}},
  location = {{New York,  NY,  US}},
  abstract = {In the highly anticipated Thinking, Fast and Slow, Kahneman takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. Kahneman exposes the extraordinary capabilities—and also the faults and biases—of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behavior. The impact of loss aversion and overconfidence on corporate strategies, the difficulties of predicting what will make us happy in the future, the challenges of properly framing risks at work and at home, the profound effect of cognitive biases on everything from playing the stock market to planning the next vacation—each of these can be understood only by knowing how the two systems shape our judgments and decisions. Engaging the reader in a lively conversation about how we think, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives—and how we can use different techniques to guard against the mental glitches that often get us into trouble. Thinking, Fast and Slow will transform the way you think about thinking. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {0-374-27563-7 (Hardcover); 1-4299-6935-0 (PDF); 978-0-374-27563-1 (Hardcover); 978-1-4299-6935-2 (PDF)},
  pagetotal = {499},
  keywords = {*CITATION,*Cognitive Processes,*Mind,*Thinking,Choice Behavior,Decision Making,Intuition,Judgment}
}

@article{kamvar-etal:2003:spectral-learning,
  title = {Spectral {{Learning}}},
  author = {Kamvar, Sepandar D and Klein, Dan and Manning, Christopher D},
  date = {2003},
  journaltitle = {Proceedings of the international joint conference on artificial intelligence},
  pages = {561--566},
  abstract = {We present a simple, easily implemented spectral learning algorithm that applies equally whether we have no supervisory information, pairwise link constraints, or labeled examples. In the unsupervised case, it performs consistently with other spectral clustering algorithms. In the supervised case, our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the 20 Newsgroups data set. Furthermore, its classification accuracy increases with the addition of unlabeled documents, demonstrating effective use of unlabeled data.},
  langid = {english},
  keywords = {*CITATION,⛔ No DOI found,Clustering,Constrained clustering,Spectral clustering},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\GX9HEA6W\\Kamvar et al. - 2003 - Spectral Learning.pdf}
}

@article{khan-etal:2012:multiple-parameter-based,
  title = {Multiple {{Parameter Based Clustering}} ({{MPC}}): {{Prospective Analysis}} for {{Effective Clustering}} in {{Wireless Sensor Network}} ({{WSN}}) {{Using K-Means Algorithm}}},
  shorttitle = {Multiple {{Parameter Based Clustering}} ({{MPC}})},
  author = {Khan, Md. Asif and Tamim, Israfil and Ahmed, Emdad and Awal, M. Abdul},
  date = {2012},
  journaltitle = {WSN},
  volume = {04},
  number = {01},
  pages = {18--24},
  issn = {1945-3078, 1945-3086},
  doi = {10.4236/wsn.2012.41003},
  url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/wsn.2012.41003},
  urldate = {2023-01-16},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Constrained clustering,K-Means},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\LQ5DZJVB\\Khan et al. - 2012 - Multiple Parameter Based Clustering (MPC) Prospec.pdf}
}

@incollection{kirch:2008:pearson-correlation-coefficient,
  title = {Pearson’s {{Correlation Coefficient}}},
  booktitle = {Encyclopedia of {{Public Health}}},
  editor = {Kirch, Wilhelm},
  date = {2008},
  pages = {1090--1091},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  url = {https://link.springer.com/10.1007/978-1-4020-5614-7_2569},
  urldate = {2023-07-06},
  isbn = {978-1-4020-5613-0 978-1-4020-5614-7},
  langid = {english},
  keywords = {*CITATION,Correlation,Metric}
}

@inproceedings{lamirel-etal:2016:new-efficient-clustering,
  title = {New Efficient Clustering Quality Indexes},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Lamirel, Jean-Charles and Dugue, Nicolas and Cuxac, Pascal},
  date = {2016-07},
  pages = {3649--3657},
  publisher = {{IEEE}},
  location = {{Vancouver, BC, Canada}},
  doi = {10/gfs3xx},
  url = {http://ieeexplore.ieee.org/document/7727669/},
  urldate = {2018-11-23},
  abstract = {This paper deals with a major challenge in clustering that is optimal model selection. It presents new efficient clustering quality indexes relying on feature maximization, which is an alternative measure to usual distributional measures relying on entropy, Chi-square metric or vector-based measures such as Euclidean distance or correlation distance. First Experiments compare the behavior of these new indexes with usual cluster quality indexes based on Euclidean distance on different kinds of test datasets for which ground truth is available. This comparison clearly highlights altogether the superior accuracy and stability of the new method on these datasets, its efficiency from low to high dimensional range and its tolerance to noise. Further experiments are then conducted on ”real life” textual data extracted from a multisource bibliographic database for which ground truth is unknown. These experiments show that the accuracy and stability of these new indexes allow to deal efficiently with diachronic analysis, when other indexes do not fit the requirements for this task.},
  eventtitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  isbn = {978-1-5090-0620-5},
  langid = {english},
  keywords = {*CITATION,FMC},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\I9SXE6GM\\Lamirel et al. - 2016 - New efficient clustering quality indexes.pdf}
}

@incollection{lamirel-etal:2017:novel-approach-feature,
  title = {A {{Novel Approach}} to {{Feature Selection Based}} on {{Quality Estimation Metrics}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Management}}},
  author = {Lamirel, Jean-Charles and Cuxac, Pascal and Hajlaoui, Kafil},
  editor = {Guillet, Fabrice and Pinaud, Bruno and Venturini, Gilles},
  date = {2017},
  series = {Studies in {{Computational Intelligence}}},
  volume = {665},
  pages = {121--140},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  url = {http://link.springer.com/10.1007/978-3-319-45763-5_7},
  urldate = {2018-11-23},
  abstract = {Feature maximization (F-max) is an unbiased quality estimation metric of unsupervised classification (clustering) that favours clusters with a maximal feature F-measure value. In this article we show that an adaptation of this metric within the framework of supervised classification allows efficient feature selection and feature contrasting to be performed. We experiment the method on different types of textual data. In this context, we demonstrate that this technique significantly improves the performance of classification methods as compared with the use of state-of-the art feature selection techniques, notably in the case of the classification of unbalanced, highly multidimensional and noisy textual data gathered in similar classes.},
  isbn = {978-3-319-45762-8 978-3-319-45763-5},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Evaluation metrics,Features selection,Filters,FMC,Multidimensional data},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\QHURTRCV\\Lamirel et al. - 2017 - A Novel Approach to Feature Selection Based on Qua.pdf}
}

@article{lampert-etal:2018:constrained-distance-based,
  title = {Constrained Distance Based Clustering for Time-Series: A Comparative and Experimental Study},
  shorttitle = {Constrained Distance Based Clustering for Time-Series},
  author = {Lampert, Thomas and Dao, Thi-Bich-Hanh and Lafabregue, Baptiste and Serrette, Nicolas and Forestier, Germain and Crémilleux, Bruno and Vrain, Christel and Gançarski, Pierre},
  date = {2018-11},
  journaltitle = {Data Min Knowl Disc},
  volume = {32},
  number = {6},
  pages = {1663--1707},
  issn = {1384-5810, 1573-756X},
  doi = {10/gfbpj8},
  url = {http://link.springer.com/10.1007/s10618-018-0573-y},
  urldate = {2020-06-02},
  abstract = {Constrained clustering is becoming an increasingly popular approach in data mining. It offers a balance between the complexity of producing a formal definition of thematic classes—required by supervised methods—and unsupervised approaches, which ignore expert knowledge and intuition. Nevertheless, the application of constrained clustering to time-series analysis is relatively unknown. This is partly due to the unsuitability of the Euclidean distance metric, which is typically used in data mining, to time-series data. This article addresses this divide by presenting an exhaustive review of constrained clustering algorithms and by modifying publicly available implementations to use a more appropriate distance measure—dynamic time warping. It presents a comparative study, in which their performance is evaluated when applied to time-series. It is found that k-means based algorithms become computationally expensive and unstable under these modifications. Spectral approaches are easily applied and offer state-of-the-art performance, whereas declarative approaches are also easily applied and guarantee constraint satisfaction. An analysis of the results raises several influencing factors to an algorithm’s performance when constraints are introduced.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Comparaison,Constrained clustering,Dynamic time warping,INTERACTIVE\_CLUSTERING,Partition clustering,Semi-supervised,Time-series},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\UMVKY7ZH\\Lampert et al. - 2018 - Constrained distance based clustering for time-ser.pdf}
}

@inproceedings{lampert-etal:2019:constrained-distance-based,
  title = {Constrained {{Distance}} Based {{K-Means Clustering}} for {{Satellite Image Time-Series}}},
  booktitle = {{{IGARSS}} 2019 - 2019 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Lampert, Thomas and Lafabregue, Baptiste and Gancarski, Pierre},
  date = {2019-07},
  pages = {2419--2422},
  publisher = {{IEEE}},
  location = {{Yokohama, Japan}},
  doi = {10/ggx3tj},
  url = {https://ieeexplore.ieee.org/document/8900147/},
  urldate = {2020-06-02},
  abstract = {The advent of high-resolution instruments for time-series sampling poses added complexity for the formal definition of thematic classes in the remote sensing domain—required by supervised methods—while unsupervised methods ignore expert knowledge and intuition. Constrained clustering is becoming an increasingly popular approach in data mining because it offers a solution to these problems, however, its application in remote sensing is relatively unknown. This article addresses this divide by adapting publicly available k-Means constrained clustering implementations to use the dynamic time warping (DTW) dissimilarity measure, which is thought to be more appropriate for time-series analysis. Adding constraints to the clustering problem increases accuracy when compared to unconstrained clustering. The output of such algorithms are homogeneous in spatially defined regions.},
  eventtitle = {{{IGARSS}} 2019 - 2019 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  isbn = {978-1-5386-9154-0},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Constrained clustering,INTERACTIVE\_CLUSTERING,K-Means},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\KWERLC9W\\Lampert et al. - 2019 - PRÉSENTATION.pdf;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\UNDG3ZHC\\Lampert et al. - 2019 - Constrained Distance based K-Means Clustering for .pdf}
}

@online{lee-sengupta:2022:introducing-ai-research,
  type = {Blog},
  title = {Introducing the Ai Research Supercluster - Meta’s Cutting-Edge Ai Supercomputer for Ai Research},
  author = {Lee, Kevin and Sengupta, Shubho},
  date = {2022},
  url = {https://ai.meta.com/blog/ai-rsc/},
  abstract = {Developing the next generation of advanced AI will require powerful new computers capable of quintillions of operations per second. Today, Meta is announcing that we’ve designed and built the AI Research SuperCluster (RSC) — which we believe is among the fastest AI supercomputers running today and will be the fastest AI supercomputer in the world when it’s fully built out in mid-2022. Our researchers have already started using RSC to train large models in natural language processing (NLP) and computer vision for research, with the aim of one day training models with trillions of parameters. RSC will help Meta’s AI researchers build new and better AI models that can learn from trillions of examples; work across hundreds of different languages; seamlessly analyze text, images, and video together; develop new augmented reality tools; and much more. Our researchers will be able to train the largest models needed to develop advanced AI for computer vision, NLP, speech recognition, and more. We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together. Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.},
  organization = {{Meta AI}},
  keywords = {*CITATION}
}

@article{lewis-etal:2019:bart-denoising-sequencetosequence,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2019},
  doi = {10.48550/ARXIV.1910.13461},
  url = {https://arxiv.org/abs/1910.13461},
  urldate = {2023-07-17},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  keywords = {*CITATION,Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\HMB79SIK\\Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf}
}

@inproceedings{li-etal:2009:constrained-clustering-spectral,
  title = {Constrained Clustering via Spectral Regularization},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Zhenguo and Liu, Jianzhuang and Tang, Xiaoou},
  date = {2009-06},
  pages = {421--428},
  publisher = {{IEEE}},
  location = {{Miami, FL}},
  doi = {10/dsh439},
  url = {https://ieeexplore.ieee.org/document/5206852/},
  urldate = {2020-07-29},
  abstract = {We propose a novel framework for constrained spectral clustering with pairwise constraints which specify whether two objects belong to the same cluster or not. Unlike previous methods that modify the similarity matrix with pairwise constraints, we adapt the spectral embedding towards an ideal embedding as consistent with the pairwise constraints as possible. Our formulation leads to a small semidefinite program whose complexity is independent of the number of objects in the data set and the number of pairwise constraints, making it scalable to large-scale problems. The proposed approach is applicable directly to multi-class problems, handles both must-link and cannotlink constraints, and can effectively propagate pairwise constraints. Extensive experiments on real image data and UCI data have demonstrated the efficacy of our algorithm.},
  eventtitle = {2009 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPR Workshops}})},
  isbn = {978-1-4244-3992-8},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Constrained clustering,INTERACTIVE\_CLUSTERING,Spectral clustering},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\DUKRVM3F\\Li et al. - 2009 - Constrained clustering via spectral regularization.pdf}
}

@article{macqueen:1967:methods-classification-analysis,
  title = {Some Methods for Classification and Analysis of Multivariate Observations.},
  author = {MacQueen, J},
  date = {1967},
  journaltitle = {Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
  volume = {1},
  number = {14},
  pages = {281--297},
  langid = {english},
  keywords = {*CITATION,⛔ No DOI found,Clustering,K-Means},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\NUMFXV2C\\MacQueen - 1967 - Some methods for classification and analysis of mu.pdf}
}

@book{manning-schutze:2000:foundations-statistical-natural,
  title = {Foundations of Statistical Natural Language Processing},
  author = {Manning, Christopher D. and Schütze, Hinrich},
  date = {2000},
  edition = {2e éd. avec des corrections},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  langid = {english},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\WINIYV9I\\Manning et Schütze - 2000 - Foundations of statistical natural language proces.pdf}
}

@article{miller-charles:1991:contextual-correlates-semantic,
  title = {Contextual Correlates of Semantic Similarity},
  author = {Miller, George A. and Charles, Walter G.},
  date = {1991},
  journaltitle = {Language and Cognitive Processes},
  volume = {6},
  number = {1},
  pages = {1--28},
  doi = {10.1080/01690969108406936},
  url = {https://doi.org/10.1080/01690969108406936},
  keywords = {*CITATION}
}

@article{murtagh-contreras:2012:algorithms-hierarchical-clustering,
  title = {Algorithms for Hierarchical Clustering: {{An}} Overview},
  author = {Murtagh, Fionn and Contreras, Pedro},
  date = {2012},
  journaltitle = {Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery},
  volume = {2},
  pages = {86--97},
  doi = {10.1002/widm.53},
  abstract = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. © 2011 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Hierarchcal clustering}
}

@article{nelder-wedderburn:1972:generalized-linear-models,
  title = {Generalized {{Linear Models}}},
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {135},
  number = {3},
  eprint = {10.2307/2344614},
  eprinttype = {jstor},
  pages = {370},
  issn = {00359238},
  doi = {10.2307/2344614},
  url = {https://www.jstor.org/stable/10.2307/2344614?origin=crossref},
  urldate = {2023-07-06},
  keywords = {*CITATION,GLM}
}

@incollection{ng-etal:2002:spectral-clustering-analysis,
  title = {On {{Spectral Clustering}}: {{Analysis}} and an Algorithm},
  shorttitle = {On {{Spectral Clustering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 14},
  author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
  editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
  date = {2002},
  pages = {849--856},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf},
  urldate = {2020-10-22},
  abstract = {Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
  keywords = {*CITATION,Clustering,Spectral clustering},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\DTJJCJL4\\Ng et al. - 2002 - On Spectral Clustering Analysis and an algorithm.pdf}
}

@online{ni-etal:2022:recent-advances-deep,
  title = {Recent {{Advances}} in {{Deep Learning Based Dialogue Systems}}: {{A Systematic Survey}}},
  shorttitle = {Recent {{Advances}} in {{Deep Learning Based Dialogue Systems}}},
  author = {Ni, Jinjie and Young, Tom and Pandelea, Vlad and Xue, Fuzhao and Cambria, Erik},
  date = {2022-03-29},
  eprint = {2105.04387},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2105.04387},
  urldate = {2023-07-26},
  abstract = {Dialogue systems are a popular natural language processing (NLP) task as it is promising in real-life applications. It is also a complicated task since many NLP tasks deserving study are involved. As a result, a multitude of novel works on this task are carried out, and most of them are deep learning based due to the outstanding performance. In this survey, we mainly focus on the deep learning based dialogue systems. We comprehensively review state-of-the-art research outcomes in dialogue systems and analyze them from two angles: model type and system type. Specifically, from the angle of model type, we discuss the principles, characteristics, and applications of different models that are widely used in dialogue systems. This will help researchers acquaint these models and see how they are applied in state-of-the-art frameworks, which is rather helpful when designing a new dialogue system. From the angle of system type, we discuss task-oriented and open-domain dialogue systems as two streams of research, providing insight into the hot topics related. Furthermore, we comprehensively review the evaluation methods and datasets for dialogue systems to pave the way for future research. Finally, some possible research trends are identified based on the recent research outcomes. To the best of our knowledge, this survey is the most comprehensive and up-to-date one at present for deep learning based dialogue systems, extensively covering the popular techniques1. We speculate that this work is a good starting point for academics who are new to the dialogue systems or those who want to quickly grasp up-to-date techniques in this area.},
  langid = {english},
  pubstate = {preprint},
  keywords = {*ALIRE/AIMPLÉMENTER,*CITATION,Chatbot,Dialogue},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\VPJSKSAR\\Ni et al. - 2022 - Recent Advances in Deep Learning Based Dialogue Sy.pdf}
}

@book{nivre:2006:inductive-dependency-parsing,
  title = {Inductive {{Dependency Parsing}}},
  author = {Nivre, Joakim},
  editorb = {Ide, Nancy and Véronis, Jean},
  editorbtype = {redactor},
  date = {2006},
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {34},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  url = {http://link.springer.com/10.1007/1-4020-4889-0},
  urldate = {2023-07-06},
  isbn = {978-1-4020-4888-3 978-1-4020-4889-0},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\YZEE8JPF\\Nivre - 2006 - Inductive Dependency Parsing.pdf}
}

@inproceedings{nothman-etal:2018:stop-word-lists,
  title = {Stop {{Word Lists}} in {{Free Open-source Software Packages}}},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  author = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
  date = {2018},
  pages = {7--12},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/W18-2502},
  url = {http://aclweb.org/anthology/W18-2502},
  urldate = {2023-07-06},
  eventtitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Preprocessing,Stopwords},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\VGBMUBTE\\Nothman et al. - 2018 - Stop Word Lists in Free Open-source Software Packa.pdf}
}

@article{oneill-connor:2023:amplifying-limitations-harms,
  title = {Amplifying {{Limitations}}, {{Harms}} and {{Risks}} of {{Large Language Models}}},
  author = {O'Neill, Michael and Connor, Mark},
  date = {2023},
  doi = {10.48550/ARXIV.2307.04821},
  url = {https://arxiv.org/abs/2307.04821},
  urldate = {2023-07-20},
  abstract = {We present this article as a small gesture in an attempt to counter what appears to be exponentially growing hype around Artificial Intelligence (AI) and its capabilities, and the distraction provided by the associated talk of science-fiction scenarios that might arise if AI should become sentient and super-intelligent. It may also help those outside of the field to become more informed about some of the limitations of AI technology. In the current context of popular discourse AI defaults to mean foundation and large language models (LLMs) such as those used to create ChatGPT. This in itself is a misrepresentation of the diversity, depth and volume of research, researchers, and technology that truly represents the field of AI. AI being a field of research that has existed in software artefacts since at least the 1950's. We set out to highlight a number of limitations of LLMs, and in so doing highlight that harms have already arisen and will continue to arise due to these limitations. Along the way we also highlight some of the associated risks for individuals and organisations in using this technology.},
  keywords = {*CITATION,Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Computers and Society (cs.CY),FOS: Computer and information sciences},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\T3MMECGA\\O'Neill et Connor - 2023 - Amplifying Limitations, Harms and Risks of Large L.pdf}
}

@software{openai:2023:chatgpt,
  title = {{{ChatGPT}}},
  author = {OpenAI},
  date = {2023},
  url = {https://chat.openai.com},
  keywords = {*CITATION}
}

@article{pedregosa-etal:2011:scikitlearn-machine-learning,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  keywords = {*CITATION,Sklearn}
}

@inproceedings{pradhan-etal:2007:semeval2007-task-17,
  title = {{{SemEval-2007}} Task 17: {{English}} Lexical Sample, {{SRL}} and All Words},
  shorttitle = {{{SemEval-2007}} Task 17},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Semantic Evaluations}} - {{SemEval}} '07},
  author = {Pradhan, Sameer S. and Loper, Edward and Dligach, Dmitriy and Palmer, Martha},
  date = {2007},
  pages = {87--92},
  publisher = {{Association for Computational Linguistics}},
  location = {{Prague, Czech Republic}},
  doi = {10.3115/1621474.1621490},
  url = {http://portal.acm.org/citation.cfm?doid=1621474.1621490},
  urldate = {2023-06-27},
  abstract = {This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 – Lexical Sample, Semantic Role Labeling (SRL) and All-Words respectively. We tabulate and analyze the results of participating systems.},
  eventtitle = {The 4th {{International Workshop}}},
  langid = {english},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\WXHN2Z2U\\Pradhan et al. - 2007 - SemEval-2007 task 17 English lexical sample, SRL .pdf}
}

@book{purves-brannon:2013:principles-cognitive-neuroscience,
  title = {Principles of Cognitive Neuroscience},
  editor = {Purves, Dale and Brannon, Elizabeth M.},
  date = {2013},
  edition = {2. ed},
  publisher = {{Sinauer}},
  location = {{Sunderland, Mass}},
  isbn = {978-0-87893-573-4},
  langid = {english},
  pagetotal = {601},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\XKKHDBM8\\Purves et Brannon - 2013 - Principles of cognitive neuroscience.pdf}
}

@report{r-core-team:2017:language-environment-statistical,
  title = {R: {{A}} Language and Environment for Statistical   Computing},
  author = {R Core Team},
  date = {2017},
  institution = {{R Foundation for Statistical Computing}},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  langid = {english},
  keywords = {*CITATION}
}

@article{radford-etal:2019:language-models-are,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  journaltitle = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\ARNHQL4P\\Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@article{ramesh-etal:2021:zeroshot-texttoimage-generation,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2102.12092},
  url = {https://arxiv.org/abs/2102.12092},
  urldate = {2023-07-25},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  version = {2},
  keywords = {*CITATION,Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\BAFHIAGD\\Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf}
}

@article{ramos:2003:using-tfidf-determine,
  title = {Using {{TF-IDF}} to {{Determine Word Relevance}} in {{Document Queries}}},
  author = {Ramos, Juan},
  date = {2003},
  journaltitle = {Proceedings of the first instructional conference on machine learning},
  abstract = {In this paper, we examine the results of applying Term Frequency Inverse Document Frequency (TF-IDF) to determine what words in a corpus of documents might be more favorable to use in a query. As the term implies, TF-IDF calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in. Words with high TF-IDF numbers imply a strong relationship with the document they appear in, suggesting that if that word were to appear in a query, the document could be of interest to the user. We provide evidence that this simple algorithm efficiently categorizes relevant words that can enhance query retrieval.},
  langid = {english},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\Q7PCWF7S\\Ramos - Using TF-IDF to Determine Word Relevance in Docume.pdf}
}

@online{roach:2023:how-microsoft-bet,
  type = {Blog},
  title = {How {{Microsoft}}’s Bet on {{Azure}} Unlocked an {{AI}} Revolution},
  author = {Roach, John},
  date = {2023},
  url = {https://news.microsoft.com/},
  organization = {{Microsoft}},
  keywords = {*CITATION}
}

@article{rosenberg-hirschberg:2007:vmeasure-conditional-entropybased,
  title = {V-{{Measure}}: {{A Conditional Entropy-Based External Cluster Evaluation Measure}}},
  author = {Rosenberg, Andrew and Hirschberg, Julia},
  date = {2007},
  abstract = {We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the “problem of matching”, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\HK8LKIDM\\Rosenberg et Hirschberg - V-Measure A Conditional Entropy-Based External Cl.pdf}
}

@article{ruiz-etal:2010:densitybased-semisupervised-clustering,
  title = {Density-Based Semi-Supervised Clustering},
  author = {Ruiz, Carlos and Spiliopoulou, Myra and Menasalvas, Ernestina},
  date = {2010-11},
  journaltitle = {Data Min Knowl Disc},
  volume = {21},
  number = {3},
  pages = {345--370},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-009-0157-y},
  url = {http://link.springer.com/10.1007/s10618-009-0157-y},
  urldate = {2023-01-12},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Clustering,Constrained clustering,DBScan},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\T78I7SNJ\\Ruiz et al. - 2010 - Density-based semi-supervised clustering.pdf}
}

@inproceedings{schild-etal:2021:conception-iterative-semisupervisee,
  title = {Conception itérative et semi-supervisée d'assistants conversationnels par regroupement interactif des questions},
  author = {Schild, Erwan and Durantin, Gautier and Lamirel, Jean-Charles and Miconi, Florian},
  date = {2021-01-25},
  volume = {RNTI E-37},
  publisher = {{Edition RNTI}},
  url = {https://hal.inria.fr/hal-03133007},
  urldate = {2021-06-14},
  abstract = {La création d’un jeu de données pour l’entrainement d’un chatbot repose sur un a priori de connaissance du domaine. En conséquence, cette étape est le plus souvent manuelle, fastidieuse et soumise aux biais. Pour garantir l’efficacité et l’objectivité de l’annotation, nous proposons une méthodologie d’apprentissage actif par annotation de contraintes. Il s’agit d’une approche itérative, reposant sur un algorithme de clustering pour segmenter les données et tirant parti de la connaissance de l’annotateur pour guider le regroupement des questions en une structure d’intentions. Dans cet article, nous étudions les paramètres optimaux de modélisation pour réaliser une segmentation exploitable en un minimum d’annotations, et montrons que cette approche permet d’aboutir à une structure cohérente pour l’entrainement d’un assistant conversationnel.},
  eventtitle = {EGC 2021 - 21èmes Journées Francophones Extraction et Gestion des Connaissances},
  langid = {french},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,INTERACTIVE\_CLUSTERING},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\T7NYJRY8\\Schild et al. - 2021 - Conception itérative et semi-supervisée d'assistan.pdf}
}

@inproceedings{schild-etal:2021:concevoir-assistant-conversationnel,
  title = {Concevoir un assistant conversationnel de manière itérative et semi-supervisée avec le clustering interactif},
  shorttitle = {Concevoir un assistant conversationnel avec le clustering interactif},
  booktitle = {TextMine 2021 (TM'2021) - En conjonction avec EGC 2021},
  author = {Schild, Erwan and Durantin, Gautier and Lamirel, Jean-Charles},
  date = {2021-01-26},
  pages = {11--14},
  publisher = {{Pascal Cuxac, Vincent Lemaire, Cédric Lopez}},
  url = {https://hal.inria.fr/hal-03133060},
  abstract = {La création d’un jeu de données nécessaire à la conception d’un assistant conversationnel résulte le plus souvent d’une étape manuelle et fastidieuse qui manque de techniques destinées à l’assister. Pour accélérer cette étape d’annotation, nous proposons une méthode de clustering interactif : il s’agit d’une approche itérative inspirée de l’apprentissage actif, reposant sur un algorithme de clustering et tirant parti d’une annotation de contraintes pour guider le regroupement des questions en une structure d’intentions. Dans cet article, nous exposons la méthodologie à mettre en oeuvre pour concevoir un assistant conversationnel opérationnel à l’aide du clustering interactif.},
  eventtitle = {Atelier - Fouille de Textes - Text Mine 2021 - En conjonction avec EGC 2021},
  langid = {french},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,⛔ No DOI found,INTERACTIVE\_CLUSTERING},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\3GD3XG3E\\Schild et al. - 2021 - Concevoir un assistant conversationnel de manière .pdf;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\IHVRXA2V\\Schild et al - 2021 - PRESENTATION.pdf}
}

@software{schild-etal:2022:cognitivefactory-interactiveclusteringgui,
  title = {Cognitivefactory/Interactive-Clustering-Gui},
  shorttitle = {Cognitivefactory/Interactive-Clustering-Gui},
  author = {SCHILD, Erwan and TTremble and Clementine-Msk},
  date = {2022-09-01},
  url = {https://zenodo.org/record/4775270},
  urldate = {2023-02-13},
  abstract = {Release {$<$}code{$>$}0.4.0{$<$}/code{$>$} of {$<$}code{$>$}cognitivefactory/interactive-clustering-gui{$<$}/code{$>$} package. {$<$}em{$>$}GitHub repository{$<$}/em{$>$} : https://github.com/cognitivefactory/interactive-clustering-gui/tree/0.4.0 {$<$}em{$>$}Main documentation{$<$}/em{$>$} : https://cognitivefactory.github.io/interactive-clustering-gui/ {$<$}em{$>$}Pypi distribution{$<$}/em{$>$} : https://pypi.org/project/cognitivefactory-interactive-clustering-gui/0.4.0/},
  organization = {{Zenodo}},
  version = {0.4.0},
  keywords = {*CITATION}
}

@article{schild-etal:2022:iterative-semisupervised-design,
  title = {Iterative and {{Semi-Supervised Design}} of {{Chatbots Using Interactive Clustering}}},
  shorttitle = {Iterative and {{Semi-Supervised Design}} of {{Chatbots Using Interactive Clustering}}},
  author = {Schild, Erwan and Durantin, Gautier and Lamirel, Jean-Charles and Miconi, Florian},
  date = {2022-04-01},
  journaltitle = {International Journal of Data Warehousing and Mining (IJDWM)},
  volume = {18},
  number = {2},
  pages = {1--19},
  issn = {1548-3924},
  doi = {10.4018/IJDWM.298007},
  url = {https://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/IJDWM.298007},
  abstract = {Chatbots represent a promising tool to automate the processing of requests in a business context. However, despite major progress in natural language processing technologies, constructing a dataset deemed relevant by business experts is a manual, iterative and error-prone process. To assist these experts during modelling and labelling, the authors propose an active learning methodology coined Interactive Clustering. It relies on interactions between computer-guided segmentation of data in intents, and response-driven human annotations imposing constraints on clusters to improve relevance.This article applies Interactive Clustering on a realistic dataset, and measures the optimal settings required for relevant segmentation in a minimal number of annotations. The usability of the method is discussed in terms of computation time, and the achieved compromise between business relevance and classification performance during training.In this context, Interactive Clustering appears as a suitable methodology combining human and computer initiatives to efficiently develop a useable chatbot.},
  langid = {english},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,INTERACTIVE\_CLUSTERING},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\QPFCLFA7\\Schild et al. - 2022 - Iterative and Semi-Supervised Design of Chatbots U.pdf;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\TW3MBXVX\\Schild et al. - 2022 - Iterative and Semi-Supervised Design of Chatbots U.pdf}
}

@software{schild:2021:cognitivefactory-interactiveclusteringcomparativestudy,
  title = {Cognitivefactory/Interactive-Clustering-Comparative-Study},
  author = {SCHILD, Erwan},
  date = {2022-11-05},
  url = {https://zenodo.org/record/5648255},
  urldate = {2023-02-13},
  abstract = {Release {$<$}code{$>$}0.1.0{$<$}/code{$>$} of {$<$}code{$>$}cognitivefactory/interactive-clustering-comparative-study{$<$}/code{$>$} repository. {$<$}em{$>$}GitHub repository{$<$}/em{$>$} : https://github.com/cognitivefactory/interactive-clustering-comparative-study/tree/0.1.0},
  organization = {{Zenodo}},
  version = {0.1.0},
  keywords = {*CITATION,clustering,comparative-study,constraints,interactive-clustering,natural-language-processing,python}
}

@software{schild:2022:cognitivefactory-interactiveclustering,
  title = {Cognitivefactory/Interactive-Clustering},
  shorttitle = {Cognitivefactory/Interactive-Clustering},
  author = {SCHILD, Erwan},
  date = {2022-08-22},
  url = {https://zenodo.org/record/4775251},
  urldate = {2023-02-13},
  abstract = {Release {$<$}code{$>$}0.5.2{$<$}/code{$>$} of {$<$}code{$>$}cognitivefactory/interactive-clustering{$<$}/code{$>$} package. {$<$}em{$>$}GitHub repository{$<$}/em{$>$} : https://github.com/cognitivefactory/interactive-clustering/tree/0.5.2 {$<$}em{$>$}Main documentation{$<$}/em{$>$} : https://cognitivefactory.github.io/interactive-clustering/ {$<$}em{$>$}Pypi distribution{$<$}/em{$>$} : https://pypi.org/project/cognitivefactory-interactive-clustering/0.5.2/},
  organization = {{Zenodo}},
  version = {0.5.2},
  keywords = {*CITATION}
}

@misc{schild:2022:french-trainset-chatbots,
  title = {French trainset for chatbots dealing with usual requests on bank cards},
  author = {SCHILD, Erwan},
  date = {2022-11-09},
  publisher = {{Zenodo}},
  url = {https://zenodo.org/record/4769949},
  urldate = {2023-02-13},
  abstract = {{$<$}strong{$>$}[EN] French training dataset for chatbots dealing with usual requests on bank cards.{$<$}/strong{$>$} {$<$}strong{$>$}Description{$<$}/strong{$>$}: This dataset represents examples of common customer requests relating to bank cards management. It can be used as a training set for a small chatbot intended to process these usual requests. {$<$}strong{$>$}Content{$<$}/strong{$>$}: The questions are asked in French. The dataset is divided into 10 intents of 100 questions each, for a total of 1 000 questions. {$<$}strong{$>$}Intents scope{$<$}/strong{$>$}: Intents are constructed in such a way that all questions arising from the same intention have the same response or action. The scope covered concerns: loss or theft of cards; the swallowed card; the card order; consultation of the bank balance; insurance provided by a card; card unlocking; virtual card management; management of bank overdraft; management of payment limits; management of contactless mode. {$<$}strong{$>$}Origin{$<$}/strong{$>$}: Intents scope is inspired by a chatbot currently in production, and the wording of the questions are inspired by the usual customers requests. {$<$}br{$>$} {$<$}strong{$>$}[FR] Jeu d'entraînement en français d'assistants conversationnels traitant des demandes courantes sur les cartes bancaires.{$<$}/strong{$>$} {$<$}strong{$>$}Description {$<$}/strong{$>$}: Cet ensemble de données représente des exemples de demandes usuelles des clients concernant la gestion des cartes bancaires. Il peut être utilisé comme jeu d'entraînement pour un assistant conversationnel destiné à traiter ces demandes courantes. {$<$}strong{$>$}Contenu {$<$}/strong{$>$}: Les questions sont formulées en français. L'ensemble de données est divisé en 10 intentions de 100 questions chacune, pour un total de 1 000 questions. {$<$}strong{$>$}Périmètre des intentions{$<$}/strong{$>$} : Les intentions sont construites de telle manière que toutes les questions issues d'une même intention ont la même réponse ou action. Le périmètre couvert concerne : la perte ou le vol de cartes ; la carte avalée ; la commande des cartes ; la consultation du solde bancaire ; l'assurance fournie par une carte ; le déverrouillage de la carte ; la gestion de cartes virtuelles ; la gestion du découvert bancaire ; la gestion des plafonds de paiement ; la gestion du mode sans contact. {$<$}strong{$>$}Origine {$<$}/strong{$>$}: Le périmètre des intentions est inspiré par un chatbot actuellement en production, et la formulation des questions est inspirée de demandes courantes de clients.},
  langid = {french},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,*PUBLICATION,Bank cards management,Chatbot,Natural Language Processing,Trainset},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\2YJEM4BD\\French_trainset_for_chatbots_dealing_with_usual_requests_on_bank_cards_v1.0.0.xlsx;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\FGW9EV2H\\French_trainset_for_chatbots_dealing_with_usual_requests_on_bank_cards_v2.0.0_UNLABELED.xlsx;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\KYG43CY9\\French_trainset_for_chatbots_dealing_with_usual_requests_on_bank_cards_v2.0.0.xlsx}
}

@software{schild:2023:cognitivefactory-featuresmaximizationmetric,
  title = {Cognitivefactory/Features-Maximization-Metric},
  shorttitle = {Cognitivefactory/Features-Maximization-Metric},
  author = {SCHILD, Erwan},
  date = {2023-02-16},
  url = {https://zenodo.org/record/7646382},
  urldate = {2023-02-16},
  abstract = {Release {$<$}code{$>$}0.1.1{$<$}/code{$>$} of {$<$}code{$>$}cognitivefactory/features-maximization-metric{$<$}/code{$>$} package. {$<$}em{$>$}GitHub repository{$<$}/em{$>$} : https://github.com/cognitivefactory/features-maximization-metric/tree/0.1.1 {$<$}em{$>$}Main documentation{$<$}/em{$>$} : https://cognitivefactory.github.io/features-maximization-metric/ {$<$}em{$>$}Pypi distribution{$<$}/em{$>$} : https://pypi.org/project/cognitivefactory-features-maximization-metric/0.1.1/},
  organization = {{Zenodo}},
  version = {0.1.1},
  keywords = {*CITATION}
}

@article{schuurmans-frasincar:2020:intent-classification-dialoguea,
  title = {Intent {{Classification}} for {{Dialogue Utterances}}},
  author = {Schuurmans, Jetze and Frasincar, Flavius},
  date = {2020-01-01},
  journaltitle = {IEEE Intell. Syst.},
  volume = {35},
  number = {1},
  pages = {82--88},
  issn = {1541-1672, 1941-1294},
  doi = {10.1109/MIS.2019.2954966},
  url = {https://ieeexplore.ieee.org/document/8910417/},
  urldate = {2023-07-26},
  abstract = {In this work we investigate several machine learning methods to tackle the problem of intent classification for dialogue utterances. We start with Bag-of-Words (BoW) in combination with Na¨ıve Bayes (NB). After that, we employ Continuous Bag-of-Words (CBoW) coupled with Support Vector Machines (SVM). Then follow Long Short-Term Memory (LSTM) networks, which are made bidirectional. The best performing model is hierarchical, such that it can take advantage of the natural taxonomy within classes. The main experiments are a comparison between these methods on an open sourced academic dataset. In the first experiment we consider the full dataset. We also consider the given subsets of data separately, in order to compare our results with state-of-the-art vendor solutions. In general we find that the SVM models outperform the LSTM models. The former models achieve the highest macro-F1 for the full dataset, and in most of the individual datasets. We also found out that the incorporation of the hierarchical structure in the intents improves the performance.},
  langid = {english},
  keywords = {Chatbot,Intention},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\FF7R2HHE\\Schuurmans et Frasincar - 2020 - Intent Classification for Dialogue Utterances.pdf}
}

@online{scialom-etal:2020:mlsum-multilingual-summarization,
  title = {{{MLSUM}}: {{The Multilingual Summarization Corpus}}},
  shorttitle = {{{MLSUM}}},
  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},
  date = {2020-04-30},
  eprint = {2004.14900},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2004.14900},
  urldate = {2023-06-07},
  abstract = {We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.},
  pubstate = {preprint},
  keywords = {*CITATION,*LU/IMPLÉMENTÉ,Computer Science - Computation and Language},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\3JYCXTTG\\mlsum_fr_train_subset_v1.0.0.schild.xlsx;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\SJDKVTSQ\\Scialom et al. - 2020 - MLSUM The Multilingual Summarization Corpus.pdf;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\VN46BQAB\\mlsum_fr_train_subset_v1.0.0.schild_UNLABELLED.xlsx;C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\8GN4N6TU\\2004.html}
}

@inproceedings{seabold-perktold:2010:statsmodels-econometric-statistical,
  title = {Statsmodels: {{Econometric}} and {{Statistical Modeling}} with {{Python}}},
  shorttitle = {Statsmodels},
  author = {Seabold, Skipper and Perktold, Josef},
  date = {2010},
  pages = {92--96},
  location = {{Austin, Texas}},
  doi = {10.25080/Majora-92bf1922-011},
  url = {https://conference.scipy.org/proceedings/scipy2010/seabold.html},
  urldate = {2023-07-07},
  eventtitle = {Python in {{Science Conference}}},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\N79637BD\\Seabold et Perktold - 2010 - Statsmodels Econometric and Statistical Modeling .pdf}
}

@article{settles:2010:active-learning-literature,
  title = {Active {{Learning Literature Survey}}},
  author = {Settles, Burr},
  date = {2010},
  pages = {67},
  abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
  langid = {english},
  keywords = {*CITATION,⛔ No DOI found,Active learning},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\SRWU4YSI\\Settles - 2010 - Active Learning Literature Survey.pdf}
}

@article{snow-etal:2008:cheap-fast-it,
  title = {Cheap and {{Fast}} - {{But}} Is It {{Good}}? {{Evaluating Non-Expert Annotations}} for {{Natural Language Tasks}}},
  author = {Snow, Rion and O'Connor, Brendan and Jurafsky, Daniel and Ng, Andrew},
  date = {2008-10},
  journaltitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  pages = {254--263},
  abstract = {Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.},
  langid = {english},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\7UTBTBGX\\Snow et al. - 2008 - Cheap and Fast - But is it Good Evaluating Non-Ex.pdf}
}

@article{sparck-jones:1972:statistical-interpretation-term,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Sparck Jones, Karen},
  date = {1972-01},
  journaltitle = {Journal of Documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  issn = {0022-0418},
  doi = {10.1108/eb026526},
  url = {https://www.emerald.com/insight/content/doi/10.1108/eb026526/full/html},
  urldate = {2023-07-06},
  langid = {english},
  keywords = {*CITATION,TF-IDF,Vectorization}
}

@article{thorndike:1953:who-belongs-family,
  title = {Who Belongs in the Family?},
  author = {Thorndike, Robert L.},
  date = {1953-12},
  journaltitle = {Psychometrika},
  volume = {18},
  number = {4},
  pages = {267--276},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/BF02289263},
  url = {http://link.springer.com/10.1007/BF02289263},
  urldate = {2023-07-06},
  langid = {english},
  keywords = {*CITATION,Elbow}
}

@article{touvron-etal:2023:llama-open-foundation,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  date = {2023},
  doi = {10.48550/ARXIV.2307.09288},
  url = {https://arxiv.org/abs/2307.09288},
  urldate = {2023-07-20},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  keywords = {*CITATION,Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\VFE8SRBF\\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf}
}

@article{tukey:1949:comparing-individual-means,
  title = {Comparing {{Individual Means}} in the {{Analysis}} of {{Variance}}},
  author = {Tukey, John W.},
  date = {1949-06},
  journaltitle = {Biometrics},
  volume = {5},
  number = {2},
  eprint = {3001913},
  eprinttype = {jstor},
  pages = {99},
  issn = {0006341X},
  doi = {10.2307/3001913},
  url = {https://www.jstor.org/stable/3001913?origin=crossref},
  urldate = {2023-07-06},
  keywords = {*CITATION}
}

@book{van-rossum-drake:2009:python-reference-manual,
  title = {Python 3 {{Reference Manual}}},
  author = {Van Rossum, Guido and Drake, Fred L.},
  date = {2009},
  edition = {CreateSpace},
  location = {{Scotts Valley, CA}},
  isbn = {1-4414-1269-7},
  keywords = {*CITATION}
}

@article{wagstaff-cardie:2000:clustering-instancelevel-constraints,
  title = {Clustering with {{Instance-level Constraints}}},
  author = {Wagstaff, Kiri and Cardie, Claire},
  date = {2000},
  journaltitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
  pages = {1103--1110},
  abstract = {Clustering algorithms conduct a search through the space of possible organizations of a data set. In this paper, we propose two types of instance-level clustering constraints – must-link and cannot-link constraints – and show how they can be incorporated into a clustering algorithm to aid that search. For three of the four data sets tested, our results indicate that the incorporation of surprisingly few such constraints can increase clustering accuracy while decreasing runtime. We also investigate the relative effects of each type of constraint and find that the type that contributes most to accuracy improvements depends on the behavior of the clustering algorithm without constraints.},
  langid = {english},
  keywords = {*CITATION,⛔ No DOI found,Clustering,Constrained clustering,Contraintes,INTERACTIVE\_CLUSTERING,Must-link and Cannot-Link},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\8WNCENKV\\Wagstaﬀ et Cardie - 2000 - Clustering with Instance-level Constraints.pdf}
}

@inproceedings{wagstaff-etal:2001:constrained-kmeans-clustering,
  title = {Constrained {{K-means}} Clustering with Background Knowledge},
  author = {Wagstaff, Kiri and Cardie, Claire and Rogers, Seth and Schrödl, Stefan},
  date = {2001-01},
  series = {Proceedings of 18th {{International Conference}} on {{Machine Learning}}},
  pages = {577--584},
  keywords = {*CITATION},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\TGGJZZWD\\Wagstaff et al. - 2001 - Constrained K-means clustering with background kno.pdf}
}

@article{xu-tian:2015:comprehensive-survey-clustering,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  author = {Xu, Dongkuan and Tian, Yingjie},
  date = {2015},
  journaltitle = {Annals of Data Science},
  volume = {2},
  pages = {165--193},
  abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table 22.},
  keywords = {*CITATION,Clustering,Comparaison}
}

@article{zhang-etal:2019:pegasus-pretraining-extracted,
  title = {{{PEGASUS}}: {{Pre-training}} with {{Extracted Gap-sentences}} for {{Abstractive Summarization}}},
  shorttitle = {{{PEGASUS}}},
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
  date = {2019},
  doi = {10.48550/ARXIV.1912.08777},
  url = {https://arxiv.org/abs/1912.08777},
  urldate = {2023-07-17},
  abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
  keywords = {*CITATION,Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {C\:\\Users\\SCHILDEW\\Documents\\Zotero\\storage\\UWWCFCEF\\Zhang et al. - 2019 - PEGASUS Pre-training with Extracted Gap-sentences.pdf}
}
